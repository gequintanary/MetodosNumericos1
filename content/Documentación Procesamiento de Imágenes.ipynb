{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ee9fe6-9cd4-4ca6-bfa0-3c2c99653cad",
   "metadata": {},
   "source": [
    "# UNIVERSIDAD CUAUHT√âMOC\n",
    "## Licenciatura en Ingenier√≠a en Software y Sistemas Computacionales\n",
    "\n",
    "**Tema:** Procesamiento de im√°genes  \n",
    "**Tipo de actividad:** Documentaci√≥n  \n",
    "\n",
    "---\n",
    "\n",
    "### Alumnos:\n",
    "* **Gabriela Estefan√≠a Quintanar Y√°√±ez**\n",
    "* **Jos√© Carlos √Åguilar Contreras**\n",
    "* **Alain Josu√© Reyes Dom√≠nguez**\n",
    "* **Fernando Navarro Rodr√≠guez**\n",   
    "* **Juan Antonio Prudente Roque**\n",
    "\n",
    "**Materia:** M√©todos Num√©ricos  \n",
    "**Maestro:** An√≥nimo  \n",
    "\n",
    "---\n",
    "\n",
    "**Lugar:** Quer√©taro, Qro.  \n",
    "**Fecha:** 28 de enero del 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99612fb3-21fb-49aa-b9bf-25680c19e961",
   "metadata": {},
   "source": [
    "# √çndice\n",
    "\n",
    "1. [**1. Introducci√≥n**](#1.-Introducci√≥n)\n",
    "2. [**2. Desarrollo**](#2.-Desarrollo)\n",
    "   * [2.1 ¬øQu√© es MindAR?](#2.1-¬øQu√©-es-MindAR?)\n",
    "   * [2.2 ¬øQu√© es OpenCV?](#2.2-¬øQu√©-es-OpenCV?)\n",
    "   * [2.3 ¬øMindAR usa OpenCV internamente?](#2.3-¬øMindAR-usa-OpenCV-internamente?)\n",
    "   * [2.4 ¬øSe puede usar OpenCV en JavaScript?](#2.4-¬øSe-puede-usar-OpenCV-en-JavaScript?)\n",
    "   * [2.5 ¬øPara qu√© sirve el algoritmo Canny Edge Detection?](#2.5-¬øPara-qu√©-sirve-el-algoritmo-Canny-Edge-Detection?)\n",
    "   * [2.6 Ejemplo de Canny en JavaScript y OpenCV](#2.6-Ejemplo-de-Canny-en-JavaScript-y-OpenCV)\n",
    "   * [2.7 ¬øCanny utiliza derivadas?](#2.7-¬øCanny-utiliza-derivadas?)\n",
    "   * [2.8 Uso de diferencias finitas en Canny](#2.8-Uso-de-diferencias-finitas-en-Canny)\n",
    "   * [2.9 Algoritmos para detectar bordes](#2.9-Algoritmos-para-detectar-bordes)\n",
    "   * [2.10 ¬øPara qu√© sirve el algoritmo de Sobel?](#2.10-¬øPara-qu√©-sirve-el-algoritmo-de-Sobel?)\n",
    "   * [2.11 ¬øSobel utiliza derivadas?](#2.11-¬øSobel-utiliza-derivadas?)\n",
    "   * [2.12 Relaci√≥n entre Sobel y diferencias finitas](#2.12-Relaci√≥n-entre-Sobel-y-diferencias-finitas)\n",
    "   * [2.13 Uso de im√°genes en escala de grises para Sobel](#2.13-Uso-de-im√°genes-en-escala-de-grises-para-Sobel)\n",
    "   * [2.14 ¬øSe puede usar Sobel en JavaScript?](#2.14-¬øSe-puede-usar-Sobel-en-JavaScript?)\n",
    "   * [2.15 ¬øQu√© es MediaPipe Face Mesh de Google?](#2.15-¬øQu√©-es-MediaPipe-Face-Mesh-de-Google?)\n",
    "   * [2.16 Construir un programa (JavaScript + HTML) MediaPipe Face Mesh](#2.16-Construir-un-programa-(JavaScript-+-HTML)-MediaPipe-Face-Mesh)\n",
    "   * [2.17 ¬øMediaPipe Face Mesh utiliza el algoritmo Sobel?](#2.17-¬øMediaPipe-Face-Mesh-utiliza-alg√∫n-algoritmo-para-procesamiento-de-im√°genes-como-el-Sobel?)\n",
    "   * [2.18 ¬øQu√© son las redes neuronales convolucionales (CNN)?](#2.18-¬øQu√©-son-las-redes-neuronales-convolucionales-(CNN)?)\n",
    "   * [2.19 Adaptar el programa del punto 2.16 para que funcione en GITHUB](#2.19-Adaptar-el-programa-del-punto-2.16-para-que-funcione-en-GITHUB)\n",
    "   * [2.20 ¬øC√≥mo se solicita el acceso a la webcam para seguimiento de cara?](#2.20-¬øC√≥mo-se-solicita-el-acceso-a-la-webcam-para-seguimiento-de-cara?)\n",
    "   * [2.21 Construir el mismo programa para editar la m√°scara y colocar accesorios](#2.21-Construir-el-mismo-programa-para-editar-la-m√°scara-y-colocar-bigote,-sombrero,-un-lunar-etc.)\n",
    "   * [2.22 Escribir el mismo concepto pero usando Sobel](#2.22-Escribir-el-mismo-concepto,-pero-usando-Sobel)\n",
    "3. [**3. Conclusiones**](#3.-Conclusiones)\n",
    "4. [**4. Referencias**](#4.-Referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba6f75-c96f-4cff-ac8c-86328177b3a8",
   "metadata": {},
   "source": [
    "# 1. Introducci√≥n\n",
    "\n",
    "El presente informe t√©cnico tiene como objetivo explorar y documentar diversas tecnolog√≠as y algoritmos fundamentales en el campo del **procesamiento de im√°genes** y la **visi√≥n por computadora**, con un enfoque particular en su implementaci√≥n en entornos web.\n",
    "\n",
    "A lo largo de este documento, se analizan herramientas clave como **MindAR**, una librer√≠a de realidad aumentada para la web, y **OpenCV**, la biblioteca est√°ndar de la industria para visi√≥n artificial. Adem√°s, se profundiza en la base matem√°tica de algoritmos cl√°sicos de detecci√≥n de bordes, como **Canny** y **Sobel**, explicando su relaci√≥n directa con conceptos de **m√©todos num√©ricos** como las **derivadas** y las **diferencias finitas**.\n",
    "\n",
    "Finalmente, se aborda la evoluci√≥n hacia m√©todos modernos basados en Inteligencia Artificial, como **MediaPipe Face Mesh** y las **Redes Neuronales Convolucionales (CNN)**, contrastando c√≥mo estos modelos aprenden caracter√≠sticas visuales complejas en comparaci√≥n con los m√©todos tradicionales de filtrado. Esta investigaci√≥n permite comprender la transici√≥n desde el procesamiento de se√±ales b√°sico hasta las aplicaciones avanzadas de realidad aumentada en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63c976-19dd-4f5d-8b1b-9fe84d77ff94",
   "metadata": {},
   "source": [
    "# 2. Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eee698-7102-448e-a734-6e9f918cd54b",
   "metadata": {},
   "source": [
    "### 2.1 ¬øQu√© es MindAR?\n",
    "\n",
    "**MindAR** es una librer√≠a de Realidad Aumentada (AR) de c√≥digo abierto y basada en la web. Su principal objetivo es permitir a los desarrolladores integrar funciones de realidad aumentada directamente en los navegadores, sin necesidad de que el usuario instale aplicaciones adicionales.\n",
    "\n",
    "#### Caracter√≠sticas principales:\n",
    "* **Seguimiento de im√°genes (Image Tracking):** Permite detectar y rastrear marcadores o im√°genes planas (como posters o tarjetas de presentaci√≥n) para superponer contenido 3D.\n",
    "* **Seguimiento facial (Face Tracking):** Capacidad para detectar rostros y aplicar filtros o efectos (similar a los filtros de Instagram o Snapchat).\n",
    "* **Independencia de plataforma:** Al ser web, funciona tanto en computadoras como en dispositivos m√≥viles (iOS y Android).\n",
    "\n",
    "#### ¬øTrabaja con JavaScript?\n",
    "S√≠, absolutamente. MindAR est√° construida principalmente con **JavaScript** y utiliza tecnolog√≠as como **Three.js** o **A-Frame** para el renderizado de los objetos en 3D.\n",
    "\n",
    "Para utilizarla, generalmente se incluye como un m√≥dulo de JavaScript o a trav√©s de una etiqueta `<script>` en el HTML. Adem√°s, aprovecha la aceleraci√≥n por hardware del dispositivo mediante **WebGL** para que el rendimiento sea fluido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d64cf4-a0b1-4297-8c65-af3ab1d28a14",
   "metadata": {},
   "source": [
    "### 2.2 ¬øQu√© es OpenCV?\n",
    "\n",
    "**OpenCV** (Open Source Computer Vision Library) es la biblioteca de software de visi√≥n artificial y aprendizaje autom√°tico (*machine learning*) m√°s importante y utilizada en el mundo. Fue creada originalmente por Intel y es de c√≥digo abierto.\n",
    "\n",
    "Su prop√≥sito es proporcionar una infraestructura com√∫n para aplicaciones de visi√≥n por computadora, permitiendo que las m√°quinas \"entiendan\" im√°genes y videos de la misma forma que lo hacemos los humanos.\n",
    "\n",
    "#### Caracter√≠sticas principales:\n",
    "* **Biblioteca Gigantesca:** Contiene m√°s de 2,500 algoritmos optimizados que incluyen desde detecci√≥n de rostros hasta seguimiento de objetos en movimiento y reconstrucci√≥n 3D.\n",
    "* **Multiplataforma:** Funciona en Windows, Linux, macOS, Android y iOS.\n",
    "* **Lenguajes:** Aunque est√° escrita originalmente en C++, tiene \"wrappers\" (adaptaciones) oficiales para Python, Java y, por supuesto, JavaScript (OpenCV.js)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716a2a8-d37f-48c5-b977-627e6a1ea512",
   "metadata": {},
   "source": [
    "### 2.3 ¬øMindAR usa OpenCV internamente?\n",
    "\n",
    "No directamente, pero s√≠ conceptualmente.\n",
    "\n",
    "**Explicaci√≥n t√©cnica:** Aunque MindAR realiza tareas que tradicionalmente se har√≠an con OpenCV (como detectar puntos de referencia faciales o rastrear una imagen), MindAR no depende de la librer√≠a OpenCV para funcionar.\n",
    "\n",
    "En su lugar, MindAR utiliza:\n",
    "\n",
    "1. **TensorFlow.js:** Para el seguimiento facial (*Face Tracking*), MindAR utiliza modelos de aprendizaje profundo (*Deep Learning*) desarrollados por Google que corren sobre TensorFlow.js.\n",
    "2. **Algoritmos propios en JavaScript/WebAssembly:** Para el seguimiento de im√°genes (*Image Tracking*), el autor de MindAR escribi√≥ algoritmos optimizados espec√≠ficamente para la web que detectan \"puntos de inter√©s\" de forma manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35874d33-71fb-4d3f-93ed-9a2508ca5a29",
   "metadata": {},
   "source": [
    "### 2.4 ¬øSe puede usar OpenCV en JavaScript?\n",
    "\n",
    "S√≠, mediante **OpenCV.js (WebAssembly)**. Un ejemplo b√°sico de uso es:\n",
    "\n",
    "```javascript\n",
    "cv.onRuntimeInitialized = () => {\n",
    "    let src = cv.imread('canvasInput');\n",
    "    let dst = new cv.Mat();\n",
    "    cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);\n",
    "    cv.imshow('canvasOutput', dst);\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f16ffc-abdf-4c2d-b3b2-afabc3ed2565",
   "metadata": {},
   "source": [
    "### Informaci√≥n Adicional: OpenCV.js\n",
    "\n",
    "> **¬øQu√© es OpenCV.js?**\n",
    "> Es una versi√≥n de OpenCV compilada espec√≠ficamente para la web usando una tecnolog√≠a llamada WebAssembly (Wasm).\n",
    ">\n",
    "> **¬øC√≥mo funciona?**\n",
    "> Permite que el c√≥digo pesado y r√°pido de C++ (en el que est√° escrito OpenCV) se ejecute dentro del navegador a una velocidad casi nativa.\n",
    ">\n",
    "> **¬øPara qu√© se usa en JS?**\n",
    "> Los desarrolladores web lo usan para tareas de procesamiento de im√°genes en tiempo real dentro de un canvas de HTML5, como aplicar filtros, detectar bordes o manipular los pixeles de la c√°mara web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f311f5-dccd-4d7b-b334-69ac88287720",
   "metadata": {},
   "source": [
    "### 2.5 ¬øPara qu√© sirve el algoritmo Canny Edge Detection?\n",
    "\n",
    "El algoritmo de Canny es considerado el est√°ndar de oro para la detecci√≥n de bordes en el procesamiento de im√°genes digitales. Su objetivo principal es reducir la cantidad de datos de una imagen (simplificarla) conservando √∫nicamente las propiedades estructurales m√°s importantes: los contornos.\n",
    "\n",
    "#### ¬øPara qu√© sirve espec√≠ficamente?\n",
    "\n",
    "* **Segmentaci√≥n de objetos:** Ayuda a separar los objetos del fondo al identificar sus l√≠mites.\n",
    "* **Reconocimiento de formas:** Al obtener solo las l√≠neas de los bordes, es mucho m√°s f√°cil para una computadora identificar si est√° viendo un c√≠rculo, un cuadrado o un rostro.\n",
    "* **Reducci√≥n de ruido:** A diferencia de otros m√©todos simples, Canny incluye un paso de filtrado que evita que el ruido (puntos aleatorios) de la imagen sea confundido con un borde real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8470399-32d9-43c9-9e93-f9156f71032f",
   "metadata": {},
   "source": [
    "### 2.6 Escribir un ejemplo del algoritmo de Canny Edge Detection usando JavaScript y OpenCV\n",
    "\n",
    "Para ejecutar este c√≥digo, se asume que ya has cargado la librer√≠a `opencv.js`. El flujo b√°sico consiste en leer una imagen desde un elemento `<canvas>`, convertirla a escala de grises (Canny funciona mejor as√≠) y aplicar la funci√≥n `cv.Canny`.\n",
    "\n",
    "```javascript\n",
    "// Suponiendo que 'imageElement' es un elemento <img> o <video>\n",
    "// y 'canvasOutput' es el <canvas> donde mostraremos el resultado.\n",
    "\n",
    "function applyCanny() {\n",
    "    // 1. Leer la imagen desde el elemento de origen\n",
    "    let src = cv.imread(imageElement);\n",
    "    let dst = new cv.Mat(); // Matriz de destino\n",
    "    \n",
    "    // 2. Convertir a escala de grises (Canny requiere un solo canal)\n",
    "    cv.cvtColor(src, src, cv.COLOR_RGBA2GRAY, 0);\n",
    "    \n",
    "    // 3. Aplicar el algoritmo de Canny\n",
    "    // Par√°metros: (entrada, salida, umbral bajo, umbral alto, tama√±o del kernel, L2gradient)\n",
    "    cv.Canny(src, dst, 50, 100, 3, false);\n",
    "    \n",
    "    // 4. Mostrar el resultado en el canvas\n",
    "    cv.imshow('canvasOutput', dst);\n",
    "    \n",
    "    // 5. Liberar memoria de las matrices (muy importante en OpenCV.js)\n",
    "    src.delete();\n",
    "    dst.delete();\n",
    "}\n",
    "\n",
    "// Este evento se dispara cuando OpenCV.js est√° listo\n",
    "cv['onRuntimeInitialized'] = () => {\n",
    "    applyCanny();\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb75a8-37cf-4713-ba01-1282fab62258",
   "metadata": {},
   "source": [
    "### 2.7 ¬øCanny utiliza derivadas?\n",
    "\n",
    "S√≠, absolutamente. El fundamento matem√°tico de la detecci√≥n de bordes es el c√°lculo de derivadas.\n",
    "\n",
    "#### ¬øPor qu√© se usan derivadas?\n",
    "En una imagen digital, un \"borde\" es simplemente un lugar donde el brillo o el color cambian de forma brusca (un salto de intensidad). En matem√°ticas, la **derivada** mide la tasa de cambio de una funci√≥n.\n",
    "\n",
    "* **Si la derivada es cero:** significa que el color es constante (no hay borde).\n",
    "* **Si la derivada tiene un valor alto:** significa que hay un cambio r√°pido de intensidad (hay un borde).\n",
    "\n",
    "#### ¬øC√≥mo las aplica el algoritmo?\n",
    "Canny no usa una derivada simple, sino que calcula el **Gradiente** de la imagen. Para ello, utiliza derivadas parciales en dos direcciones:\n",
    "\n",
    "1. **Derivada en X ($\\frac{\\partial f}{\\partial x}$):** Detecta cambios de intensidad en sentido horizontal (bordes verticales).\n",
    "2. **Derivada en Y ($\\frac{\\partial f}{\\partial y}$):** Detecta cambios de intensidad en sentido vertical (bordes horizontales).\n",
    "\n",
    "Luego, combina ambas para obtener la **magnitud del gradiente**, que nos dice qu√© tan \"fuerte\" es el borde, y la **direcci√≥n del gradiente**, que nos dice hacia d√≥nde apunta ese cambio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae525c-adf2-4a9c-ad80-15bd699c7db8",
   "metadata": {},
   "source": [
    "### 2.8 ¬øEn qu√© forma el algoritmo Canny Edge utiliza las diferencias finitas en su c√°lculo?\n",
    "\n",
    "En el procesamiento de im√°genes digitales, no podemos aplicar derivadas continuas porque la imagen est√° compuesta por elementos discretos (p√≠xeles). Para resolver esto, el algoritmo de Canny utiliza las **diferencias finitas** como una aproximaci√≥n num√©rica de la derivada.\n",
    "\n",
    "* **Aproximaci√≥n de la pendiente:** En lugar de un l√≠mite matem√°tico que tiende a cero, el algoritmo estima el cambio de intensidad restando los valores de los p√≠xeles adyacentes.\n",
    "* **Diferencias Centrales:** Com√∫nmente se utiliza la diferencia entre los vecinos de un p√≠xel para obtener una mejor estimaci√≥n del gradiente. Por ejemplo:\n",
    "    * **En el eje X:** Se calcula la diferencia entre el p√≠xel de la derecha y el de la izquierda: $f(x+1, y) - f(x-1, y)$.\n",
    "    * **En el eje Y:** Se calcula la diferencia entre el p√≠xel de arriba y el de abajo: $f(x, y+1) - f(x, y-1)$.\n",
    "* **Convoluci√≥n:** Estas diferencias se implementan mediante el uso de **kernels** (peque√±as matrices) que se deslizan sobre la imagen multiplicando y sumando valores, lo que permite automatizar el c√°lculo de la diferencia finita para cada punto de la fotograf√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b705a5e-f5a0-45ac-a40e-87ea68366ba8",
   "metadata": {},
   "source": [
    "### 2.9 ¬øCu√°les son los algoritmos para detectar bordes en procesamiento de im√°genes?\n",
    "\n",
    "Existen diversos algoritmos, cada uno con un enfoque matem√°tico distinto. Los m√°s comunes son:\n",
    "\n",
    "| Algoritmo | Tipo de Operador | Caracter√≠sticas |\n",
    "| :--- | :--- | :--- |\n",
    "| **Canny** | √ìptimo (Multi-etapa) | Es el m√°s preciso; usa supresi√≥n de no-m√°ximos y doble umbral para reducir bordes falsos. |\n",
    "| **Sobel** | Gradiente (1¬™ Derivada) | Calcula la magnitud del cambio de intensidad en X e Y; es muy usado por su sencillez. |\n",
    "| **Prewitt** | Gradiente (1¬™ Derivada) | Similar a Sobel, pero utiliza un kernel que da el mismo peso a todos los p√≠xeles vecinos. |\n",
    "| **Laplaciano** | 2¬™ Derivada | Detecta bordes buscando los \"cruces por cero\" en la segunda derivada de la imagen. |\n",
    "| **Roberts** | Gradiente (Diagonal) | Uno de los m√°s antiguos; calcula diferencias en las diagonales de un bloque de 2x2 p√≠xeles. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91942016-1b29-4004-a813-93e6c29041db",
   "metadata": {},
   "source": [
    "### 2.10 ¬øPara qu√© sirve el algoritmo de Sobel en procesamiento de im√°genes?\n",
    "\n",
    "El algoritmo de Sobel es un operador utilizado principalmente para la detecci√≥n de bordes y el resaltado de contornos en im√°genes digitales. Su funci√≥n principal es transformar una imagen original en una representaci√≥n donde se acent√∫an las variaciones bruscas de luminosidad.\n",
    "\n",
    "#### Sus aplicaciones principales incluyen:\n",
    "\n",
    "* **Identificaci√≥n de contornos:** Permite localizar d√≥nde termina un objeto y empieza otro bas√°ndose en los cambios de intensidad de los p√≠xeles.\n",
    "* **C√°lculo de la direcci√≥n del borde:** A diferencia de otros filtros simples, Sobel no solo detecta si hay un borde, sino que tambi√©n indica la orientaci√≥n (horizontal, vertical o diagonal) de dicho cambio.\n",
    "* **Reducci√≥n de datos:** Simplifica la imagen original eliminando detalles irrelevantes y manteniendo √∫nicamente la estructura geom√©trica esencial.\n",
    "* **Preprocesamiento para visi√≥n artificial:** Se utiliza como paso previo para algoritmos m√°s complejos, ayudando a las computadoras a reconocer formas o texturas espec√≠ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def564e5-3a8f-403e-a354-03e38cdfce85",
   "metadata": {},
   "source": [
    "### 2.11 ¬øEn qu√© forma el algoritmo de Sobel utiliza derivadas?\n",
    "\n",
    "Utiliza la derivada para encontrar la pendiente de la intensidad de los p√≠xeles. En el procesamiento de im√°genes se tratan a las im√°genes digitales como una funci√≥n $f(x, y)$, en donde:\n",
    "\n",
    "* **$x, y$:** Son las coordenadas espaciales (p√≠xeles).\n",
    "* **$f(x, y)$:** Es la intensidad del brillo que se encuentra en ese punto.\n",
    "\n",
    "Con esto en mente, un ‚Äúborde‚Äù no ser√≠a m√°s que una zona en donde el valor de nuestra funci√≥n cambia dr√°sticamente. En c√°lculo, la herramienta que se utiliza para medir este cambio es la derivada.\n",
    "\n",
    "#### Gradiente de la imagen\n",
    "Las im√°genes cuentan con dos dimensiones (Eje $x, y$), por lo que no tenemos una sola derivada, sino un **vector gradiente**. Este vector apunta en la direcci√≥n del mayor aumento de intensidad y se define como:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} G_x \\\\ G_y \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "Donde:\n",
    "* **$G_x$:** Es la derivada parcial respecto a $x$ (Para representar cambios horizontales).\n",
    "* **$G_y$:** Es la derivada parcial respecto a $y$ (Para representar cambios verticales).\n",
    "\n",
    "#### La Aproximaci√≥n de la Derivada\n",
    "En c√°lculo continuo, la derivada siempre va a necesitar de un l√≠mite que tiende a cero. Pero una imagen no puede contar con una distancia menor a un p√≠xel, por lo que Sobel utiliza una **aproximaci√≥n discreta**. La derivada se calcula como una diferencia central. En lugar de utilizar la f√≥rmula de la derivada cl√°sica, Sobel realiza una estimaci√≥n de la pendiente mirando los vecinos del p√≠xel:\n",
    "\n",
    "* **En X:** Mira el p√≠xel de la derecha y de la izquierda.\n",
    "* **En Y:** Mira el p√≠xel de arriba y los de abajo.\n",
    "\n",
    "#### El ‚ÄúOperador‚Äù o Kernel\n",
    "A partir de aqu√≠ es donde el algoritmo de Sobel se diferencia por completo de una derivada simple. Sobel no solo va a restar p√≠xeles, sino que utilizar√° una **matriz de convoluci√≥n de 3x3** (matriz empleada para aplicar efectos matem√°ticos a una imagen digital)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe0ad0-cafd-4837-a425-d0637cc29b12",
   "metadata": {},
   "source": [
    "### 2.12 ¬øQu√© relaci√≥n tiene Sobel con diferencias finitas?\n",
    "\n",
    "El algoritmo de Sobel y las diferencias finitas est√°n estrechamente vinculados, ya que el primero es una aplicaci√≥n pr√°ctica y mejorada de las segundas en el mundo digital:\n",
    "\n",
    "* **Fundamento Matem√°tico:** Sobel utiliza las **diferencias finitas** como base para aproximar la derivada de la intensidad de la imagen.\n",
    "* **Aproximaci√≥n Discreta:** Dado que en una imagen no existe una distancia menor a un p√≠xel, Sobel implementa la **\"diferencia central\"** (un tipo de diferencia finita) para estimar la pendiente mirando a los vecinos del p√≠xel.\n",
    "* **Estructura del Kernel:** Los valores dentro de las matrices de convoluci√≥n de Sobel ($G_x$ y $G_y$) est√°n dise√±ados para realizar restas entre p√≠xeles opuestos, lo cual es la esencia de la f√≥rmula de diferencias finitas aplicada a una rejilla bidimensional.\n",
    "* **Evoluci√≥n T√©cnica:** Se puede decir que Sobel es una **\"diferencia finita con esteroides\"**, pues toma la resta b√°sica de p√≠xeles y le a√±ade un filtrado de suavizado para reducir el ruido que las diferencias finitas simples suelen amplificar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f7be5-dd8a-443c-82d9-f780e4e2bc37",
   "metadata": {},
   "source": [
    "### 2.13 ¬øPara usar im√°genes en Sobel se requiere de im√°genes en grises?\n",
    "\n",
    "S√≠. Aunque t√©cnicamente se podr√≠a aplicar a cada canal de color por separado, el est√°ndar y la pr√°ctica correcta en el procesamiento de im√°genes es utilizar escala de grises. Esto se debe a las siguientes razones t√©cnicas:\n",
    "\n",
    "* **Simplificaci√≥n matem√°tica:** Al trabajar en escala de grises, el algoritmo solo tiene que calcular el gradiente de un √∫nico valor de intensidad por p√≠xel, en lugar de procesar tres canales (Rojo, Verde y Azul) de forma independiente.\n",
    "* **Enfoque en la luminancia:** Los bordes se definen matem√°ticamente como cambios bruscos en el brillo o intensidad luminosa; la informaci√≥n de color suele ser irrelevante para detectar la estructura geom√©trica de un objeto.\n",
    "* **Eficiencia de c√≥mputo:** Procesar una imagen monocrom√°tica reduce significativamente la carga de procesamiento y el uso de memoria, lo cual es vital para aplicaciones que funcionan en tiempo real dentro de un navegador web.\n",
    "* **Evita artefactos y ruido:** Aplicar Sobel a canales de color individuales puede generar bordes \"falsos\" o inconsistentes en zonas donde el matiz cambia, pero la intensidad luminosa permanece constante, lo que confundir√≠a al detector de bordes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98457043-5f3d-4a02-a979-0270abc8c0ae",
   "metadata": {},
   "source": [
    "### 2.14 ¬øLos algoritmos que usa MindAR son de fuente abierta?\n",
    "\n",
    "S√≠. MindAR es un proyecto completamente de c√≥digo abierto (**Open Source**), lo que permite que su funcionamiento interno sea transparente y accesible para cualquier desarrollador.\n",
    "\n",
    "* **Licencia:** Se distribuye bajo la **licencia MIT**, una de las m√°s permisivas en la industria, lo que facilita su uso en proyectos acad√©micos, personales y comerciales.\n",
    "* **Transparencia t√©cnica:** Al ser de fuente abierta, se puede verificar que utiliza algoritmos propios escritos en JavaScript y WebAssembly para el rastreo de im√°genes (*Image Tracking*), optimizados espec√≠ficamente para el navegador.\n",
    "* **Integraci√≥n de terceros:** Aunque es una librer√≠a independiente, integra de forma abierta otros modelos de c√≥digo abierto como **TensorFlow.js** para las funciones de seguimiento facial.\n",
    "* **Independencia:** Su naturaleza \"Open Source\" garantiza que no depende de servicios en la nube propietarios, permitiendo que el procesamiento ocurra localmente en el dispositivo del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d3a75-ca17-4745-9b59-ec8b86e833e6",
   "metadata": {},
   "source": [
    "### 2.15 ¬øQu√© es MediaPipe Face Mesh de Google?\n",
    "\n",
    "MediaPipe Face Mesh es una soluci√≥n de inteligencia artificial de alto rendimiento que estima **468 puntos de referencia faciales en 3D** en tiempo real. \n",
    "\n",
    "Esto quiere decir que es una herramienta que ha sido entrenada viendo millones de rostros humanos para que, en cuanto una persona se ponga frente a la c√°mara, sepa d√≥nde est√° la nariz sin que se tengan que programar reglas matem√°ticas complicadas (por ejemplo: *\"si tiene dos orejas triangulares, bigotes de 5cm y ojos amarillos, es un gato\"*).\n",
    "\n",
    "A diferencia de otros sistemas que solo detectan si hay una cara o d√≥nde est√°n los ojos, Face Mesh crea una **\"malla\" completa** que se adapta a la geometr√≠a de tu rostro. Esto permite aplicaciones de realidad aumentada (AR) muy precisas (la IA encuentra tu rostro y la RA dibuja, por ejemplo, orejas de perro), como probarse maquillaje, gafas o efectos de deformaci√≥n facial.\n",
    "\n",
    "#### Puntos clave:\n",
    "\n",
    "* **Velocidad:** Est√° optimizado para correr en m√≥viles y navegadores web sin necesidad de servidores potentes.\n",
    "* **3D Real:** Aunque la c√°mara es 2D, el modelo estima la profundidad (coordenada Z).\n",
    "* **Landmarks:** Utiliza una topolog√≠a fija, lo que significa que el punto \"4\" siempre ser√° la punta de la nariz, sin importar qui√©n sea la persona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880946bd-17e6-4993-9736-79d9061ad4cf",
   "metadata": {},
   "source": [
    "### 2.16 Construir un programa (JavaScript + HTML) MediaPipe Face Mesh\n",
    "\n",
    "Este programa abre la webcam y dibuja los 468 puntos faciales 3D sobre un elemento `<canvas>`.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>MediaPipe FaceMesh ‚Äì 468 puntos</title>\n",
    "    \n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js](https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js)\"></script>\n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js](https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js)\"></script>\n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js](https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js)\"></script>\n",
    "    \n",
    "    <style>\n",
    "        body {\n",
    "            margin: 0;\n",
    "            background: #000;\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            height: 100vh;\n",
    "        }\n",
    "        canvas {\n",
    "            border: 2px solid #2ecc71;\n",
    "            border-radius: 10px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <video id=\"input_video\" style=\"display:none;\"></video>\n",
    "    \n",
    "    <canvas id=\"output_canvas\" width=\"640\" height=\"480\"></canvas>\n",
    "    \n",
    "    <script>\n",
    "        const videoElement = document.getElementById(\"input_video\");\n",
    "        const canvasElement = document.getElementById(\"output_canvas\");\n",
    "        const canvasCtx = canvasElement.getContext(\"2d\");\n",
    "\n",
    "        // Inicializar FaceMesh\n",
    "        const faceMesh = new FaceMesh({\n",
    "            locateFile: (file) => {\n",
    "                return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;\n",
    "            }\n",
    "        });\n",
    "\n",
    "        faceMesh.setOptions({\n",
    "            maxNumFaces: 1,\n",
    "            refineLandmarks: true,\n",
    "            minDetectionConfidence: 0.5,\n",
    "            minTrackingConfidence: 0.5\n",
    "        });\n",
    "\n",
    "        faceMesh.onResults((results) => {\n",
    "            // Limpiar el canvas y dibujar la imagen de la c√°mara\n",
    "            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n",
    "            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);\n",
    "\n",
    "            if (results.multiFaceLandmarks) {\n",
    "                for (const landmarks of results.multiFaceLandmarks) {\n",
    "                    // Dibuja los 468 puntos (Landmarks)\n",
    "                    drawLandmarks(canvasCtx, landmarks, {\n",
    "                        color: \"#ffffff\",\n",
    "                        lineWidth: 0.5,\n",
    "                        radius: 0.5\n",
    "                    });\n",
    "                }\n",
    "            }\n",
    "        });\n",
    "\n",
    "        // Configuraci√≥n de la C√°mara\n",
    "        const camera = new Camera(videoElement, {\n",
    "            onFrame: async () => {\n",
    "                await faceMesh.send({ image: videoElement });\n",
    "            },\n",
    "            width: 640,\n",
    "            height: 480\n",
    "        });\n",
    "\n",
    "        camera.start();\n",
    "    </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18524b4-e235-4364-9043-c5b84fc93a48",
   "metadata": {},
   "source": [
    "### 2.17 ¬øMediaPipe Face Mesh utiliza alg√∫n algoritmo para procesamiento de im√°genes como el Sobel?\n",
    "\n",
    "No directamente. MediaPipe Face Mesh utiliza **Redes Neuronales Convolucionales (CNN)** modernas. \n",
    "\n",
    "Mientras que el operador de Sobel es un filtro manual dise√±ado por humanos para resaltar bordes calculando el gradiente de intensidad (usando matem√°ticas de derivadas), las CNN de MediaPipe \"aprenden\" sus propios filtros durante el entrenamiento.\n",
    "\n",
    "#### Proceso de funcionamiento:\n",
    "\n",
    "1. **Entrenamiento:** Los ingenieros le muestran millones de fotos de personas reales. En cada foto, un humano marc√≥ manualmente: \"Esto es un ojo\", \"Esto es la barbilla\".\n",
    "2. **Aprendizaje de filtros:** La IA descubri√≥ que, para encontrar un ojo, primero necesita encontrar bordes (como hace Sobel), pero luego necesita combinar esos bordes para formar curvas, y luego combinar curvas para formar la forma de un p√°rpado.\n",
    "\n",
    "#### Diferencia clave:\n",
    "\n",
    "* **Sobel:** Busca cambios bruscos de color de forma puramente matem√°tica.\n",
    "* **MediaPipe:** Busca patrones complejos (la curvatura de un labio, la sombra de un p√°rpado) bas√°ndose en millones de im√°genes previas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936045d-0326-41aa-b0fe-14e70fb364eb",
   "metadata": {},
   "source": [
    "### 2.18 ¬øQu√© son las redes neuronales convolucionales (CNN)?\n",
    "\n",
    "Son un tipo de red neuronal artificial dise√±ada especialmente para analizar im√°genes y videos. B√°sicamente, son modelos que **‚Äúaprenden a ver‚Äù** patrones visuales, igual que lo hace el cerebro humano, pero usando matem√°ticas.\n",
    "\n",
    "#### Aplicaciones comunes:\n",
    "* Reconocimiento facial.\n",
    "* Detecci√≥n de objetos (personas, autos, se√±ales).\n",
    "* Clasificaci√≥n de im√°genes.\n",
    "* Visi√≥n por computadora en medicina, seguridad y rob√≥tica.\n",
    "* Filtros inteligentes en apps y redes sociales.\n",
    "\n",
    "---\n",
    "\n",
    "#### ¬øC√≥mo funcionan?\n",
    "Una CNN procesa una imagen por capas, de lo simple a lo complejo:\n",
    "\n",
    "**1. Capas convolucionales (Detecci√≥n de rasgos b√°sicos)**\n",
    "Aplican filtros (**kernels**) que detectan bordes, l√≠neas y texturas.\n",
    "* **Operaci√≥n:** El Kernel (usualmente de 3x3) se desliza sobre la imagen. En cada posici√≥n, multiplica sus valores por los del p√≠xel y suma el resultado.\n",
    "* **Detecci√≥n:** Si el Kernel est√° dise√±ado para l√≠neas verticales, el resultado ser√° un **mapa de caracter√≠sticas** donde solo resaltan esas l√≠neas.\n",
    "\n",
    "**2. Capas de activaci√≥n (ReLU)**\n",
    "Deciden qu√© informaci√≥n es importante aplicando la regla matem√°tica:\n",
    "$$f(x) = \\max(0, x)$$\n",
    "* **Explicaci√≥n:** Si el resultado es negativo (ruido), lo convierte en 0 (lo apaga). Si es positivo, lo deja pasar. Esto ayuda a que la red se enfoque solo en rasgos que \"brillan\", como el borde de una nariz, eliminando el fondo irrelevante.\n",
    "\n",
    "**3. Capas de Pooling (Simplificaci√≥n)**\n",
    "Reducen el tama√±o de la imagen manteniendo lo m√°s relevante. \n",
    "* **El proceso (Max Pooling):** Divide la imagen en cuadritos (ej. 2x2) y de cada cuadro solo se queda con el n√∫mero m√°s alto.\n",
    "* **¬øPor qu√©?:** El n√∫mero m√°s alto representa la presencia m√°s fuerte de un rasgo (ej. \"aqu√≠ hay una pesta√±a\"). No importa la posici√≥n exacta, sino que el rasgo est√° ah√≠. Esto reduce los datos y hace que la red sea mucho m√°s r√°pida.\n",
    "\n",
    "**4. Capas completamente conectadas (Clasificaci√≥n)**\n",
    "Es la etapa final donde se combinan todos los rasgos aprendidos (bordes + curvas + formas) para dictar qu√© objeto es. Une todo y dice: **\"Es una cara\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddee2b9-9e72-4a3a-9769-48251b97e6bf",
   "metadata": {},
   "source": [
    "### 2.19 Adaptar el programa del punto 2.16 para que funcione en GITHUB\n",
    "\n",
    "#### Paso 1: Preparaci√≥n del entorno local\n",
    "1. **Abrir el explorador de archivos** en la computadora.\n",
    "2. **Crear una carpeta nueva** llamada `Primer Parcial`.\n",
    "3. **Mover el archivo** (aseg√∫rate de que tenga la extensi√≥n `.html` o `.ipynb` seg√∫n corresponda) dentro de esa carpeta. Se recomienda el nombre `Ejercicio_2.16.html`.\n",
    "\n",
    "#### Paso 2: Subida al Repositorio Remoto (GitHub)\n",
    "1. **Acceder a GitHub:** Entrar a la cuenta personal y abrir el repositorio conjunto `MetodosNumericos1`.\n",
    "2. **Cargar archivos:** Hacer clic en el bot√≥n **Add file** (Agregar archivo) y seleccionar la opci√≥n **Upload files** (Subir archivos).\n",
    "3. **Arrastrar carpeta:** Arrastra la carpeta completa `Primer Parcial` desde el explorador de archivos directamente a la zona de carga de GitHub.\n",
    "4. **Confirmar cambios:** En la parte inferior, escribe un mensaje de \"Commit\" (ejemplo: *\"Subida de Ejercicio 2.16 Face Mesh\"*) y presiona el bot√≥n verde **Commit changes**.\n",
    "\n",
    "#### Paso 3: Verificaci√≥n de la Estructura\n",
    "1. **Confirmar ubicaci√≥n:** Verifica que en la ra√≠z del repositorio aparezca ahora la carpeta `Primer Parcial/`.\n",
    "2. **Visualizaci√≥n:** Entra a la carpeta y verifica que los archivos se visualicen correctamente. Si subiste un cuaderno de Jupyter, GitHub mostrar√° el icono correspondiente, confirmando que reconoce el formato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41f39f-44e8-4a36-b490-68c5e12c232e",
   "metadata": {},
   "source": [
    "### 2.20 ¬øC√≥mo se solicita el acceso a la webcam para seguimiento de cara?\n",
    "\n",
    "Para que el sistema de seguimiento facial funcione en un entorno web, es necesario establecer una comunicaci√≥n entre el hardware (c√°mara) y el software (JavaScript). Este proceso se rige por protocolos de seguridad estrictos:\n",
    "\n",
    "* **La API `getUserMedia`:** Es la funci√≥n est√°ndar de JavaScript utilizada para solicitar acceso a flujos de medios. En el c√≥digo, se invoca a trav√©s de la librer√≠a de MediaPipe (`Camera`), la cual gestiona la petici√≥n de permisos al sistema operativo.\n",
    "* **Protocolo HTTPS:** Por seguridad, los navegadores modernos (Chrome, Firefox, Edge) solo permiten el acceso a la webcam en sitios que utilicen conexiones seguras **HTTPS** o en servidores locales (**localhost**). Si intentas ejecutarlo desde un archivo local simple o una URL insegura, la c√°mara no se activar√°.\n",
    "* **Interacci√≥n del Usuario:** El navegador muestra obligatoriamente una ventana emergente con el mensaje *¬øPermitir que [sitio] use su c√°mara?*. Esta es una medida de seguridad que no se puede saltar; si el usuario deniega el permiso, el programa debe estar preparado para capturar el error y notificar que no se puede realizar el seguimiento.\n",
    "* **Configuraci√≥n del flujo:** Al solicitar el acceso, se definen par√°metros como la **resoluci√≥n** (ej. 640 x 480 p√≠xeles) y los **cuadros por segundo (FPS)**. Una vez concedido el permiso, el flujo de video se env√≠a al elemento `<video>` de HTML, que act√∫a como la fuente de datos para que el modelo de IA procese cada fotograma en busca de los 468 puntos faciales.\n",
    "\n",
    "```javascript\n",
    "const camera = new Camera(videoElement, {\n",
    "  onFrame: async () => {\n",
    "    // Env√≠a cada fotograma de la c√°mara al modelo Face Mesh\n",
    "    await faceMesh.send({ image: videoElement });\n",
    "  },\n",
    "  width: 640,\n",
    "  height: 480\n",
    "});\n",
    "\n",
    "// Activa la c√°mara y dispara la solicitud de permisos en el navegador\n",
    "camera.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929673b-fb67-4862-bf38-e2db630f2094",
   "metadata": {},
   "source": [
    "#### Explicaci√≥n del proceso:\n",
    "\n",
    "* **Instanciaci√≥n (`new Camera`):** Se crea un objeto que vincula el elemento `<video>` de HTML con la c√°mara f√≠sica del dispositivo.\n",
    "* **Configuraci√≥n de dimensiones:** Se definen los par√°metros de ancho (`width`) y alto (`height`). El navegador intentar√° ajustar la c√°mara a esta resoluci√≥n si el hardware lo permite.\n",
    "* **El m√©todo `.start()`:** Esta es la instrucci√≥n que dispara la ventana de permisos del navegador. Es una promesa as√≠ncrona; es decir, el programa espera a que el usuario haga clic en **\"Permitir\"** antes de continuar con la ejecuci√≥n.\n",
    "* **Bucle de captura (`onFrame`):** Una vez otorgado el acceso, esta funci√≥n se ejecuta autom√°ticamente en cada fotograma, enviando la imagen de la c√°mara al modelo de IA (`faceMesh.send`) para el rastreo de los puntos faciales en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e987ca-9d37-4fd0-be97-e7a96a793c23",
   "metadata": {},
   "source": [
    "### 2.21 Construir el mismo programa para editar la m√°scara y colocar bigote, sombrero, un lunar etc.\n",
    "\n",
    "Este programa utiliza los puntos de referencia de **MediaPipe Face Mesh** para calcular la posici√≥n, el tama√±o y la inclinaci√≥n de accesorios virtuales sobre el rostro en tiempo real.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>FaceMesh AR ‚Äì Filtros Interactivos</title>\n",
    "    \n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js](https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js)\"></script>\n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js](https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js)\"></script>\n",
    "    \n",
    "    <style>\n",
    "        body { margin: 0; overflow: hidden; background: black; font-family: sans-serif; }\n",
    "        video, canvas {\n",
    "            position: absolute;\n",
    "            top: 0; left: 0;\n",
    "            width: 100vw; height: 100vh;\n",
    "            object-fit: cover;\n",
    "        }\n",
    "        .panel {\n",
    "            position: fixed;\n",
    "            top: 10px; left: 10px;\n",
    "            z-index: 10;\n",
    "            background: rgba(0, 0, 0, 0.7);\n",
    "            padding: 15px;\n",
    "            border-radius: 12px;\n",
    "            border: 1px solid #2ecc71;\n",
    "        }\n",
    "        button {\n",
    "            display: block;\n",
    "            width: 180px;\n",
    "            margin: 8px 0;\n",
    "            padding: 8px;\n",
    "            cursor: pointer;\n",
    "            background: #273c75;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            transition: 0.3s;\n",
    "        }\n",
    "        button:hover { background: #192a56; }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div class=\"panel\">\n",
    "        <button onclick=\"toggle('glasses')\">üëì Lentes</button>\n",
    "        <button onclick=\"toggle('mustache')\">ü•∏ Bigote</button>\n",
    "        <button onclick=\"toggle('hat')\">üé© Sombrero</button>\n",
    "        <button onclick=\"toggle('mole')\">üü§ Lunar</button>\n",
    "        <button onclick=\"toggle('points')\">üü¢ Mostrar FaceMesh</button>\n",
    "    </div>\n",
    "\n",
    "    <video id=\"video\" autoplay muted playsinline></video>\n",
    "    <canvas id=\"canvas\"></canvas>\n",
    "\n",
    "    <script>\n",
    "        const video = document.getElementById(\"video\");\n",
    "        const canvas = document.getElementById(\"canvas\");\n",
    "        const ctx = canvas.getContext(\"2d\");\n",
    "\n",
    "        function resize() {\n",
    "            canvas.width = window.innerWidth;\n",
    "            canvas.height = window.innerHeight;\n",
    "        }\n",
    "        resize();\n",
    "        window.onresize = resize;\n",
    "\n",
    "        const filters = {\n",
    "            glasses: false, mustache: false, hat: false, mole: false, points: false\n",
    "        };\n",
    "\n",
    "        const faceMesh = new FaceMesh({\n",
    "            locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`\n",
    "        });\n",
    "\n",
    "        faceMesh.setOptions({\n",
    "            maxNumFaces: 1,\n",
    "            refineLandmarks: true,\n",
    "            staticImageMode: false\n",
    "        });\n",
    "\n",
    "        faceMesh.onResults(draw);\n",
    "\n",
    "        function draw(res) {\n",
    "            ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "            if (res.image) {\n",
    "                ctx.drawImage(res.image, 0, 0, canvas.width, canvas.height);\n",
    "            }\n",
    "\n",
    "            if (!res.multiFaceLandmarks || res.multiFaceLandmarks.length === 0) return;\n",
    "            const f = res.multiFaceLandmarks[0];\n",
    "\n",
    "            if (filters.points) {\n",
    "                ctx.fillStyle = \"lime\";\n",
    "                f.forEach(p => {\n",
    "                    ctx.beginPath();\n",
    "                    ctx.arc(p.x * canvas.width, p.y * canvas.height, 1.5, 0, Math.PI * 2);\n",
    "                    ctx.fill();\n",
    "                });\n",
    "            }\n",
    "            drawFilters(f);\n",
    "        }\n",
    "\n",
    "        function drawFilters(f) {\n",
    "            // Puntos clave: le (ojo izq), re (ojo der), lip (labio sup), cheek (mejilla)\n",
    "            const le = f[33], re = f[263], lip = f[13], cheek = f[50];\n",
    "            const cx = (le.x + re.x) / 2 * canvas.width;\n",
    "            const cy = (le.y + re.y) / 2 * canvas.height;\n",
    "            const d = Math.abs(le.x - re.x) * canvas.width; // Distancia focal entre ojos\n",
    "\n",
    "            if (filters.glasses) {\n",
    "                ctx.strokeStyle = \"black\"; ctx.lineWidth = 4;\n",
    "                ctx.strokeRect(cx - d * 0.7, cy - d * 0.25, d * 0.6, d * 0.35);\n",
    "                ctx.strokeRect(cx + d * 0.1, cy - d * 0.25, d * 0.6, d * 0.35);\n",
    "                ctx.beginPath();\n",
    "                ctx.moveTo(cx - d * 0.1, cy); ctx.lineTo(cx + d * 0.1, cy);\n",
    "                ctx.stroke();\n",
    "            }\n",
    "\n",
    "            if (filters.mustache) {\n",
    "                const mx = lip.x * canvas.width;\n",
    "                const my = lip.y * canvas.height - d * 0.15;\n",
    "                ctx.strokeStyle = \"#2a1a12\"; ctx.lineWidth = d * 0.18; ctx.lineCap = \"round\";\n",
    "                ctx.beginPath();\n",
    "                ctx.moveTo(mx, my);\n",
    "                ctx.bezierCurveTo(mx - d * 0.3, my - d * 0.2, mx - d, my + d * 0.3, mx - d * 1.1, my + d * 0.4);\n",
    "                ctx.moveTo(mx, my);\n",
    "                ctx.bezierCurveTo(mx + d * 0.3, my - d * 0.2, mx + d, my + d * 0.3, mx + d * 1.1, my + d * 0.4);\n",
    "                ctx.stroke();\n",
    "            }\n",
    "\n",
    "            if (filters.mole) {\n",
    "                ctx.fillStyle = \"#3b2b1f\";\n",
    "                ctx.beginPath();\n",
    "                ctx.arc(cheek.x * canvas.width + 12, cheek.y * canvas.height, 5, 0, Math.PI * 2);\n",
    "                ctx.fill();\n",
    "            }\n",
    "\n",
    "            if (filters.hat) {\n",
    "                ctx.fillStyle = \"#6b4a2b\";\n",
    "                ctx.fillRect(cx - d * 1.2, cy - d * 1.25, d * 2.4, d * 0.35); // Ala\n",
    "                ctx.fillRect(cx - d * 0.5, cy - d * 1.9, d, d * 0.65);        // Copa\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function toggle(f) { filters[f] = !filters[f]; }\n",
    "\n",
    "        const camera = new Camera(video, {\n",
    "            onFrame: async () => { await faceMesh.send({ image: video }); }\n",
    "        });\n",
    "\n",
    "        navigator.mediaDevices.getUserMedia({ video: true }).then(stream => {\n",
    "            video.srcObject = stream;\n",
    "            camera.start();\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e313f2f-5e26-45c0-a248-7082cbfdef9c",
   "metadata": {},
   "source": [
    "### 2.22 Escribir el mismo concepto, pero usando Sobel\n",
    "\n",
    "Este programa integra la detecci√≥n de rostros en tiempo real con un filtro de procesamiento de imagen manual (Sobel). El algoritmo recorre cada p√≠xel del canvas, calcula el gradiente de intensidad y resalta los bordes despu√©s de haber dibujado los accesorios de Realidad Aumentada.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>FaceMesh + Filtro Sobel</title>\n",
    "    \n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js](https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js)\"></script>\n",
    "    <script src=\"[https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js](https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js)\"></script>\n",
    "    \n",
    "    <style>\n",
    "        body { margin: 0; overflow: hidden; background: black; font-family: sans-serif; }\n",
    "        video, canvas { position: absolute; width: 100vw; height: 100vh; object-fit: cover; }\n",
    "        #ui {\n",
    "            position: fixed; top: 10px; left: 10px; z-index: 10;\n",
    "            background: rgba(0, 0, 0, 0.7); padding: 15px; border-radius: 10px;\n",
    "            border: 1px solid #3498db;\n",
    "        }\n",
    "        button { \n",
    "            display: block; width: 200px; margin: 5px 0; padding: 8px; \n",
    "            background: #2980b9; color: white; border: none; border-radius: 4px;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        button:hover { background: #3498db; }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div id=\"ui\">\n",
    "        <button onclick=\"toggle('glasses')\">üëì Lentes</button>\n",
    "        <button onclick=\"toggle('mustache')\">ü•∏ Bigote</button>\n",
    "        <button onclick=\"toggle('hat')\">üé© Sombrero</button>\n",
    "        <button onclick=\"toggle('mole')\">üü§ Lunar</button>\n",
    "        <button onclick=\"toggle('points')\">üü¢ FaceMesh</button>\n",
    "        <button onclick=\"toggle('sobel')\" style=\"background: #e67e22;\">üß† Activar Sobel</button>\n",
    "    </div>\n",
    "\n",
    "    <video id=\"video\" autoplay muted playsinline></video>\n",
    "    <canvas id=\"canvas\"></canvas>\n",
    "\n",
    "    <script>\n",
    "        const video = document.getElementById(\"video\");\n",
    "        const canvas = document.getElementById(\"canvas\");\n",
    "        const ctx = canvas.getContext(\"2d\");\n",
    "\n",
    "        function resize() {\n",
    "            canvas.width = window.innerWidth;\n",
    "            canvas.height = window.innerHeight;\n",
    "        }\n",
    "        resize();\n",
    "        window.onresize = resize;\n",
    "\n",
    "        const filters = {\n",
    "            glasses: false, mustache: false, hat: false, mole: false, points: false, sobel: false\n",
    "        };\n",
    "\n",
    "        const faceMesh = new FaceMesh({\n",
    "            locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`\n",
    "        });\n",
    "\n",
    "        faceMesh.setOptions({\n",
    "            maxNumFaces: 1,\n",
    "            refineLandmarks: true,\n",
    "            staticImageMode: false\n",
    "        });\n",
    "\n",
    "        faceMesh.onResults(drawResults);\n",
    "\n",
    "        function drawResults(res) {\n",
    "            ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "            ctx.drawImage(res.image, 0, 0, canvas.width, canvas.height);\n",
    "\n",
    "            if (res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0) {\n",
    "                const f = res.multiFaceLandmarks[0];\n",
    "\n",
    "                if (filters.points) {\n",
    "                    ctx.fillStyle = \"lime\";\n",
    "                    f.forEach(p => {\n",
    "                        ctx.beginPath();\n",
    "                        ctx.arc(p.x * canvas.width, p.y * canvas.height, 1.5, 0, Math.PI * 2);\n",
    "                        ctx.fill();\n",
    "                    });\n",
    "                }\n",
    "                drawFilters(f);\n",
    "            }\n",
    "\n",
    "            // Aplicar Sobel al final para procesar tanto la c√°mara como los accesorios\n",
    "            if (filters.sobel) applySobel();\n",
    "        }\n",
    "\n",
    "        function toggle(f) { filters[f] = !filters[f]; }\n",
    "\n",
    "        function drawFilters(f) {\n",
    "            const le = f[33], re = f[263], lip = f[13], cheek = f[50];\n",
    "            const cx = (le.x + re.x) / 2 * canvas.width;\n",
    "            const cy = (le.y + re.y) / 2 * canvas.height;\n",
    "            const d = Math.abs(le.x - re.x) * canvas.width;\n",
    "\n",
    "            if (filters.glasses) {\n",
    "                ctx.strokeStyle = \"black\"; ctx.lineWidth = 4;\n",
    "                ctx.strokeRect(cx - d * 0.7, cy - d * 0.25, d * 0.6, d * 0.35);\n",
    "                ctx.strokeRect(cx + d * 0.1, cy - d * 0.25, d * 0.6, d * 0.35);\n",
    "            }\n",
    "            if (filters.mustache) {\n",
    "                const mx = lip.x * canvas.width, my = lip.y * canvas.height - d * 0.15;\n",
    "                ctx.strokeStyle = \"#2a1a12\"; ctx.lineWidth = d * 0.18; ctx.lineCap = \"round\";\n",
    "                ctx.beginPath();\n",
    "                ctx.moveTo(mx, my);\n",
    "                ctx.bezierCurveTo(mx - d * 0.3, my - d * 0.2, mx - d, my + d * 0.3, mx - d * 1.1, my + d * 0.4);\n",
    "                ctx.stroke();\n",
    "            }\n",
    "            if (filters.hat) {\n",
    "                ctx.fillStyle = \"#6b4a2b\";\n",
    "                ctx.fillRect(cx - d * 1.2, cy - d * 1.25, d * 2.4, d * 0.35);\n",
    "                ctx.fillRect(cx - d * 0.5, cy - d * 1.9, d, d * 0.65);\n",
    "            }\n",
    "            if (filters.mole) {\n",
    "                ctx.fillStyle = \"#3b2b1f\";\n",
    "                ctx.beginPath();\n",
    "                ctx.arc(cheek.x * canvas.width + 12, cheek.y * canvas.height, 5, 0, Math.PI * 2);\n",
    "                ctx.fill();\n",
    "            }\n",
    "        }\n",
    "\n",
    "        /* ALGORITMO DE SOBEL (Detecci√≥n de bordes) */\n",
    "        function applySobel() {\n",
    "            const img = ctx.getImageData(0, 0, canvas.width, canvas.height);\n",
    "            const d = img.data, w = img.width;\n",
    "            const gx = [-1, 0, 1, -2, 0, 2, -1, 0, 1];\n",
    "            const gy = [-1, -2, -1, 0, 0, 0, 1, 2, 1];\n",
    "            const out = new Uint8ClampedArray(d.length);\n",
    "\n",
    "            for (let i = 0; i < d.length; i += 4) {\n",
    "                let sx = 0, sy = 0;\n",
    "                for (let k = 0; k < 9; k++) {\n",
    "                    const p = i + ((k % 3) - 1) * 4 + (Math.floor(k / 3) - 1) * w * 4;\n",
    "                    if (p < 0 || p >= d.length) continue;\n",
    "                    // Conversi√≥n a gris para el c√°lculo del gradiente\n",
    "                    const g = d[p] * 0.3 + d[p + 1] * 0.59 + d[p + 2] * 0.11;\n",
    "                    sx += g * gx[k]; sy += g * gy[k];\n",
    "                }\n",
    "                const m = Math.min(255, Math.sqrt(sx * sx + sy * sy));\n",
    "                out[i] = out[i + 1] = out[i + 2] = m; \n",
    "                out[i + 3] = 255; // Opacidad total\n",
    "            }\n",
    "            img.data.set(out);\n",
    "            ctx.putImageData(img, 0, 0);\n",
    "        }\n",
    "\n",
    "        const camera = new Camera(video, {\n",
    "            onFrame: async () => { await faceMesh.send({ image: video }); }\n",
    "        });\n",
    "\n",
    "        navigator.mediaDevices.getUserMedia({ video: true }).then(s => {\n",
    "            video.srcObject = s;\n",
    "            camera.start();\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831d5b1-f8cb-4054-9a61-9169ea3af983",
   "metadata": {},
   "source": [
    "# 3. Conclusiones\n",
    "\n",
    "A lo largo de esta documentaci√≥n se analiz√≥ el funcionamiento y la relaci√≥n entre herramientas modernas de visi√≥n por computadora como **MindAR**, **OpenCV** y **MediaPipe Face Mesh**, junto con algoritmos cl√°sicos como **Canny** y **Sobel**, permitiendo comprender tanto su base matem√°tica como su aplicaci√≥n pr√°ctica en entornos web.\n",
    "\n",
    "Se concluye que los algoritmos tradicionales de procesamiento de im√°genes, como **Sobel y Canny**, est√°n fuertemente fundamentados en conceptos de **c√°lculo num√©rico**, espec√≠ficamente en el uso de derivadas y diferencias finitas, lo que demuestra la importancia de los m√©todos num√©ricos en la detecci√≥n de bordes y an√°lisis de im√°genes digitales. Estos algoritmos siguen siendo relevantes como t√©cnicas de preprocesamiento y como base conceptual para sistemas m√°s avanzados.\n",
    "\n",
    "Por otro lado, herramientas modernas como **MediaPipe Face Mesh y MindAR** representan una evoluci√≥n del procesamiento de im√°genes al incorporar **redes neuronales convolucionales (CNN)**, las cuales aprenden autom√°ticamente patrones visuales complejos sin depender directamente de operadores cl√°sicos. Aun as√≠, estas redes se apoyan impl√≠citamente en los mismos principios b√°sicos de detecci√≥n de bordes y extracci√≥n de caracter√≠sticas.\n",
    "\n",
    "Asimismo, se comprob√≥ que es posible implementar soluciones completas de visi√≥n por computadora directamente en el navegador utilizando **JavaScript**, combinando tecnolog√≠as como **WebAssembly, Canvas, WebGL** y acceso a la webcam, lo que permite desarrollar aplicaciones de realidad aumentada accesibles, multiplataforma y en tiempo real sin necesidad de software adicional.\n",
    "\n",
    "Finalmente, este trabajo permiti√≥ integrar teor√≠a matem√°tica, programaci√≥n y aplicaciones pr√°cticas, demostrando que el procesamiento de im√°genes es un √°rea multidisciplinaria donde los conceptos de **m√©todos num√©ricos, algoritmos cl√°sicos e inteligencia artificial** convergen para resolver problemas reales en la ingenier√≠a de software y sistemas computacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b73749-c92d-4f81-92c4-0f931caf623b",
   "metadata": {},
   "source": [
    "# 4. Referencias\n",
    "\n",
    "* **MindAR Docs:** [https://hiukim.github.io/mind-ar-js-doc](https://hiukim.github.io/mind-ar-js-doc)\n",
    "* **OpenCV Docs:** [https://docs.opencv.org](https://docs.opencv.org)\n",
    "* **MediaPipe:** [https://google.github.io/mediapipe](https://google.github.io/mediapipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
