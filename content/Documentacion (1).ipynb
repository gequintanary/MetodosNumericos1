{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4b9882e9-3534-4819-9462-a1cb9df0d857",
      "cell_type": "markdown",
      "source": "# Algoritmo de Sobel\n\n## 1. Sustento Matem√°tico\nEl operador de Sobel calcula una aproximaci√≥n discreta del **gradiente** de la intensidad de una imagen. Este m√©todo est√° fundamentado en el concepto de **diferencias finitas** para aproximar las derivadas parciales de la funci√≥n de imagen.\n\n### 1.1 Definici√≥n del Gradiente\nPara una funci√≥n continua $f(x, y)$, el gradiente se define como un vector que apunta en la direcci√≥n de mayor cambio de intensidad:\n\n$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n\n### 1.2 Magnitud del Gradiente\nLa magnitud del gradiente representa la \"fuerza\" del borde y se calcula mediante la norma del vector:\n\n$$|\\nabla f| = \\sqrt{\\left( \\frac{\\partial f}{\\partial x} \\right)^2 + \\left( \\frac{\\partial f}{\\partial y} \\right)^2}$$",
      "metadata": {}
    },
    {
      "id": "6763bc84-fdcd-426c-8be4-e33f43a3714c",
      "cell_type": "markdown",
      "source": "## 2. Relaci√≥n con M√©todos Num√©ricos: Diferencias Finitas\n\n### 2.1 Aproximaci√≥n de Derivadas\nEl m√©todo de Sobel utiliza **diferencias finitas ponderadas**, una t√©cnica cl√°sica de m√©todos num√©ricos para aproximar derivadas cuando solo disponemos de datos discretos (como los p√≠xeles de una imagen).\n\n* **Derivada Parcial en X (direcci√≥n horizontal):**\n    Representa el cambio de intensidad a lo largo de las columnas:\n    $$\\frac{\\partial f}{\\partial x} \\approx G_x = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix} * I$$\n\n* **Derivada Parcial en Y (direcci√≥n vertical):**\n    Representa el cambio de intensidad a lo largo de las filas:\n    $$\\frac{\\partial f}{\\partial y} \\approx G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix} * I$$\n\n### 2.2 Interpretaci√≥n como Operador de Diferenciaci√≥n\n| Aspecto Num√©rico | Aplicaci√≥n en Sobel |\n| :--- | :--- |\n| **Diferencias centrales** | M√°scara con valores $-1, 0, +1$ |\n| **Suavizado (regularizaci√≥n)** | Coeficientes $1, 2, 1$ (promedio ponderado) |\n| **Convoluci√≥n discreta** | Operador aplicado sobre vecindad $3 \\times 3$ |",
      "metadata": {}
    },
    {
      "id": "3e21483a-d68b-45e4-8fb3-9c39ba897b2e",
      "cell_type": "markdown",
      "source": "## 3. Pseudoc√≥digo del Algoritmo\n\n### ALGORITMO SOBEL(I: imagen de entrada)\n\n**ENTRADA:** $I[m][n]$ (matriz de intensidades en escala de grises)  \n**SALIDA:** $G[m][n]$ (Magnitud del gradiente), $\\theta[m][n]$ (Direcci√≥n del gradiente)\n\n**PASOS:**\n\n1. **INICIALIZAR:** Matrices $G_x, G_y, G, \\theta$ con ceros (del mismo tama√±o que $I$).\n2. **DEFINIR** m√°scaras de convoluci√≥n:\n\n   **M√°scara X:**\n   $$\\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}$$\n\n   **M√°scara Y:**\n   $$\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}$$\n\n3. **PARA** cada p√≠xel $(i, j)$ donde $1 \\leq i \\leq m-2$ y $1 \\leq j \\leq n-2$:\n   \n   a. **Extraer** vecindad $3 \\times 3$ centrada en $(i, j)$: $V = I[i-1:i+2, j-1:j+2]$\n   \n   b. **Calcular** gradiente horizontal: $G_x[i][j] = \\sum (V \\odot MascaraX)$\n   \n   c. **Calcular** gradiente vertical: $G_y[i][j] = \\sum (V \\odot MascaraY)$\n   \n   d. **Calcular** magnitud: $G[i][j] = \\sqrt{G_x[i][j]^2 + G_y[i][j]^2}$\n   \n   e. **Calcular** direcci√≥n (opcional): $\\theta[i][j] = \\arctan2(G_y[i][j], G_x[i][j])$\n\n4. **NORMALIZAR** $G$ al rango $[0, 255]$ si es necesario.\n5. **RETORNAR** $G, \\theta$",
      "metadata": {}
    },
    {
      "id": "b9196316-9850-4dbf-b7f7-90fd05ad30ba",
      "cell_type": "markdown",
      "source": "## 4. Desarrollo Te√≥rico (Series de Taylor)\n\nPara fundamentar el error de aproximaci√≥n, analizamos las expansiones de Taylor:\n\n### Para Diferencias Progresivas\n$$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}{2}f''(x) + O(h^2)$$\n$$\\underbrace{Error: O(h)}_{\\text{Primer orden}}$$\n\n### Para Diferencias Regresivas\n$$f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x) - f(x-h)}{h} + \\frac{h}{2}f''(x) + O(h^2)$$\n$$\\underbrace{Error: O(h)}_{\\text{Primer orden}}$$\n\n### Para Diferencias Centrales (Sobel)\nRestando las expansiones de Taylor anteriores:\n$$f(x+h) - f(x-h) = 2hf'(x) + O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x+h) - f(x-h)}{2h} + O(h^2)$$\n$$\\underbrace{Error: O(h^2)}_{\\text{Segundo orden (M√°s preciso)}}$$\n",
      "metadata": {}
    },
    {
      "id": "87cd8818-a918-46ec-96a9-854eba0f8de8",
      "cell_type": "markdown",
      "source": "## 5. Comparaci√≥n de Operadores\n\nDependiendo del m√©todo num√©rico elegido para aproximar la derivada, las m√°scaras (kernels) resultantes cambian su estructura y precisi√≥n:\n\n### Sobel con Diferencias Regresivas (No est√°ndar)\nEsta versi√≥n utiliza una aproximaci√≥n hacia atr√°s. Aunque es computacionalmente sencilla, no es la m√°s utilizada porque tiende a desplazar visualmente la posici√≥n de los bordes en la imagen:\n\n$$\\begin{bmatrix} -1 & 1 & 0 \\\\ -2 & 2 & 0 \\\\ -1 & 1 & 0 \\end{bmatrix}$$\n\n### Sobel Est√°ndar (Diferencias Centrales)\nEs el est√°ndar de la industria y el que se implementa en la mayor√≠a de las librer√≠as. Al usar diferencias centrales, logra una mejor simetr√≠a y un error de segundo orden $O(h^2)$:\n\n**Gradiente en X ($G_x$):**\n$$G_x = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}$$\n\n**Gradiente en Y ($G_y$):**\n$$G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}$$",
      "metadata": {}
    },
    {
      "id": "42c16de8-afd8-4525-a1b6-82971edd0384",
      "cell_type": "markdown",
      "source": "## 6. Interpretaci√≥n Geom√©trica\n\nDesde una perspectiva geom√©trica, el algoritmo de Sobel no solo detecta cambios, sino que interpreta la imagen como una **superficie continua de intensidades**, donde cada p√≠xel tiene una \"altura\" basada en su valor de gris.\n\n### 1. El Plano Tangente\nAl calcular $G_x$ y $G_y$, el algoritmo est√° estimando la inclinaci√≥n de un **plano tangente** a la superficie de la imagen en el punto $(i, j)$. \n* $G_x$ representa la pendiente del plano en direcci√≥n este-oeste.\n* $G_y$ representa la pendiente del plano en direcci√≥n norte-sur.\n\n### 2. El Vector Gradiente\nEl vector $\\nabla f = (G_x, G_y)$ es perpendicular a las l√≠neas de contorno de la imagen. \n* Su **direcci√≥n** $\\theta$ apunta hacia donde la intensidad crece m√°s r√°pidamente.\n* Su **magnitud** $|G|$ indica qu√© tan \"empinada\" es la pendiente. Un borde es, simplemente, una zona con una pendiente muy pronunciada.\n\n### 3. Visualizaci√≥n de Bordes\nEn la pr√°ctica, los bordes detectados corresponden a los lugares donde la superficie de intensidad tiene su m√°xima tasa de cambio. Geom√©tricamente, el operador de Sobel act√∫a como un **filtro de paso alto**, eliminando las zonas planas (frecuencias bajas) y resaltando las transiciones abruptas (frecuencias altas).",
      "metadata": {}
    },
    {
      "id": "459688ec-2681-4547-8cbc-a8f692b3394c",
      "cell_type": "code",
      "source": "%%html\n<div id=\"jup-container\" style=\"position: relative; width: 100%; height: 550px; background: #000; overflow: hidden; border-radius: 8px;\">\n    <div id=\"ui\" style=\"position: absolute; top: 10px; left: 10px; z-index: 10; background: rgba(0,0,0,0.6); padding: 10px; border-radius: 10px; display: flex; flex-wrap: wrap; gap: 5px;\">\n        <button onclick=\"toggle('glasses')\" style=\"cursor:pointer; padding: 5px 10px;\">üëì Lentes</button>\n        <button onclick=\"toggle('mustache')\" style=\"cursor:pointer; padding: 5px 10px;\">ü•∏ Bigote</button>\n        <button onclick=\"toggle('hat')\" style=\"cursor:pointer; padding: 5px 10px;\">üé© Sombrero</button>\n        <button onclick=\"toggle('mole')\" style=\"cursor:pointer; padding: 5px 10px;\">üü§ Lunar</button>\n        <button onclick=\"toggle('points')\" style=\"cursor:pointer; padding: 5px 10px;\">üü¢ FaceMesh</button>\n        <button onclick=\"toggle('sobel')\" style=\"cursor:pointer; padding: 5px 10px; background: #ffeb3b; border: none; font-weight: bold;\">üß† Sobel</button>\n    </div>\n\n    <video id=\"video\" style=\"display: none;\" autoplay muted playsinline></video>\n    <canvas id=\"canvas\" style=\"width: 100%; height: 100%; object-fit: cover;\"></canvas>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\"></script>\n\n<script>\n(function() {\n    const video = document.getElementById(\"video\");\n    const canvas = document.getElementById(\"canvas\");\n    const container = document.getElementById(\"jup-container\");\n    const ctx = canvas.getContext(\"2d\");\n\n    function resize(){\n        canvas.width = container.clientWidth;\n        canvas.height = container.clientHeight;\n    }\n    resize();\n    window.addEventListener('resize', resize);\n\n    const filters = {\n        glasses: false, mustache: false, hat: false,\n        mole: false, points: false, sobel: false\n    };\n\n    window.toggle = function(f) {\n        filters[f] = !filters[f];\n    };\n\n    /* ========= FACEMESH ========= */\n    const faceMesh = new FaceMesh({\n        locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`\n    });\n\n    faceMesh.setOptions({\n        maxNumFaces: 1,\n        refineLandmarks: true,\n        staticImageMode: false\n    });\n\n    faceMesh.onResults(drawResults);\n\n    function drawResults(res){\n        ctx.clearRect(0, 0, canvas.width, canvas.height);\n        ctx.drawImage(res.image, 0, 0, canvas.width, canvas.height);\n\n        if(res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0){\n            const f = res.multiFaceLandmarks[0];\n\n            if(filters.points){\n                ctx.fillStyle = \"lime\";\n                f.forEach(p => {\n                    ctx.beginPath();\n                    ctx.arc(p.x * canvas.width, p.y * canvas.height, 2, 0, Math.PI * 2);\n                    ctx.fill();\n                });\n            }\n            drawFilters(f);\n        }\n\n        if(filters.sobel) applySobel();\n    }\n\n    /* ========= FILTROS (L√≥gica Matem√°tica) ========= */\n    function drawFilters(f){\n        const le=f[33], re=f[263], lip=f[13], cheek=f[50];\n        const cx=(le.x+re.x)/2*canvas.width;\n        const cy=(le.y+re.y)/2*canvas.height;\n        const d=Math.abs(le.x-re.x)*canvas.width;\n\n        if(filters.glasses){\n            ctx.strokeStyle=\"black\"; ctx.lineWidth=4;\n            ctx.strokeRect(cx-d*0.7,cy-d*0.25,d*0.6,d*0.35);\n            ctx.strokeRect(cx+d*0.1,cy-d*0.25,d*0.6,d*0.35);\n            ctx.beginPath(); ctx.moveTo(cx-d*0.1,cy); ctx.lineTo(cx+d*0.1,cy); ctx.stroke();\n        }\n\n        if(filters.mustache){\n            const mx=lip.x*canvas.width;\n            const my=lip.y*canvas.height-d*0.15;\n            ctx.strokeStyle=\"#2a1a12\"; ctx.lineWidth=d*0.18; ctx.lineCap=\"round\";\n            ctx.beginPath();\n            ctx.moveTo(mx,my);\n            ctx.bezierCurveTo(mx-d*0.3,my-d*0.2,mx-d,my+d*0.3,mx-d*1.1,my+d*0.4);\n            ctx.moveTo(mx,my);\n            ctx.bezierCurveTo(mx+d*0.3,my-d*0.2,mx+d,my+d*0.3,mx+d*1.1,my+d*0.4);\n            ctx.stroke();\n        }\n\n        if(filters.mole){\n            ctx.fillStyle=\"#3b2b1f\"; ctx.beginPath();\n            ctx.arc(cheek.x*canvas.width+12,cheek.y*canvas.height,5,0,Math.PI*2);\n            ctx.fill();\n        }\n\n        if(filters.hat){\n            ctx.fillStyle=\"#6b4a2b\";\n            ctx.fillRect(cx-d*1.2,cy-d*1.25,d*2.4,d*0.35);\n            ctx.fillRect(cx-d*0.5,cy-d*1.9,d,d*0.65);\n        }\n    }\n\n    /* ========= SOBEL (Algoritmo de Convoluci√≥n) ========= */\n    function applySobel(){\n        const img = ctx.getImageData(0, 0, canvas.width, canvas.height);\n        const d = img.data, w = img.width;\n        const gx = [-1, 0, 1, -2, 0, 2, -1, 0, 1];\n        const gy = [-1, -2, -1, 0, 0, 0, 1, 2, 1];\n        const out = new Uint8ClampedArray(d.length);\n\n        for(let i=0; i<d.length; i+=4){\n            let sx=0, sy=0;\n            for(let k=0; k<9; k++){\n                const col = (k % 3) - 1;\n                const row = Math.floor(k / 3) - 1;\n                const p = i + (col * 4) + (row * w * 4);\n                if(p < 0 || p >= d.length) continue;\n                const gray = d[p] * 0.3 + d[p+1] * 0.59 + d[p+2] * 0.11;\n                sx += gray * gx[k];\n                sy += gray * gy[k];\n            }\n            const m = Math.min(255, Math.sqrt(sx*sx + sy*sy));\n            out[i] = out[i+1] = out[i+2] = m;\n            out[i+3] = 255;\n        }\n        img.data.set(out);\n        ctx.putImageData(img, 0, 0);\n    }\n\n    /* ========= INICIO DE C√ÅMARA ========= */\n    const camera = new Camera(video, {\n        onFrame: async () => {\n            await faceMesh.send({ image: video });\n        },\n        width: 640,\n        height: 480\n    });\n\n    navigator.mediaDevices.getUserMedia({video: true})\n        .then(stream => {\n            video.srcObject = stream;\n            camera.start();\n        })\n        .catch(err => {\n            console.error(\"Error al acceder a la c√°mara: \", err);\n            alert(\"No se pudo acceder a la c√°mara. Aseg√∫rate de dar permisos en el navegador.\");\n        });\n})();\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "274ff0f0-e39d-47e5-9175-2921fa874804",
      "cell_type": "markdown",
      "source": "# Algoritmo de Canny (Canny Edge Detector)\n\n## 1. Sustento Matem√°tico (Criterios de Optimizaci√≥n)\nA diferencia de otros operadores, el detector de bordes de Canny se dise√±√≥ como un problema de optimizaci√≥n matem√°tica. John Canny estableci√≥ tres criterios fundamentales para considerar una detecci√≥n de bordes como \"√≥ptima\":\n\n### 1.1 Criterios de Calidad\n* **Baja Tasa de Error (Detecci√≥n):** El algoritmo debe marcar tantos bordes reales como sea posible y evitar falsos positivos causados por el ruido.\n* **Buena Localizaci√≥n:** La distancia entre el p√≠xel marcado como borde y el centro del borde real debe ser m√≠nima.\n* **Respuesta √önica:** Un solo borde real no debe generar m√∫ltiples bordes detectados (evitar el \"efecto de doble borde\").\n\n### 1.2 Formulaci√≥n Matem√°tica\nPara lograr esto, se busca una funci√≥n $f(x)$ que maximice el producto de la relaci√≥n se√±al-ruido ($SNR$) y la localizaci√≥n ($L$):\n\n$$SNR = \\frac{|\\int_{-W}^{W} G(-x)f(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} f^2(x)dx}}$$\n\n$$L = \\frac{|\\int_{-W}^{W} G'(-x)f'(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} (f')^2(x)dx}}$$\n\nDonde:\n* $G(x)$ representa la se√±al del borde.\n* $f(x)$ es el filtro aplicado.\n* $\\sigma$ es la desviaci√≥n est√°ndar del ruido.",
      "metadata": {}
    },
    {
      "id": "e958b92b-c224-4c66-8664-9e4b933fa1b6",
      "cell_type": "markdown",
      "source": "# Algoritmo de Canny (Canny Edge Detector)\n\n## 1. Sustento Matem√°tico (Criterios de Optimizaci√≥n)\nA diferencia de otros operadores, el detector de bordes de Canny se dise√±√≥ como un problema de optimizaci√≥n matem√°tica. John Canny estableci√≥ tres criterios fundamentales para considerar una detecci√≥n de bordes como \"√≥ptima\":\n\n### 1.1 Criterios de Calidad\n* **Baja Tasa de Error (Detecci√≥n):** El algoritmo debe marcar tantos bordes reales como sea posible y evitar falsos positivos causados por el ruido.\n* **Buena Localizaci√≥n:** La distancia entre el p√≠xel marcado como borde y el centro del borde real debe ser m√≠nima.\n* **Respuesta √önica:** Un solo borde real no debe generar m√∫ltiples bordes detectados (evitar el \"efecto de doble borde\").\n\n### 1.2 Formulaci√≥n Matem√°tica\nPara lograr esto, se busca una funci√≥n $f(x)$ que maximice el producto de la relaci√≥n se√±al-ruido ($SNR$) y la localizaci√≥n ($L$):\n\n$$SNR = \\frac{|\\int_{-W}^{W} G(-x)f(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} f^2(x)dx}}$$\n\n$$L = \\frac{|\\int_{-W}^{W} G'(-x)f'(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} (f')^2(x)dx}}$$\n\nDonde:\n* $G(x)$ representa la se√±al del borde.\n* $f(x)$ es el filtro aplicado.\n* $\\sigma$ es la desviaci√≥n est√°ndar del ruido.",
      "metadata": {}
    },
    {
      "id": "efb873d3-df75-432e-9593-31e27b3cf859",
      "cell_type": "markdown",
      "source": "## 2. Relaci√≥n con M√©todos Num√©ricos: Regularizaci√≥n\n\n### 2.1 Suavizado Gaussiano\nDado que el c√°lculo de derivadas es una operaci√≥n que amplifica el ruido (frecuencias altas), el algoritmo de Canny aplica una etapa de **suavizado** o **regularizaci√≥n** previa. Esto se basa en la convoluci√≥n de la imagen original $I$ con un n√∫cleo (kernel) Gaussiano.\n\nLa funci√≥n Gaussiana en 2D se define como:\n\n$$G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}$$\n\n### 2.2 Discretizaci√≥n del Kernel\nEn m√©todos num√©ricos, aproximamos esta funci√≥n continua mediante una m√°scara discreta. Por ejemplo, para un $\\sigma = 1.4$, un kernel $5 \\times 5$ aproximado es:\n\n$$K = \\frac{1}{273} \\begin{bmatrix} 1 & 4 & 7 & 4 & 1 \\\\ 4 & 16 & 26 & 16 & 4 \\\\ 7 & 26 & 41 & 26 & 7 \\\\ 4 & 16 & 26 & 16 & 4 \\\\ 1 & 4 & 7 & 4 & 1 \\end{bmatrix}$$\n\n### 2.3 Operaci√≥n de Convoluci√≥n\nLa imagen suavizada $I_{suave}$ se obtiene mediante la suma ponderada en la vecindad del p√≠xel:\n\n$$I_{suave}(i, j) = \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I(i-m, j-n) \\cdot K(m, n)$$\n\n> **Importante:** Esta etapa reduce el error en la aproximaci√≥n de las derivadas parciales que se calcular√°n en el siguiente paso.",
      "metadata": {}
    },
    {
      "id": "0ac0c641-7acf-47eb-adf6-55c6a02e87e9",
      "cell_type": "markdown",
      "source": "## 3. C√°lculo de Gradiente y Direcci√≥n\n\nUna vez suavizada la imagen, el algoritmo de Canny utiliza operadores de diferenciaci√≥n (como Sobel) para encontrar las variaciones de intensidad en las direcciones $x$ e $y$.\n\n### 3.1 Componentes del Gradiente\nSe aplican las m√°scaras de diferencias finitas para obtener las derivadas parciales:\n\n* **Gradiente Horizontal:** $G_x = M_x * I_{suave}$\n* **Gradiente Vertical:** $G_y = M_y * I_{suave}$\n\n### 3.2 Magnitud y Orientaci√≥n\nA partir de estas componentes, se calcula la magnitud total del cambio y la direcci√≥n del vector gradiente:\n\n* **Magnitud del Gradiente:**\n    $$M(i, j) = \\sqrt{G_x(i, j)^2 + G_y(i, j)^2}$$\n\n* **√Ångulo de Orientaci√≥n:**\n    $$\\alpha(i, j) = \\arctan\\left(\\frac{G_y(i, j)}{G_x(i, j)}\\right)$$\n\n### 3.3 Cuantizaci√≥n de Direcciones\nPara el procesamiento num√©rico posterior, el √°ngulo $\\alpha$ se redondea a uno de los cuatro sectores de una vecindad $3 \\times 3$:\n\n| √Ångulo Aproximado | Direcci√≥n del Borde |\n| :--- | :--- |\n| **$0^\\circ$** | Horizontal |\n| **$45^\\circ$** | Diagonal Positiva |\n| **$90^\\circ$** | Vertical |\n| **$135^\\circ$** | Diagonal Negativa |",
      "metadata": {}
    },
    {
      "id": "5f6fb268-ef5f-4d99-95ba-7fbc5647c060",
      "cell_type": "markdown",
      "source": "## 4. Supresi√≥n de No M√°ximos (Non-Maximum Suppression)\n\nEste proceso es una t√©cnica de **adelgazamiento de bordes**. Su objetivo es eliminar los p√≠xeles que no forman parte de la cresta del gradiente, dejando √∫nicamente los m√°ximos locales.\n\n### 4.1 L√≥gica Algor√≠tmica\nPara cada p√≠xel $(i, j)$, el algoritmo compara su magnitud $M(i, j)$ con las magnitudes de los dos p√≠xeles vecinos en la direcci√≥n del gradiente $\\alpha(i, j)$.\n\n* Si $M(i, j)$ es la mayor de las tres, el p√≠xel se conserva.\n* Si no es la mayor, se suprime asign√°ndole un valor de $0$.\n\n### 4.2 Esquema de Comparaci√≥n seg√∫n la Direcci√≥n\nDependiendo del √°ngulo cuantizado, se eligen los vecinos espec√≠ficos:\n\n| √Ångulo ($\\alpha$) | Vecino 1 | Vecino 2 | Eje de Comparaci√≥n |\n| :--- | :---: | :---: | :--- |\n| **$0^\\circ$** | $(i, j+1)$ | $(i, j-1)$ | Horizontal |\n| **$45^\\circ$** | $(i-1, j+1)$ | $(i+1, j-1)$ | Diagonal Positiva |\n| **$90^\\circ$** | $(i-1, j)$ | $(i+1, j)$ | Vertical |\n| **$135^\\circ$** | $(i-1, j-1)$ | $(i+1, j+1)$ | Diagonal Negativa |\n\n### 4.3 Resultado Num√©rico\nEl resultado de esta operaci√≥n es una imagen de bordes delgados (frecuentemente llamada *Thin Edges*), definida matem√°ticamente como:\n\n$$M_{thin}(i, j) = \\begin{cases} M(i, j) & \\text{si } M(i, j) > M_{vecinos} \\\\ 0 & \\text{en otro caso} \\end{cases}$$",
      "metadata": {}
    },
    {
      "id": "474c44d5-06da-4150-bd6a-a5e9533d17b6",
      "cell_type": "markdown",
      "source": "## 5. Umbralizaci√≥n con Hist√©resis\n\nEste paso determina qu√© bordes son genuinos y cu√°les deben descartarse. A diferencia de una umbralizaci√≥n simple, Canny utiliza dos niveles de decisi√≥n para mantener la conectividad de los bordes.\n\n### 5.1 Definici√≥n de Umbrales\nSe establecen dos valores cr√≠ticos:\n* **$T_{high}$ (Umbral Superior):** Define los bordes de alta confianza.\n* **$T_{low}$ (Umbral Inferior):** Permite la inclusi√≥n de bordes m√°s d√©biles siempre que tengan sustento estructural.\n\n### 5.2 Clasificaci√≥n de P√≠xeles\nPara cada p√≠xel de la imagen resultante del paso anterior, se aplica la siguiente l√≥gica:\n\n| Categor√≠a | Condici√≥n de Magnitud ($M$) | Decisi√≥n Final |\n| :--- | :--- | :--- |\n| **Borde Fuerte** | $M \\geq T_{high}$ | Se conserva (P√≠xel de borde definitivo) |\n| **Borde D√©bil** | $T_{low} \\leq M < T_{high}$ | Se conserva **solo si** est√° conectado a uno fuerte |\n| **No-Borde** | $M < T_{low}$ | Se elimina (Valor 0) |\n\n### 5.3 An√°lisis de Conectividad (Hysteresis)\nMatem√°ticamente, un p√≠xel d√©bil $W$ en la posici√≥n $(i, j)$ se convierte en borde si existe un p√≠xel fuerte $S$ en su vecindad de Moore ($8$ vecinos):\n\n$$B(i, j) = \\begin{cases} 1 & \\text{si } M(i, j) \\geq T_{high} \\\\ 1 & \\text{si } T_{low} \\leq M(i, j) < T_{high} \\text{ y } \\exists \\text{ fuerte en vecindad} \\\\ 0 & \\text{en otro caso} \\end{cases}$$\n\n> **Ventaja:** Este m√©todo evita que un borde se \"rompa\" debido a peque√±as variaciones de intensidad a lo largo de su trayectoria.",
      "metadata": {}
    },
    {
      "id": "652065e7-f8e1-47a5-b2a4-6b8df2f631fd",
      "cell_type": "markdown",
      "source": "## 6. Pseudoc√≥digo del Algoritmo de Canny\n\n**ALGORITMO CANNY_EDGE_DETECTOR($I$, $\\sigma$, $T_{low}$, $T_{high}$)**\n\n**ENTRADA:** * $I$: Imagen original en escala de grises.  \n* $\\sigma$: Desviaci√≥n est√°ndar para el suavizado Gaussiano.  \n* $T_{low}, T_{high}$: Umbrales para la hist√©resis.\n\n**SALIDA:** * $E$: Imagen binaria con los bordes detectados.\n\n**PASOS:**\n\n1. **REDUCCI√ìN DE RUIDO:** Aplicar filtro Gaussiano de tama√±o $(2k+1) \\times (2k+1)$ para obtener $I_{suave}$.  \n   $$I_{suave} = I * G_{\\sigma}$$\n\n2. **C√ÅLCULO DE GRADIENTES:** Aplicar m√°scaras de Sobel para obtener derivadas parciales $G_x$ y $G_y$.  \n   Calcular magnitud $M$ y direcci√≥n $\\alpha$ para cada p√≠xel.\n\n3. **SUPRESI√ìN DE NO M√ÅXIMOS:** **PARA** cada p√≠xel $(i, j)$ en $M$:  \n   * Determinar direcci√≥n del gradiente cuantizada (0, 45, 90, 135).  \n   * Comparar $M(i, j)$ con vecinos en dicha direcci√≥n.  \n   * **SI** $M(i, j)$ no es el m√°ximo local, $M_{thin}(i, j) = 0$.\n\n4. **UMBRALIZACI√ìN DOBLE:** Clasificar p√≠xeles en $M_{thin}$ como **Fuertes**, **D√©biles** o **Suprimidos** seg√∫n $T_{low}$ y $T_{high}$.\n\n5. **SEGUIMIENTO DE BORDES (HISTERESIS):** **PARA** cada p√≠xel d√©bil:  \n   * **SI** est√° conectado a un p√≠xel fuerte (vecindad de 8), marcar como **Borde**.  \n   * **SINO**, marcar como **Fondo**.\n\n6. **RETORNAR** Imagen binaria final $E$.",
      "metadata": {}
    },
    {
      "id": "963894ad-1b42-4468-b87b-13a4baceb1d5",
      "cell_type": "markdown",
      "source": "## 7. Interpretaci√≥n Geom√©trica y Comparativa\n\n### 7.1 La Geometr√≠a de la Cresta\nDesde una perspectiva geom√©trica, mientras que Sobel identifica una \"zona de cambio\" (una rampa de intensidad), Canny busca la **l√≠nea de cresta** exacta de esa rampa. La supresi√≥n de no m√°ximos act√∫a como una operaci√≥n de proyecci√≥n que colapsa la pendiente del gradiente en un solo punto geom√©trico de m√°xima curvatura.\n\n### 7.2 Comparativa: Sobel vs. Canny\n\n| Caracter√≠stica | Operador de Sobel | Detector de Canny |\n| :--- | :--- | :--- |\n| **Complejidad** | Baja (1 solo paso) | Alta (m√∫ltiples etapas) |\n| **Grosor del Borde** | Grueso (varios p√≠xeles) | Delgado (exactamente 1 p√≠xel) |\n| **Sensibilidad al Ruido** | Alta | Baja (gracias al suavizado previo) |\n| **Continuidad** | Bordes suelen presentar huecos | Bordes continuos (por hist√©resis) |\n| **Precisi√≥n Matem√°tica** | $O(h^2)$ en derivaci√≥n | √ìptima seg√∫n criterios de SNR y Localizaci√≥n |\n\n### 7.3 Conclusi√≥n\nEl m√©todo de Canny representa la evoluci√≥n del sustento matem√°tico de las **diferencias finitas**. No se limita a calcular la derivada parcial, sino que aplica una cadena de filtros num√©ricos (Gauss, Sobel, Supresi√≥n y Conectividad) para transformar una se√±al ruidosa en una representaci√≥n geom√©trica precisa de los bordes.",
      "metadata": {}
    },
    {
      "id": "8be91f3b-11c5-4514-98f9-fb673331b034",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6f3fc2c-a4ff-41a0-a79f-f540bc9ec05b",
      "cell_type": "markdown",
      "source": "# A-Frame: Framework de Realidad Virtual y Gr√°ficos 3D\n\n## 1. Sustento Tecnol√≥gico y el Grafo de Escena\n\nA-Frame es un framework web de c√≥digo abierto basado en la arquitectura **Entity-Component-System (ECS)** que se ejecuta sobre WebGL. Desde la perspectiva de los m√©todos num√©ricos, A-Frame es un motor que gestiona un **Grafo de Escena**, donde cada nodo representa una transformaci√≥n espacial.\n\n### 1.1 El Grafo de Escena como Estructura de Datos\nEl sistema organiza los objetos en una jerarqu√≠a de √°rbol. La posici√≥n, rotaci√≥n y escala de cualquier entidad se calculan mediante la acumulaci√≥n de transformaciones de sus ancestros.\n\n### 1.2 Justificaci√≥n Matem√°tica: Transformaciones Homog√©neas\nCualquier entidad $E$ en el espacio 3D se representa mediante una **Matriz de Transformaci√≥n Homog√©nea $4 \\times 4$**. Esto permite unificar traslaci√≥n, rotaci√≥n y escala en una sola operaci√≥n matricial:\n\n$$M = \\begin{bmatrix} \nr_{11} & r_{12} & r_{13} & t_x \\\\ \nr_{21} & r_{22} & r_{23} & t_y \\\\ \nr_{31} & r_{32} & r_{33} & t_z \\\\ \n0 & 0 & 0 & 1 \n\\end{bmatrix}$$\n\nDonde:\n* El subbloque $3 \\times 3$ superior izquierdo gestiona la **Rotaci√≥n ($R$)** y la **Escala ($S$)**.\n* La columna $[t_x, t_y, t_z]^T$ gestiona la **Traslaci√≥n ($T$)**.\n\nLa posici√≥n final de un objeto \"hijo\" ($P_{final}$) respecto al origen global se obtiene mediante la multiplicaci√≥n de matrices de su jerarqu√≠a:\n$$P_{final} = M_{padre} \\cdot M_{hijo} \\cdot P_{local}$$\n\n### 1.3 Coordenadas y Unidades\nA-Frame utiliza un sistema de mano derecha (Right-handed coordinate system):\n* **Eje X:** Positivo hacia la derecha.\n* **Eje Y:** Positivo hacia arriba.\n* **Eje Z:** Positivo hacia afuera de la pantalla (hacia el usuario).\n\n> **Nota de Aplicaci√≥n:** En VR, la unidad de medida est√° estandarizada como $1 \\text{ unidad} = 1 \\text{ metro}$, lo cual es crucial para la precisi√≥n del c√°lculo de la c√°mara y la percepci√≥n de profundidad del usuario.",
      "metadata": {}
    },
    {
      "id": "6396a6c9-e189-41c3-8e4e-057e642968d1",
      "cell_type": "markdown",
      "source": "## 2. Geometr√≠a Anal√≠tica y √Ålgebra Vectorial\n\nEn A-Frame, cada objeto y cada interacci√≥n se define mediante vectores en $\\mathbb{R}^3$. La manipulaci√≥n de estos vectores permite resolver problemas de posicionamiento, orientaci√≥n y detecci√≥n de colisiones.\n\n### 2.1 Representaci√≥n Vectorial\nCualquier punto o direcci√≥n en el espacio se define por un vector columna:\n$$\\vec{v} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$$\n\nPara garantizar c√°lculos precisos (como en el caso de las direcciones de las luces o la vista de la c√°mara), es necesario trabajar con **Vectores Unitarios (Normalizaci√≥n)**:\n$$\\hat{u} = \\frac{\\vec{v}}{\\|\\vec{v}\\|} = \\frac{1}{\\sqrt{x^2 + y^2 + z^2}} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$$\n\n### 2.2 Operaciones Fundamentales y su Aplicaci√≥n Num√©rica\n\n#### A. Producto Punto (Dot Product)\nSe utiliza para calcular el √°ngulo entre dos vectores (como la mirada del usuario y un objeto) y en modelos de iluminaci√≥n:\n$$\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta)$$\n* **Uso:** Si $\\vec{a} \\cdot \\vec{b} > 0$, el objeto est√° frente a la c√°mara; si es $< 0$, est√° detr√°s.\n\n#### B. Producto Cruz (Cross Product)\nEs esencial para calcular la **Normal de una Superficie** ($\\vec{N}$), lo cual define c√≥mo se renderiza una cara 3D:\n$$\\vec{N} = \\vec{v_1} \\times \\vec{v_2} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\end{vmatrix}$$\n* **Justificaci√≥n:** El vector resultante es perpendicular al plano formado por $\\vec{v_1}$ y $\\vec{v_2}$, permitiendo al motor calcular el rebote de la luz.\n\n### 2.3 Interpolaci√≥n Lineal (LERP)\nPara animaciones fluidas entre dos puntos $\\vec{P_0}$ y $\\vec{P_1}$, A-Frame utiliza interpolaci√≥n num√©rica:\n$$\\vec{P}(t) = (1 - t)\\vec{P_0} + t\\vec{P_1}$$\nDonde $t \\in [0, 1]$ representa el paso del tiempo normalizado. Esta es la base de los m√©todos de aproximaci√≥n para trayectorias de movimiento.",
      "metadata": {}
    },
    {
      "id": "55ec38f7-44df-4f6a-9aaf-a0c27a5d3b5e",
      "cell_type": "markdown",
      "source": "## 3. Rotaciones y Representaci√≥n Algebraica: Cuaterniones\n\nEn el espacio 3D de A-Frame, las rotaciones se pueden definir mediante √°ngulos de Euler (Giro en X, Y, Z), pero internamente el motor utiliza **Cuaterniones** para realizar los c√°lculos num√©ricos de forma eficiente y evitar el fen√≥meno del *Gimbal Lock* (bloqueo de rotaci√≥n).\n\n### 3.1 El Problema de los √Ångulos de Euler\nCuando rotamos un objeto usando tres √°ngulos ($\\phi, \\theta, \\psi$), la matriz de rotaci√≥n total es el producto de tres matrices individuales:\n$$R_{total} = R_z(\\psi)R_y(\\theta)R_x(\\phi)$$\n**Justificaci√≥n Num√©rica:** Este m√©todo pierde un grado de libertad cuando dos ejes de rotaci√≥n se alinean (singularidad), lo que impide c√°lculos de trayectoria suaves en sistemas de navegaci√≥n VR.\n\n### 3.2 Soluci√≥n mediante Cuaterniones ($\\mathbb{H}$)\nUn cuaterni√≥n es un n√∫mero complejo en cuatro dimensiones que representa una rotaci√≥n en torno a un eje arbitrario $\\vec{u}$ por un √°ngulo $\\alpha$:\n$$\\mathbf{q} = (s, \\vec{v}) = \\cos\\left(\\frac{\\alpha}{2}\\right) + \\sin\\left(\\frac{\\alpha}{2}\\right)(u_x\\mathbf{i} + u_y\\mathbf{j} + u_z\\mathbf{k})$$\n\nDonde se cumple la identidad fundamental:\n$$\\mathbf{i}^2 = \\mathbf{j}^2 = \\mathbf{k}^2 = \\mathbf{ijk} = -1$$\n\n### 3.3 Esferical Linear Interpolation (SLERP)\nPara animar rotaciones de manera num√©ricamente estable y uniforme, A-Frame utiliza la f√≥rmula de **SLERP**, que interpola entre dos cuaterniones $q_0$ y $q_1$:\n\n$$SLERP(q_0, q_1, t) = \\frac{\\sin((1-t)\\Omega)}{\\sin\\Omega}q_0 + \\frac{\\sin(t\\Omega)}{\\sin\\Omega}q_1$$\n\nDonde:\n* $t \\in [0, 1]$ es el factor de interpolaci√≥n.\n* $\\cos\\Omega = q_0 \\cdot q_1$ (producto escalar de los cuaterniones).\n\n### 3.4 Ventajas en el C√≥mputo Num√©rico\n1. **Menor costo computacional:** Multiplicar dos cuaterniones requiere menos operaciones de punto flotante que multiplicar dos matrices $3 \\times 3$.\n2. **Normalizaci√≥n:** Para que un cuaterni√≥n represente una rotaci√≥n v√°lida, debe ser un cuaterni√≥n unitario:\n$$\\|\\mathbf{q}\\| = \\sqrt{s^2 + x^2 + y^2 + z^2} = 1$$\nEsto permite corregir errores de redondeo num√©rico acumulados simplemente renormalizando el vector resultante.",
      "metadata": {}
    },
    {
      "id": "c9f0e7e2-d7f2-454d-8ffd-1b838e23deeb",
      "cell_type": "markdown",
      "source": "## 4. El Pipeline de Renderizado: De 3D a la Pantalla 2D\n\nPara que A-Frame pueda mostrar una escena en un monitor o en un visor de VR, debe resolver una serie de ecuaciones que proyectan los puntos del espacio euclidiano $\\mathbb{R}^3$ al plano de la pantalla $\\mathbb{R}^2$.\n\n### 4.1 Matriz de Vista (View Matrix)\nPrimero, se transforma todo el universo de la escena para que la c√°mara sea el origen $(0,0,0)$. Si la c√°mara tiene una posici√≥n $C$ y una orientaci√≥n $R$, la matriz de vista $V$ es la inversa de la matriz de transformaci√≥n de la c√°mara:\n$$V = (M_{camera})^{-1}$$\nEsto justifica que mover la c√°mara hacia adelante es num√©ricamente equivalente a mover todo el mundo hacia atr√°s.\n\n### 4.2 Matriz de Proyecci√≥n de Perspectiva\nPara simular el ojo humano, los objetos lejanos deben verse m√°s peque√±os. Esto se logra dividiendo las coordenadas por su profundidad ($z$). La matriz de proyecci√≥n $P$ transforma un volumen truncado (frustum) en un cubo normalizado:\n\n$$P = \\begin{bmatrix} \n\\frac{1}{aspect \\cdot \\tan(\\frac{fov}{2})} & 0 & 0 & 0 \\\\ \n0 & \\frac{1}{\\tan(\\frac{fov}{2})} & 0 & 0 \\\\ \n0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2fn}{f-n} \\\\ \n0 & 0 & -1 & 0 \n\\end{bmatrix}$$\n\nDonde:\n* **$fov$:** Campo de visi√≥n (Field of View).\n* **$aspect$:** Relaci√≥n de aspecto de la pantalla (ancho/alto).\n* **$n, f$:** Planos de corte cercano (near) y lejano (far).\n\n### 4.3 Coordenadas de Dispositivo Normalizadas (NDC)\nTras multiplicar un punto $P_{world}$ por las matrices de Vista y Proyecci√≥n, obtenemos coordenadas en un espacio intermedio. El paso final es la **Divisi√≥n de Perspectiva**:\n$$x_{ndc} = \\frac{x_{clip}}{w_{clip}}, \\quad y_{ndc} = \\frac{y_{clip}}{w_{clip}}, \\quad z_{ndc} = \\frac{z_{clip}}{w_{clip}}$$\n\nEste proceso asegura que todos los puntos visibles queden dentro del rango $[-1, 1]$. Cualquier valor fuera de este rango es descartado num√©ricamente (**Clipping**), optimizando el uso de la GPU.\n\n### 4.4 Resumen del Flujo de Datos\nLa posici√≥n final de un v√©rtice en pantalla ($v_{screen}$) se calcula como:\n$$v_{screen} = P \\cdot V \\cdot M \\cdot v_{local}$$\n\n> **Justificaci√≥n Num√©rica:** Este encadenamiento de multiplicaciones de matrices permite procesar miles de v√©rtices en paralelo mediante hardware especializado (GPU), resolviendo sistemas lineales masivos en milisegundos.",
      "metadata": {}
    },
    {
      "id": "d920411d-2f91-4b02-aafa-fa883612909b",
      "cell_type": "markdown",
      "source": "## 5. Modelos Num√©ricos de Iluminaci√≥n y Sombreado (Shading)\n\nEl renderizado en A-Frame se basa en calcular la intensidad de color en cada p√≠xel mediante modelos matem√°ticos que aproximan la interacci√≥n de la luz con las superficies.\n\n### 5.1 El Modelo de Reflexi√≥n de Lambert\nPara superficies opacas (mate), A-Frame utiliza la ley del coseno de Lambert. La intensidad de iluminaci√≥n $I$ en un punto depende del √°ngulo entre el vector de la luz $\\vec{L}$ y el vector normal de la superficie $\\vec{N}$.\n\nLa aproximaci√≥n num√©rica se resuelve mediante el **Producto Punto**:\n$$I = L_c \\cdot K_d \\cdot \\max(0, \\hat{N} \\cdot \\hat{L})$$\n\nDonde:\n* $L_c$: Intensidad y color de la fuente de luz.\n* $K_d$: Coeficiente de reflexi√≥n difusa del material.\n* $\\hat{N} \\cdot \\hat{L}$: Coseno del √°ngulo entre los vectores normalizados. Si el √°ngulo es $> 90^\\circ$, el producto punto es negativo y la luz no incide (valor 0).\n\n### 5.2 Modelo Especular (Phong/Blinn-Phong)\nPara simular el brillo en materiales como metal o pl√°stico, se a√±ade un componente que depende de la posici√≥n del observador (la c√°mara) $\\vec{V}$. Se calcula un vector intermedio llamado **Vector Medio** ($\\vec{H}$):\n\n$$\\vec{H} = \\frac{\\vec{L} + \\vec{V}}{\\|\\vec{L} + \\vec{V}\\|}$$\n\nLa intensidad especular se aproxima como:\n$$I_{spec} = K_s \\cdot (\\hat{N} \\cdot \\hat{H})^n$$\n* $n$: Es el exponente de brillo (Shininess). A mayor $n$, el reflejo es m√°s peque√±o y definido.\n\n### 5.3 Implementaci√≥n en Shaders\nEstos c√°lculos no se realizan en la CPU, sino que se delegan a la GPU mediante programas llamados **Shaders** escritos en GLSL. Existen dos aproximaciones principales:\n\n| M√©todo | Aplicaci√≥n Num√©rica | Calidad Visual |\n| :--- | :--- | :--- |\n| **Gouraud Shading** | Calcula la iluminaci√≥n en los v√©rtices e interpola linealmente el color en el resto de la cara. | Baja (bordes visibles) |\n| **Phong Shading** | Interpola las **normales** de los v√©rtices y calcula la iluminaci√≥n p√≠xel por p√≠xel. | Alta (suavidad matem√°tica) |\n\n### 5.4 Justificaci√≥n del Error de Aproximaci√≥n\nComo A-Frame busca el rendimiento en tiempo real (60 FPS), utiliza estos modelos emp√≠ricos en lugar de resolver la **Ecuaci√≥n de Renderizado de Kajiya** (que es una integral compleja). Esto es un ejemplo cl√°sico de c√≥mo los m√©todos num√©ricos sacrifican exactitud f√≠sica por eficiencia computacional.",
      "metadata": {}
    },
    {
      "id": "15cf8d60-5fc1-43fb-aead-4c24ed432e6b",
      "cell_type": "markdown",
      "source": "## 6. Arquitectura ECS y el Ciclo de Actualizaci√≥n Num√©rica\n\nA-Frame utiliza el patr√≥n **Entity-Component-System (ECS)**, el cual permite desacoplar los datos de la l√≥gica, facilitando el procesamiento masivo de entidades.\n\n### 6.1 Componentes y Sistemas\n* **Entidades ($E$):** Identificadores √∫nicos (puntos en el grafo).\n* **Componentes ($C$):** Contenedores de datos (ej. posici√≥n, velocidad, masa).\n* **Sistemas ($S$):** La l√≥gica que opera sobre los datos. Desde el punto de vista num√©rico, un sistema es un **solucionador** que itera sobre un conjunto de estados.\n\n### 6.2 El M√©todo `tick`: Resoluci√≥n en Tiempo Real\nEl motor de A-Frame ejecuta una funci√≥n llamada `tick` aproximadamente cada $16.67$ ms (para alcanzar 60 FPS). Este es, en esencia, un paso de tiempo discreto $\\Delta t$:\n\n$$S_{t+1} = S_t + f(S_t, \\Delta t)$$\n\nDonde $f$ representa la funci√≥n de actualizaci√≥n (f√≠sica, animaciones, IA). Al ser un m√©todo iterativo, si $\\Delta t$ var√≠a (debido a ca√≠das de rendimiento), el sistema debe compensarlo para mantener la consistencia temporal, una t√©cnica com√∫n en la resoluci√≥n de ecuaciones diferenciales ordinarias (EDO) para simulaciones f√≠sicas.",
      "metadata": {}
    },
    {
      "id": "7e18a43a-1c67-4738-b9a6-d8fcb4115ebd",
      "cell_type": "markdown",
      "source": "## 6. Arquitectura ECS y el Ciclo de Actualizaci√≥n Num√©rica\n\nA-Frame utiliza el patr√≥n **Entity-Component-System (ECS)**, el cual permite desacoplar los datos de la l√≥gica, facilitando el procesamiento masivo de entidades.\n\n### 6.1 Componentes y Sistemas\n* **Entidades ($E$):** Identificadores √∫nicos (puntos en el grafo).\n* **Componentes ($C$):** Contenedores de datos (ej. posici√≥n, velocidad, masa).\n* **Sistemas ($S$):** La l√≥gica que opera sobre los datos. Desde el punto de vista num√©rico, un sistema es un **solucionador** que itera sobre un conjunto de estados.\n\n### 6.2 El M√©todo `tick`: Resoluci√≥n en Tiempo Real\nEl motor de A-Frame ejecuta una funci√≥n llamada `tick` aproximadamente cada $16.67$ ms (para alcanzar 60 FPS). Este es, en esencia, un paso de tiempo discreto $\\Delta t$:\n\n$$S_{t+1} = S_t + f(S_t, \\Delta t)$$\n\nDonde $f$ representa la funci√≥n de actualizaci√≥n (f√≠sica, animaciones, IA). Al ser un m√©todo iterativo, si $\\Delta t$ var√≠a (debido a ca√≠das de rendimiento), el sistema debe compensarlo para mantener la consistencia temporal, una t√©cnica com√∫n en la resoluci√≥n de ecuaciones diferenciales ordinarias (EDO) para simulaciones f√≠sicas.",
      "metadata": {}
    },
    {
      "id": "1a9e3e94-9d59-4c05-ad6a-6af623cbf4ee",
      "cell_type": "markdown",
      "source": "## 7. Implementaci√≥n de una Escena Base\n\nA continuaci√≥n, se presenta la estructura declarativa de una escena. Aunque el usuario escribe etiquetas tipo HTML, el motor traduce cada etiqueta a los objetos matem√°ticos (matrices, mallas y vectores) descritos anteriormente.\n\n```html\n<a-scene>\n  <a-sky color=\"#ECECEC\"></a-sky>\n\n  <a-box position=\"-1 0.5 -3\" \n         rotation=\"0 45 0\" \n         color=\"#4CC3D9\"\n         shadow>\n  </a-box>\n\n  <a-plane position=\"0 0 -4\" \n           rotation=\"-90 0 0\" \n           width=\"4\" \n           height=\"4\" \n           color=\"#7BC8A4\">\n  </a-plane>\n\n  <a-camera position=\"0 1.6 0\"></a-camera>\n</a-scene>",
      "metadata": {}
    },
    {
      "id": "46e61636-acd8-4987-8726-5a69a55ba0f3",
      "cell_type": "markdown",
      "source": "### 7.1 Resumen de la Justificaci√≥n\nEl uso de **A-Frame** en el contexto de **M√©todos Num√©ricos** permite:\n\n* **Visualizaci√≥n de Transformaciones:** Observar en tiempo real el efecto de las multiplicaciones matriciales y la composici√≥n de transformaciones lineales.\n* **Simulaci√≥n de Modelos F√≠sicos:** Implementar algoritmos de colisi√≥n, gravedad y movimiento basados en **integraci√≥n num√©rica** (como el m√©todo de Euler o Verlet).\n* **Abstracci√≥n de WebGL:** Manipular el pipeline de renderizado (shading, proyecci√≥n y rasterizaci√≥n) sin la necesidad de escribir manualmente miles de l√≠neas de c√≥digo de bajo nivel para la GPU, permitiendo enfocarse en la l√≥gica matem√°tica del espacio tridimensional.\n\n---\n\n> **Conclusi√≥n del M√≥dulo:** Tanto los operadores de gradiente (**Sobel**), el algoritmo de optimizaci√≥n de bordes (**Canny**), como los motores de renderizado (**A-Frame**), comparten una base com√∫n: la discretizaci√≥n de funciones continuas para su resoluci√≥n mediante c√≥mputo digital. Mientras los primeros analizan la se√±al, el √∫ltimo la sintetiza, pero ambos dependen del rigor del √°lgebra lineal y el c√°lculo diferencial.",
      "metadata": {}
    },
    {
      "id": "83c51a8f-23db-4794-a403-14a326091d32",
      "cell_type": "code",
      "source": "from IPython.display import display, HTML\nimport os\n\n# PEGUE AQU√ç TU C√ìDIGO CON PEQUE√ëAS MEJORAS DE FUNCIONALIDAD\n# (Agregamos autoStart: false y un bot√≥n de inicio para evitar bloqueos)\nhtml_content = \"\"\"\n<!DOCTYPE html>\n<html lang=\"es\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n  <title>Filtros AR Profesionales</title>\n\n  <script src=\"https://aframe.io/releases/1.4.2/aframe.min.js\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/mind-ar@1.2.5/dist/mindar-face-aframe.prod.js\"></script>\n\n  <style>\n    body { margin: 0; overflow: hidden; background: black; font-family: sans-serif; }\n    \n    /* PANTALLA DE INICIO (Necesaria para permisos) */\n    #start-screen {\n        position: fixed; top: 0; left: 0; width: 100%; height: 100%;\n        background: black; z-index: 9999;\n        display: flex; flex-direction: column; align-items: center; justify-content: center;\n        color: white;\n    }\n    #start-btn {\n        background: #00e676; color: black; padding: 15px 30px;\n        border: none; border-radius: 30px; font-size: 20px; font-weight: bold;\n        cursor: pointer; margin-top: 20px;\n    }\n\n    /* TUS ESTILOS ORIGINALES */\n    #ui {\n      position: fixed;\n      bottom: 20px;\n      left: 50%;\n      transform: translateX(-50%);\n      display: none; /* Oculto al principio */\n      gap: 10px;\n      z-index: 10;\n    }\n    .control-btn {\n      padding: 10px 14px;\n      font-size: 20px;\n      border-radius: 12px;\n      border: 2px solid white;\n      background: rgba(0,0,0,0.5);\n      color: white;\n      cursor: pointer;\n    }\n    .control-btn.active { background: #00e676; color: black; border-color: #00e676; }\n  </style>\n</head>\n\n<body>\n\n<div id=\"start-screen\">\n    <h1>Filtros AR</h1>\n    <button id=\"start-btn\" onclick=\"startAR()\">üì∏ ACTIVAR C√ÅMARA</button>\n</div>\n\n<div id=\"ui\">\n  <button class=\"control-btn\" onclick=\"toggle('sombrero', this)\">üé©</button>\n  <button class=\"control-btn\" onclick=\"toggle('lentes', this)\">üï∂Ô∏è</button>\n  <button class=\"control-btn\" onclick=\"toggle('bigote', this)\">üßî</button>\n  <button class=\"control-btn\" onclick=\"toggle('lunar', this)\">‚ö´</button>\n</div>\n\n<a-scene\n  mindar-face=\"autoStart: false\"\n  embedded\n  vr-mode-ui=\"enabled:false\"\n  device-orientation-permission-ui=\"enabled:false\"\n  renderer=\"colorManagement:true\">\n\n  <a-camera active=\"false\" position=\"0 0 0\"></a-camera>\n\n  <a-entity mindar-face-target=\"anchorIndex:1\">\n\n    <a-entity id=\"sombrero\" visible=\"false\" position=\"0 0.9 0\">\n      <a-cylinder radius=\"1.1\" height=\"0.05\" color=\"#111\"></a-cylinder>\n      <a-cylinder position=\"0 0.35 0\" radius=\"0.45\" height=\"0.7\" color=\"#111\"></a-cylinder>\n    </a-entity>\n\n    <a-entity id=\"lentes\" visible=\"false\" position=\"0 0.30 0.06\">\n      <a-torus position=\"-0.18 0 0\" radius=\"0.12\" radius-tubular=\"0.012\" color=\"#C0C0C0\"></a-torus>\n      <a-circle position=\"-0.18 0 0.01\" radius=\"0.105\"\n        material=\"color:#3fa9f5; opacity:0.45; transparent:true\"></a-circle>\n\n      <a-torus position=\"0.18 0 0\" radius=\"0.12\" radius-tubular=\"0.012\" color=\"#C0C0C0\"></a-torus>\n      <a-circle position=\"0.18 0 0.01\" radius=\"0.105\"\n        material=\"color:#3fa9f5; opacity:0.45; transparent:true\"></a-circle>\n\n      <a-box position=\"0 0 0\" scale=\"0.10 0.02 0.02\" color=\"#C0C0C0\"></a-box>\n    </a-entity>\n\n  </a-entity>\n\n  <a-entity mindar-face-target=\"anchorIndex:330\">\n    <a-sphere\n      id=\"lunar\"\n      radius=\"0.03\"\n      color=\"black\"\n      visible=\"false\">\n    </a-sphere>\n  </a-entity>\n\n  <a-entity mindar-face-target=\"anchorIndex:164\"> <a-box\n      id=\"bigote\"\n      position=\"0 -0.1 0\"\n      scale=\"0.5 0.1 0.1\"\n      color=\"black\"\n      visible=\"false\">\n    </a-box>\n  </a-entity>\n\n</a-scene>\n\n<script>\n    const startScreen = document.getElementById('start-screen');\n    const startBtn = document.getElementById('start-btn');\n    const ui = document.getElementById('ui');\n    const scene = document.querySelector('a-scene');\n\n    // 1. FUNCION DE INICIO (Para permisos de navegador)\n    function startAR() {\n        startBtn.innerText = \"Cargando...\";\n        startBtn.disabled = true;\n        \n        // Arrancar MindAR\n        scene.systems['mindar-face-system'].start();\n    }\n\n    // 2. EVENTO: C√ÅMARA LISTA\n    scene.addEventListener('arReady', () => {\n        startScreen.style.display = 'none'; // Quitar pantalla inicio\n        ui.style.display = 'flex';          // Mostrar botones\n    });\n\n    // 3. TOGGLE CORREGIDO\n    // El getAttribute devuelve string \"true\"/\"false\", hay que compararlo\n    function toggle(id, btn) {\n        const el = document.getElementById(id);\n        const isVisible = el.getAttribute('visible');\n        \n        // Si es \"true\" o true, lo volvemos false. Si no, true.\n        const newState = !(isVisible === true || isVisible === 'true');\n        \n        el.setAttribute('visible', newState);\n        \n        // Estilo visual del bot√≥n\n        if(newState) btn.classList.add('active');\n        else btn.classList.remove('active');\n    }\n</script>\n\n</body>\n</html>\n\"\"\"\n\n# GUARDAR EL ARCHIVO HTML\nnombre_archivo = \"mis_filtros.html\"\nwith open(nombre_archivo, \"w\") as f:\n    f.write(html_content)\n\nprint(f\"‚úÖ Archivo creado: {nombre_archivo}\")\n\n# BOT√ìN PARA ABRIRLO\ndisplay(HTML(f\"\"\"\n    <div style=\"background: #222; padding: 20px; border-radius: 10px; text-align: center;\">\n        <h2 style=\"color: white;\">¬°Listo!</h2>\n        <a href=\"{nombre_archivo}\" target=\"_blank\" style=\"\n            background: #00e676; color: black; text-decoration: none;\n            padding: 15px 30px; font-size: 20px; font-weight: bold;\n            border-radius: 30px; display: inline-block; margin-top: 10px;\">\n            üöÄ ABRIR FILTROS EN PESTA√ëA NUEVA\n        </a>\n    </div>\n\"\"\"))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d4201534-eed5-4b19-82d0-a304dab3472c",
      "cell_type": "markdown",
      "source": "# Three.js: Motor de Gr√°ficos Imperativo y √Ålgebra Lineal\n\n## 1. Fundamentos Matem√°ticos y la Tuber√≠a de Datos (Pipeline)\n\nA diferencia de los frameworks declarativos, **Three.js** es una biblioteca de bajo nivel que permite la manipulaci√≥n directa de los b√∫feres de memoria y las operaciones matriciales. Su funcionamiento se basa en la resoluci√≥n de transformaciones en espacios de $n$-dimensiones.\n\n### 1.1 El Objeto `BufferGeometry`: Eficiencia en Memoria\nDesde la perspectiva num√©rica, Three.js no trata a los objetos como entidades abstractas, sino como **arreglos de datos tipados (TypedArrays)**. Una malla (mesh) se define por un conjunto de atributos vectoriales almacenados de forma contigua para maximizar el *throughput* de la GPU:\n\n* **V√©rtices ($V$):** Un vector plano $[x_1, y_1, z_1, ..., x_n, y_n, z_n] \\in \\mathbb{R}^{3n}$.\n* **Normales ($N$):** Vectores unitarios para el c√°lculo de iluminaci√≥n.\n* **√çndices ($I$):** Define la topolog√≠a de la malla mediante la conectividad de los v√©rtices para formar tri√°ngulos.\n\n### 1.2 La Cadena de Transformaciones (Modelo-Vista-Proyecci√≥n)\nPara renderizar un solo punto, Three.js debe resolver la ecuaci√≥n fundamental de la computaci√≥n gr√°fica, que es una cadena de multiplicaciones de matrices de $4 \\times 4$:\n\n$$P_{clip} = M_{proj} \\cdot M_{view} \\cdot M_{model} \\cdot v_{local}$$\n\nCada componente tiene una justificaci√≥n num√©rica espec√≠fica:\n1. **$M_{model}$ (World Matrix):** Transforma coordenadas locales al espacio global mediante traslaci√≥n, rotaci√≥n y escala.\n2. **$M_{view}$ (Camera Matrix):** Es la matriz inversa de la posici√≥n de la c√°mara ($C^{-1}$), que reorienta el universo respecto al observador.\n3. **$M_{proj}$ (Projection Matrix):** Aplica la distorsi√≥n de perspectiva, mapeando un volumen arbitrario a coordenadas normalizadas $[-1, 1]$.\n\n### 1.3 El Grafo de Escena y Matrices Globales\nThree.js utiliza un sistema de actualizaci√≥n recursiva para las matrices. Para cualquier objeto $i$, su matriz global $M_{global}^i$ se define como:\n\n$$M_{global}^i = M_{global}^{padre} \\cdot M_{local}^i$$\n\n> **Dato para M√©todos Num√©ricos:** Para optimizar el rendimiento, Three.js no recalcula estas matrices en cada frame a menos que la propiedad `.matrixWorldNeedsUpdate` sea verdadera, lo que ejemplifica una estrategia de **ahorro computacional** en sistemas din√°micos.",
      "metadata": {}
    },
    {
      "id": "baa27541-8cb2-4eff-b305-d911391cbb23",
      "cell_type": "markdown",
      "source": "## 2. Geometr√≠a Computacional: Raycasting e Intersecci√≥n\n\nEl **Raycasting** es la t√©cnica que utiliza Three.js para resolver problemas de interactividad y detecci√≥n de colisiones. Matem√°ticamente, consiste en proyectar una l√≠nea recta en el espacio y encontrar el punto de intersecci√≥n con la geometr√≠a m√°s cercana.\n\n### 2.1 Ecuaci√≥n Param√©trica del Rayo\nUn rayo se define como una funci√≥n lineal que depende de un par√°metro de tiempo o distancia $t$:\n\n$$\\vec{R}(t) = \\vec{O} + t\\vec{D}$$\n\nDonde:\n* $\\vec{O}$: Es el origen del rayo (usualmente la posici√≥n de la c√°mara).\n* $\\vec{D}$: Es el vector de direcci√≥n unitario ($\\|\\vec{D}\\| = 1$).\n* $t$: Es la distancia desde el origen ($t \\geq 0$).\n\n### 2.2 Algoritmo de Intersecci√≥n Rayo-Tri√°ngulo (M√∂ller-Trumbore)\nPara determinar si un rayo golpea un objeto, Three.js debe resolver si el rayo intersecta alguno de los tri√°ngulos de la malla. Dado un tri√°ngulo con v√©rtices $V_1, V_2, V_3$, un punto $P$ dentro del tri√°ngulo se define por sus **Coordenadas Baric√©ntricas** $(u, v)$:\n\n$$P(u, v) = (1 - u - v)V_1 + uV_2 + vV_3$$\n\nIgualando la ecuaci√≥n del rayo con la del plano del tri√°ngulo, el sistema de ecuaciones lineales resultante es:\n\n$$\\vec{O} + t\\vec{D} = (1 - u - v)V_1 + uV_2 + vV_3$$\n\nEste sistema se resuelve num√©ricamente para $t, u, v$ mediante la regla de Cramer, lo que permite a la GPU o CPU determinar:\n1. **Si hay intersecci√≥n:** Si $u \\geq 0, v \\geq 0$ y $u + v \\leq 1$.\n2. **La distancia exacta:** El valor de $t$.\n\n### 2.3 Frustum Culling: Optimizaci√≥n por Descarte\nAntes de realizar los c√°lculos costosos de intersecci√≥n, Three.js aplica m√©todos de **delimitaci√≥n volum√©trica**. Se envuelve cada objeto en una **Esfera de Delimitaci√≥n (Bounding Sphere)** m√≠nima:\n\n$$\\|\\vec{P} - \\vec{C}\\|^2 \\leq r^2$$\n\n**Justificaci√≥n Num√©rica:** Es mucho m√°s barato computacionalmente verificar la intersecci√≥n con una esfera (una sola ecuaci√≥n cuadr√°tica) que con miles de tri√°ngulos. Si el rayo no toca la esfera, se descarta el objeto inmediatamente, reduciendo la complejidad algor√≠tmica de $O(n)$ a $O(1)$ para ese objeto.",
      "metadata": {}
    },
    {
      "id": "bb1fa405-1352-4ef1-8904-690717cc38eb",
      "cell_type": "markdown",
      "source": "## 3. Interpolaci√≥n y Curvas Param√©tricas\n\nEn Three.js, la animaci√≥n y el movimiento de c√°maras u objetos se basan en la aproximaci√≥n de trayectorias mediante funciones polin√≥micas. Esto es fundamental para evitar movimientos \"rob√≥ticos\" y asegurar la continuidad en las derivadas de la posici√≥n (velocidad y aceleraci√≥n).\n\n### 3.1 Curvas de B√©zier (Interpolaci√≥n Polin√≥mica)\nPara definir trayectorias suaves, se utilizan las **Curvas de B√©zier**, que se basan en los **Polinomios de Bernstein**. Una curva de tercer grado (c√∫bica) se define mediante cuatro puntos de control $P_0, P_1, P_2, P_3$:\n\n$$B(t) = (1-t)^3 P_0 + 3(1-t)^2 t P_1 + 3(1-t) t^2 P_2 + t^3 P_3, \\quad t \\in [0, 1]$$\n\n**Justificaci√≥n Num√©rica:** Esta f√≥rmula garantiza que la curva sea tangente a los segmentos de control, permitiendo un control geom√©trico preciso de la trayectoria sin necesidad de resolver sistemas de ecuaciones globales pesados.\n\n### 3.2 Splines de Catmull-Rom\nA diferencia de B√©zier, donde la curva no siempre pasa por los puntos de control, Three.js utiliza frecuentemente **Catmull-Rom Splines** para que la trayectoria pase exactamente por los nodos definidos. La ecuaci√≥n local para un segmento entre $P_i$ y $P_{i+1}$ es:\n\n$$P(t) = \\frac{1}{2} \\begin{bmatrix} 1 & t & t^2 & t^3 \\end{bmatrix} \\begin{bmatrix} 0 & 2 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 2 & -5 & 4 & -1 \\\\ -1 & 3 & -3 & 1 \\end{bmatrix} \\begin{bmatrix} P_{i-1} \\\\ P_i \\\\ P_{i+1} \\\\ P_{i+2} \\end{bmatrix}$$\n\n* **Continuidad $C^1$:** Este m√©todo asegura que la primera derivada (velocidad) sea continua en todos los puntos, eliminando cambios bruscos de direcci√≥n.\n\n### 3.3 El Ciclo de Animaci√≥n y Delta Time ($\\Delta t$)\nEn m√©todos num√©ricos, la integraci√≥n del movimiento requiere un paso de tiempo. Three.js utiliza el reloj del sistema para calcular el `delta`:\n\n$$\\text{Posici√≥n}_{nuevo} = \\text{Posici√≥n}_{anterior} + (\\text{Velocidad} \\cdot \\Delta t)$$\n\nSi la velocidad no es constante, se aplican m√©todos de integraci√≥n como **Euler Simple** o, en simulaciones m√°s avanzadas dentro de Three.js, **Integraci√≥n de Verlet**, que es m√°s estable num√©ricamente para sistemas de part√≠culas:\n\n$$x_{t+ \\Delta t} = 2x_t - x_{t- \\Delta t} + a \\cdot \\Delta t^2$$",
      "metadata": {}
    },
    {
      "id": "39267076-6cc4-4c58-84c7-95e88f45c6b4",
      "cell_type": "markdown",
      "source": "## 4. Iluminaci√≥n Avanzada y Mapeo de Normales (Normal Mapping)\n\nEn Three.js, la apariencia de una superficie no depende solo de la cantidad de tri√°ngulos, sino de c√≥mo se manipulan los vectores normales en el nivel de los fragmentos (p√≠xeles). Esto se basa en la aplicaci√≥n de **√Ålgebra Lineal** para simular micro-geometr√≠a.\n\n### 4.1 Espacio Tangente y Base TBN\nPara aplicar un mapa de normales (una textura que indica hacia d√≥nde \"mira\" cada p√≠xel), Three.js debe construir un sistema de coordenadas local para cada punto de la superficie, llamado **Espacio Tangente**. Este se define por tres vectores ortonormales:\n* **$\\vec{T}$ (Tangente):** Vector en la direcci√≥n del eje U de la textura.\n* **$\\vec{B}$ (Bitangente):** Vector en la direcci√≥n del eje V de la textura.\n* **$\\vec{N}$ (Normal):** Vector perpendicular a la cara.\n\nEstos vectores forman la **Matriz TBN**, que permite transformar vectores del \"Espacio de Textura\" al \"Espacio del Mundo\":\n$$M_{TBN} = \\begin{bmatrix} T_x & B_x & N_x \\\\ T_y & B_y & N_y \\\\ T_z & B_z & N_z \\end{bmatrix}$$\n\n### 4.2 Perturbaci√≥n de la Normal\nEl valor de color de un p√≠xel en el mapa de normales $(R, G, B)$ se decodifica para obtener un vector unitario en el rango $[-1, 1]$:\n$$\\vec{n}_{local} = \\begin{bmatrix} 2R - 1 \\\\ 2G - 1 \\\\ 2B - 1 \\end{bmatrix}$$\n\nLa normal final utilizada para el c√°lculo de iluminaci√≥n ($\\vec{n}_{final}$) es el producto de la matriz TBN por este vector local:\n$$\\vec{n}_{final} = M_{TBN} \\cdot \\vec{n}_{local}$$\n\n### 4.3 El Modelo de PBR (Physically Based Rendering)\nThree.js utiliza en su `MeshStandardMaterial` un modelo num√©rico avanzado llamado **Ecuaci√≥n de Reflectancia**, que es una integral de iluminaci√≥n:\n$$L_o(p, \\omega_o) = \\int_{\\Omega} f_r(p, \\omega_i, \\omega_o) L_i(p, \\omega_i) (\\vec{n} \\cdot \\omega_i) d\\omega_i$$\n\n**Justificaci√≥n Num√©rica:** Como resolver esta integral en tiempo real es imposible, Three.js utiliza una aproximaci√≥n de **Suma de Riemann** o **Importancia de Muestreo (Importance Sampling)** para simular c√≥mo la luz se refleja en micro-facetas, bas√°ndose en la funci√≥n de distribuci√≥n de micro-facetas de **Trowbridge-Reitz (GGX)**.\n\n### 4.4 C√°lculo de Sombras (Shadow Mapping)\nLas sombras se calculan mediante una comparaci√≥n de profundidad ($z$). Para cada p√≠xel, el motor resuelve:\n$$Visible = \\begin{cases} 1 & \\text{si } z_{p√≠xel} \\leq z_{shadow\\_map} + \\epsilon \\\\ 0 & \\text{en otro caso} \\end{cases}$$\n*Donde $\\epsilon$ es un factor de sesgo (bias) introducido para corregir errores de precisi√≥n num√©rica de punto flotante.*",
      "metadata": {}
    },
    {
      "id": "895dbbb4-2286-4a29-a3fe-51bb6bd81893",
      "cell_type": "markdown",
      "source": "## 5. El Ciclo de Renderizado (Render Loop) y Estabilidad Num√©rica\n\nEn Three.js, la generaci√≥n de im√°genes no es un evento est√°tico, sino un proceso iterativo continuo que debe mantener la estabilidad num√©rica para evitar errores de precisi√≥n o \"saltos\" en la simulaci√≥n.\n\n### 5.1 Discretizaci√≥n del Tiempo\nCada iteraci√≥n del ciclo de renderizado representa un paso de tiempo discreto $\\Delta t$. Para que las simulaciones f√≠sicas sean consistentes, Three.js utiliza el m√©todo de `requestAnimationFrame`, que busca sincronizarse con la tasa de refresco del monitor.\n\nLa actualizaci√≥n del estado del sistema se define como:\n$$X_{t+1} = X_t + \\int_{t}^{t+\\Delta t} v(t) dt \\approx X_t + v_t \\cdot \\Delta t$$\n\n### 5.2 Manejo del Error de Redondeo (Floating Point Precision)\nDado que WebGL utiliza precisi√≥n de punto flotante de 32 bits, en escenas con escalas masivas (ej. simulaciones astron√≥micas) se presenta el error de **Z-Fighting**.\nEste fen√≥meno ocurre cuando dos profundidades $z_1$ y $z_2$ son tan cercanas que la precisi√≥n num√©rica no puede distinguirlas:\n$$|z_1 - z_2| < \\epsilon$$\nDonde $\\epsilon$ es la resoluci√≥n del b√∫fer de profundidad. Three.js permite mitigar esto mediante el uso de un **Logarithmic Depth Buffer**, que redistribuye la precisi√≥n de forma no lineal (logar√≠tmica) para dar m√°s detalle a objetos lejanos.",
      "metadata": {}
    },
    {
      "id": "7f3d8e88-15a3-4e1e-9669-e6415a97092f",
      "cell_type": "markdown",
      "source": "## 5. El Ciclo de Renderizado (Render Loop) y Estabilidad Num√©rica\n\nEn Three.js, la generaci√≥n de im√°genes no es un evento est√°tico, sino un proceso iterativo continuo que debe mantener la estabilidad num√©rica para evitar errores de precisi√≥n o \"saltos\" en la simulaci√≥n.\n\n### 5.1 Discretizaci√≥n del Tiempo\nCada iteraci√≥n del ciclo de renderizado representa un paso de tiempo discreto $\\Delta t$. Para que las simulaciones f√≠sicas sean consistentes, Three.js utiliza el m√©todo de `requestAnimationFrame`, que busca sincronizarse con la tasa de refresco del monitor.\n\nLa actualizaci√≥n del estado del sistema se define como:\n$$X_{t+1} = X_t + \\int_{t}^{t+\\Delta t} v(t) dt \\approx X_t + v_t \\cdot \\Delta t$$\n\n### 5.2 Manejo del Error de Redondeo (Floating Point Precision)\nDado que WebGL utiliza precisi√≥n de punto flotante de 32 bits, en escenas con escalas masivas (ej. simulaciones astron√≥micas) se presenta el error de **Z-Fighting**.\nEste fen√≥meno ocurre cuando dos profundidades $z_1$ y $z_2$ son tan cercanas que la precisi√≥n num√©rica no puede distinguirlas:\n$$|z_1 - z_2| < \\epsilon$$\nDonde $\\epsilon$ es la resoluci√≥n del b√∫fer de profundidad. Three.js permite mitigar esto mediante el uso de un **Logarithmic Depth Buffer**, que redistribuye la precisi√≥n de forma no lineal (logar√≠tmica) para dar m√°s detalle a objetos lejanos.",
      "metadata": {}
    },
    {
      "id": "22ac4f83-dc69-4e5e-bd53-eeb36dc969fe",
      "cell_type": "markdown",
      "source": "## 6. Estructura de Implementaci√≥n y C√≥digo Base\n\nA diferencia de A-Frame, en Three.js la configuraci√≥n de la escena requiere la instanciaci√≥n expl√≠cita de los objetos matem√°ticos (C√°mara, Escena, Renderizador).\n\n### 6.1 Componentes Esenciales del Script\nEl siguiente bloque muestra c√≥mo se traduce la teor√≠a de matrices y vectores a c√≥digo funcional:\n\n```javascript\n// 1. Inicializaci√≥n del Motor (Espacio de Renderizado)\nconst scene = new THREE.Scene();\nconst camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\nconst renderer = new THREE.WebGLRenderer();\n\n// 2. Creaci√≥n de Geometr√≠a Discreta\n// Se define la topolog√≠a de la malla (8 v√©rtices, 12 caras triangulares)\nconst geometry = new THREE.BoxGeometry(1, 1, 1);\n\n// 3. Definici√≥n del Material (Modelo de Iluminaci√≥n)\n// Utiliza algoritmos de sombreado de fragmentos para calcular la luz\nconst material = new THREE.MeshStandardMaterial({ color: 0x00ff00 });\n\n// 4. Creaci√≥n de la Malla (Mesh = Geometr√≠a + Material)\nconst cube = new THREE.Mesh(geometry, material);\nscene.add(cube);\n\n// 5. Bucle de Animaci√≥n (M√©todo Iterativo)\nfunction animate() {\n  requestAnimationFrame(animate);\n\n  // Aplicaci√≥n de rotaci√≥n incremental (Transformaci√≥n de matriz local)\n  cube.rotation.x += 0.01;\n  cube.rotation.y += 0.01;\n\n  // Resoluci√≥n de la Matriz de Proyecci√≥n y dibujado en el Canvas\n  renderer.render(scene, camera);\n}\nanimate();",
      "metadata": {}
    },
    {
      "id": "38209b45-0e63-40b6-8cec-9a68e310872c",
      "cell_type": "code",
      "source": "from IPython.display import display, HTML\n\ncodigo_seguro = \"\"\"\n<div id=\"app-container\">\n    <div class=\"controls\">\n        <button id=\"btn-glasses\">üëì Lentes</button>\n        <button id=\"btn-mustache\">ü•∏ Bigote</button>\n        <button id=\"btn-hat\">üé© Sombrero</button>\n        <button id=\"btn-mole\">üü§ Lunar</button>\n        <button id=\"btn-points\">üü¢ Puntos (Debug)</button>\n    </div>\n\n    <div class=\"viewport\">\n        <video id=\"input-video\" style=\"display:none\" autoplay muted playsinline></video>\n        <canvas id=\"output-canvas\"></canvas>\n        <div id=\"status-text\">Iniciando sistema...</div>\n    </div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\"></script>\n\n<style>\n    #app-container {\n        font-family: sans-serif;\n        width: 640px;\n        margin: 0 auto;\n        background: #222;\n        padding: 10px;\n        border-radius: 8px;\n        color: white;\n    }\n\n    .viewport {\n        position: relative;\n        width: 640px;\n        height: 480px;\n        background: black;\n        border: 2px solid #444;\n        overflow: hidden;\n    }\n\n    canvas {\n        width: 100%;\n        height: 100%;\n        object-fit: cover;\n    }\n\n    .controls {\n        display: flex;\n        gap: 5px;\n        margin-bottom: 10px;\n        justify-content: center;\n        flex-wrap: wrap;\n    }\n\n    button {\n        background: #444;\n        color: white;\n        border: 1px solid #666;\n        padding: 8px 12px;\n        cursor: pointer;\n        border-radius: 4px;\n        font-weight: bold;\n    }\n\n    button:hover { background: #555; }\n    \n    /* Clase para bot√≥n activo (Verde) */\n    button.active {\n        background: #00e676;\n        color: black;\n        border-color: #00a152;\n    }\n\n    #status-text {\n        position: absolute;\n        bottom: 10px;\n        left: 10px;\n        background: rgba(0, 0, 0, 0.7);\n        padding: 5px 10px;\n        border-radius: 4px;\n        font-size: 14px;\n        pointer-events: none;\n    }\n</style>\n\n<script>\n(function() {\n    // 1. Configuraci√≥n Inicial\n    const container = document.getElementById('app-container');\n    const videoElement = container.querySelector('#input-video');\n    const canvasElement = container.querySelector('#output-canvas');\n    const ctx = canvasElement.getContext('2d');\n    const statusText = container.querySelector('#status-text');\n    \n    // Forzamos tama√±o\n    canvasElement.width = 640;\n    canvasElement.height = 480;\n\n    // Estado de filtros\n    const state = {\n        glasses: false,\n        mustache: false,\n        hat: false,\n        mole: false,\n        points: false // Activado por defecto para probar\n    };\n\n    // 2. Conectar Botones\n    function setupButton(id, key) {\n        const btn = container.querySelector('#' + id);\n        if(!btn) return;\n        btn.onclick = () => {\n            state[key] = !state[key];\n            // Visualmente cambiar el bot√≥n\n            if(state[key]) btn.classList.add('active');\n            else btn.classList.remove('active');\n            console.log(\"Toggle \" + key + \": \" + state[key]);\n        };\n    }\n\n    setupButton('btn-glasses', 'glasses');\n    setupButton('btn-mustache', 'mustache');\n    setupButton('btn-hat', 'hat');\n    setupButton('btn-mole', 'mole');\n    setupButton('btn-points', 'points');\n\n    // 3. L√≥gica de Dibujo (Filtros 2D)\n    function drawScene(results) {\n        // Limpiar\n        ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n        \n        // A. Dibujar la imagen de la c√°mara DE FONDO\n        ctx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);\n\n        // B. Verificar si hay cara\n        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\n            statusText.innerText = \"‚úÖ Rostro detectado - Filtros activos\";\n            statusText.style.color = \"#00e676\";\n            \n            const landmarks = results.multiFaceLandmarks[0];\n            drawFilters(landmarks);\n        } else {\n            statusText.innerText = \"‚ö†Ô∏è Buscando rostro... (Ac√©rcate a la c√°mara)\";\n            statusText.style.color = \"orange\";\n        }\n    }\n\n    function drawFilters(landmarks) {\n        const w = canvasElement.width;\n        const h = canvasElement.height;\n\n        // Puntos Clave\n        const leftEye = landmarks[33];\n        const rightEye = landmarks[263];\n        const noseTip = landmarks[1];\n        const upperLip = landmarks[13];\n        const forehead = landmarks[10];\n        const leftCheek = landmarks[234];\n\n        // Matem√°ticas para posici√≥n y tama√±o\n        const cx = (leftEye.x + rightEye.x) / 2 * w;\n        const cy = (leftEye.y + rightEye.y) / 2 * h;\n        const dx = (rightEye.x - leftEye.x) * w;\n        const dy = (rightEye.y - leftEye.y) * h;\n        const dist = Math.sqrt(dx*dx + dy*dy); // Distancia entre ojos\n        const angle = Math.atan2(dy, dx); // Inclinaci√≥n de la cabeza\n\n        ctx.save(); // Guardar estado para rotaciones\n\n        // --- 1. PUNTOS (DEBUG) ---\n        if (state.points) {\n            ctx.fillStyle = \"#00FF00\";\n            for (let point of landmarks) {\n                ctx.beginPath();\n                ctx.arc(point.x * w, point.y * h, 1, 0, 2 * Math.PI);\n                ctx.fill();\n            }\n        }\n\n        // --- 2. LENTES ---\n        if (state.glasses) {\n            ctx.translate(cx, cy);\n            ctx.rotate(angle);\n            \n            ctx.strokeStyle = \"black\";\n            ctx.lineWidth = 5;\n            // Marco Izq\n            ctx.strokeRect(-dist*0.9, -dist*0.3, dist*0.8, dist*0.5);\n            // Marco Der\n            ctx.strokeRect(dist*0.1, -dist*0.3, dist*0.8, dist*0.5);\n            // Puente\n            ctx.beginPath();\n            ctx.moveTo(-dist*0.1, 0);\n            ctx.lineTo(dist*0.1, 0);\n            ctx.stroke();\n\n            ctx.rotate(-angle); // Deshacer rotaci√≥n\n            ctx.translate(-cx, -cy); // Deshacer traslaci√≥n\n        }\n\n        // --- 3. BIGOTE ---\n        if (state.mustache) {\n            const mx = upperLip.x * w;\n            const my = upperLip.y * h;\n            \n            ctx.translate(mx, my);\n            ctx.rotate(angle);\n\n            ctx.fillStyle = \"#3e2723\";\n            ctx.beginPath();\n            ctx.ellipse(0, -dist*0.1, dist*0.6, dist*0.15, 0, 0, Math.PI*2);\n            ctx.fill();\n\n            ctx.rotate(-angle);\n            ctx.translate(-mx, -my);\n        }\n\n        // --- 4. SOMBRERO ---\n        if (state.hat) {\n            const hx = forehead.x * w;\n            const hy = forehead.y * h;\n\n            ctx.translate(hx, hy);\n            ctx.rotate(angle);\n\n            ctx.fillStyle = \"#1a237e\"; // Azul oscuro\n            // Copa\n            ctx.fillRect(-dist*0.8, -dist*1.5, dist*1.6, dist*1.0);\n            // Ala\n            ctx.fillRect(-dist*1.2, -dist*0.5, dist*2.4, dist*0.2);\n\n            ctx.rotate(-angle);\n            ctx.translate(-hx, -hy);\n        }\n\n        // --- 5. LUNAR ---\n        if (state.mole) {\n            const lx = leftCheek.x * w;\n            const ly = leftCheek.y * h;\n            ctx.fillStyle = \"black\";\n            ctx.beginPath();\n            ctx.arc(lx + dist*0.2, ly, dist*0.08, 0, Math.PI*2);\n            ctx.fill();\n        }\n\n        ctx.restore();\n    }\n\n    // 4. Iniciar MediaPipe FaceMesh\n    const faceMesh = new FaceMesh({\n        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n    });\n\n    faceMesh.setOptions({\n        maxNumFaces: 1,\n        refineLandmarks: true,\n        minDetectionConfidence: 0.5,\n        minTrackingConfidence: 0.5\n    });\n\n    faceMesh.onResults(drawScene);\n\n    // 5. Iniciar C√°mara\n    const camera = new Camera(videoElement, {\n        onFrame: async () => {\n            await faceMesh.send({image: videoElement});\n        },\n        width: 640,\n        height: 480\n    });\n\n    statusText.innerText = \"Solicitando c√°mara...\";\n    camera.start()\n        .then(() => statusText.innerText = \"C√°mara activa. Cargando modelo...\")\n        .catch(err => statusText.innerText = \"Error de c√°mara: \" + err);\n\n})();\n</script>\n\"\"\"\n\ndisplay(HTML(codigo_seguro))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6b88eae5-70bd-48b0-b26e-77ab5d53727b",
      "cell_type": "markdown",
      "source": "# MediaPipe: Inferencia Perceptual y Optimizaci√≥n de Grafos\n\n## 1. Fundamentos de Visi√≥n Computacional y Redes Convolucionales\n\nMediaPipe es un framework basado en el paradigma de **ML Pipelines**. Su funci√≥n principal en el contexto num√©rico es la **regresi√≥n de coordenadas**, donde una imagen de entrada $I$ se procesa para obtener un conjunto de vectores de caracter√≠sticas.\n\n### 1.1 Inferencia como Funci√≥n de Mapeo\nLa detecci√≥n de una cara o mano se puede expresar como una funci√≥n $f$ que mapea el espacio de la imagen al espacio de par√°metros:\n$$f(I) = \\Theta$$\nDonde:\n* $I \\in \\mathbb{R}^{W \\times H \\times 3}$: Es la matriz de p√≠xeles (ancho, alto, canales RGB).\n* $\\Theta = \\{\\vec{P}_1, \\vec{P}_2, ..., \\vec{P}_n\\}$: Es el conjunto de puntos clave (landmarks), donde cada $\\vec{P}_i = (x, y, z)$.\n\n### 1.2 La Convoluci√≥n Discreta (Sustento del Modelo)\nPara extraer caracter√≠sticas de la imagen, MediaPipe utiliza **Redes Neuronales Convolucionales (CNN)**. La operaci√≥n fundamental es la convoluci√≥n discreta entre la imagen $I$ y un filtro (kernel) $K$ de tama√±o $m \\times n$:\n\n$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i-m, j-n) \\cdot K(m, n)$$\n\n**Justificaci√≥n Num√©rica:** Este proceso act√∫a como un detector de gradientes y patrones (similar a los filtros de Sobel/Canny pero con pesos aprendidos), reduciendo la dimensionalidad de la imagen a un vector de caracter√≠sticas abstractas.\n\n### 1.3 El Grafo de Flujo (Calculators)\nMediaPipe organiza el procesamiento en un **Grafo Ac√≠clico Dirigido (DAG)**. Cada nodo es una \"calculadora\" que recibe un paquete de datos $T$ (Tensor) y emite un resultado tras aplicar una transformaci√≥n num√©rica:\n$$T_{out} = \\phi(T_{in})$$\nEsta arquitectura permite el **paralelismo masivo**, resolviendo m√∫ltiples etapas de la tuber√≠a de datos simult√°neamente.\n\n### 1.4 Normalizaci√≥n y Coordenadas Relativas\nMediaPipe no entrega coordenadas en p√≠xeles, sino valores normalizados en el rango $[0, 1]$. La conversi√≥n a coordenadas de pantalla (necesaria para Three.js) es una **Transformaci√≥n Lineal Escalar**:\n\n$$P_{pixel} = \\begin{bmatrix} x_{norm} \\cdot W \\\\ y_{norm} \\cdot H \\\\ z_{norm} \\cdot W \\end{bmatrix}$$\n\n*Donde $W$ y $H$ son el ancho y alto de la imagen.* La coordenada $z$ se calcula de manera relativa al centro de gravedad del objeto detectado, permitiendo una estimaci√≥n de profundidad sin necesidad de una c√°mara est√©reo.",
      "metadata": {}
    },
    {
      "id": "a4113ec0-85be-4a6a-827a-144f636bdc67",
      "cell_type": "markdown",
      "source": "## 2. El Modelo Face Mesh y Regresi√≥n de Puntos Clave\n\nEl sistema **Face Mesh** de MediaPipe predice la posici√≥n de 468 puntos (landmarks) en 3D. A diferencia de un detector de objetos simple que solo encuentra una caja delimitadora, este modelo realiza una **Regresi√≥n de Coordenadas de Alta Dimensionalidad**.\n\n### 2.1 Funci√≥n de P√©rdida y Optimizaci√≥n\nPara que el modelo aprenda a colocar los puntos correctamente, durante su entrenamiento se minimiza una **Funci√≥n de P√©rdida (Loss Function)**, generalmente basada en el **Error Cuadr√°tico Medio (MSE)** entre la posici√≥n predicha ($\\hat{P}$) y la real ($P$):\n\n$$\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} \\|\\vec{P}_i - \\hat{\\vec{P}}_i\\|^2$$\n\nDonde $n = 468$. El modelo utiliza el **Gradiente Descendente** para ajustar los pesos de la red y minimizar este error num√©rico.\n\n### 2.2 Reconstrucci√≥n de la Superficie (Topolog√≠a Fija)\nMediaPipe no solo entrega puntos aislados; los entrega con una **conectividad predefinida**. Esto significa que los 468 puntos forman una malla de tri√°ngulos fija.\n* **Justificaci√≥n:** Se utiliza una matriz de adyacencia constante para definir c√≥mo se conectan los v√©rtices. Esto permite calcular el vector normal ($\\vec{N}$) de cualquier parte de la cara mediante el producto cruz de las aristas del tri√°ngulo formado por los landmarks:\n$$\\vec{N}_{cara} = (\\vec{P}_{v2} - \\vec{P}_{v1}) \\times (\\vec{P}_{v3} - \\vec{P}_{v1})$$\n\n### 2.3 Estimaci√≥n de Profundidad Relativa\nUna de las mayores proezas num√©ricas de MediaPipe es obtener la coordenada $z$ (profundidad) a partir de una c√°mara 2D. Esto lo logra mediante **Inferencia de Profundidad**:\n1. El modelo asume un modelo de cabeza promedio (Weak Perspective Projection).\n2. La coordenada $z$ de los puntos se predice bas√°ndose en la deformaci√≥n y el tama√±o relativo de las facciones.\n3. El origen ($z=0$) se sit√∫a generalmente en el centro de masa de la cara o en el puente de la nariz.\n\n### 2.4 Alineaci√≥n Espacial: Transformaciones Afines\nPara alinear los filtros (lentes, sombreros) con la cara detectada, se resuelven transformaciones afines que mantienen el paralelismo de las l√≠neas. Si tenemos tres puntos de referencia en la cara (ojos y nariz), podemos calcular la **Matriz de Transformaci√≥n Af√≠n $A$** que mapea el objeto al espacio facial:\n\n$$\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$$\n\nEste sistema de ecuaciones se resuelve en cada frame para asegurar que el \"filtro\" siga el movimiento del usuario con precisi√≥n milim√©trica.",
      "metadata": {}
    },
    {
      "id": "b3a23061-217e-4fd6-9232-6e8cfd205786",
      "cell_type": "markdown",
      "source": "## 3. El Problema PnP y Estimaci√≥n de la Pose (Head Pose Estimation)\n\nPara que los filtros 3D se orienten correctamente, MediaPipe debe resolver el problema de **Perspectiva de n-Puntos (PnP)**. Esto consiste en encontrar la rotaci√≥n ($R$) y traslaci√≥n ($\\vec{t}$) de la cabeza a partir de las coordenadas 2D observadas en la imagen.\n\n### 3.1 El Modelo de C√°mara Pinhole\nEl sistema asume que los puntos del mundo 3D ($X, Y, Z$) se proyectan en el plano de la imagen ($u, v$) siguiendo la ecuaci√≥n de proyecci√≥n central:\n\n$$s \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{K} [ \\mathbf{R} | \\vec{t} ] \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$$\n\nDonde:\n* **$\\mathbf{K}$:** Es la matriz de par√°metros intr√≠nsecos de la c√°mara (focal, centro √≥ptico).\n* **$[\\mathbf{R}|\\vec{t}]$:** Es la matriz de par√°metros extr√≠nsecos (la pose que queremos hallar).\n* **$s$:** Es un factor de escala.\n\n### 3.2 Resoluci√≥n mediante Iteraci√≥n Num√©rica\nComo la imagen es una proyecci√≥n 2D de un objeto 3D, el problema est√° **sobredeterminado**. MediaPipe utiliza algoritmos como **Levenberg-Marquardt** para minimizar el **Error de Reproyecci√≥n**:\n\n$$\\min_{\\mathbf{R}, \\vec{t}} \\sum_{i=1}^{n} \\| p_i - \\text{proj}(\\mathbf{K}, \\mathbf{R}, \\vec{t}, P_i) \\|^2$$\n\nEste es un m√©todo num√©rico de optimizaci√≥n que ajusta la rotaci√≥n y posici√≥n hasta que los puntos del modelo 3D \"calzan\" perfectamente sobre los p√≠xeles de la cara detectada.\n\n### 3.3 Extracci√≥n de los √Ångulos de Euler (Tait-Bryan)\nUna vez obtenida la matriz de rotaci√≥n $\\mathbf{R}$, se descomponen los √°ngulos para aplicarlos en Three.js o A-Frame:\n* **Pitch (X):** Inclinaci√≥n arriba/abajo.\n* **Yaw (Y):** Giro izquierda/derecha.\n* **Roll (Z):** Inclinaci√≥n lateral.\n\nLa matriz de rotaci√≥n se define como el producto:\n$$\\mathbf{R} = R_z(\\psi)R_y(\\theta)R_x(\\phi)$$\n\n### 3.4 Estabilizaci√≥n de la Pose (Filtrado de Se√±al)\nDebido al ruido en la captura de video, los valores de $R$ y $t$ pueden oscilar. Para evitar esto, se aplican t√©cnicas de **Suavizado Exponencial**:\n\n$$S_t = \\alpha \\cdot X_t + (1 - \\alpha) \\cdot S_{t-1}$$\n\nDonde $\\alpha \\in [0, 1]$ es el factor de suavizado. Este m√©todo num√©rico permite que el sombrero o los lentes no \"tiemblen\" ante peque√±as variaciones de luz o movimiento del sensor.",
      "metadata": {}
    },
    {
      "id": "849d8e6f-6774-49f3-9b03-252c8b9d033f",
      "cell_type": "markdown",
      "source": "## 4. Estabilizaci√≥n de Datos: El Filtro \"One Euro\" ($1‚Ç¨$)\n\nEn la visi√≥n computacional en tiempo real, los landmarks detectados suelen presentar un fen√≥meno de \"jitter\" (vibraci√≥n aleatoria) debido a errores de cuantizaci√≥n en la imagen y ruido en el sensor. MediaPipe utiliza el **Filtro One Euro**, un filtro de paso bajo adaptativo de primer orden.\n\n### 4.1 La Limitaci√≥n de los Filtros Tradicionales\nUn filtro de paso bajo est√°ndar tiene un compromiso (trade-off) cr√≠tico:\n1. **Si el corte es bajo:** Se elimina el ruido, pero se introduce **lag** (retraso) en movimientos r√°pidos.\n2. **Si el corte es alto:** El sistema responde r√°pido, pero los puntos tiemblan cuando el usuario est√° quieto.\n\n### 4.2 El Modelo Matem√°tico Adaptativo\nEl filtro $1‚Ç¨$ resuelve esto ajustando din√°micamente su frecuencia de corte $f_c$ bas√°ndose en la velocidad de la se√±al. La ecuaci√≥n fundamental del filtro de paso bajo es:\n\n$$\\hat{X}_k = \\alpha X_k + (1 - \\alpha) \\hat{X}_{k-1}$$\n\nDonde el factor de suavizado $\\alpha$ se calcula a partir de la frecuencia de corte:\n$$\\alpha = \\frac{1}{1 + \\frac{1}{2\\pi \\cdot \\Delta t \\cdot f_c}}$$\n\n### 4.3 Adaptaci√≥n mediante la Velocidad (Derivada Discreta)\nLa clave num√©rica est√° en definir $f_c$ como una funci√≥n de la velocidad de cambio de la se√±al ($\\dot{X}$):\n\n1. **C√°lculo de la velocidad:** $\\dot{X}_k = \\frac{X_k - \\hat{X}_{k-1}}{\\Delta t}$\n2. **Suavizado de la velocidad:** $\\hat{\\dot{X}}_k = \\text{LowPass}(\\dot{X}_k, \\alpha_{speed})$\n3. **Frecuencia de corte adaptativa:** $f_c = f_{c_{min}} + \\beta |\\hat{\\dot{X}}_k|$\n\nDonde:\n* **$f_{c_{min}}$:** Evita el jitter cuando el usuario est√° quieto.\n* **$\\beta$:** Coeficiente de intensidad que aumenta la frecuencia de corte si hay mucha velocidad, reduciendo el lag.\n\n### 4.4 Justificaci√≥n en M√©todos Num√©ricos\nEste algoritmo representa un **controlador proporcional** simple aplicado al procesamiento de se√±ales. Permite que MediaPipe entregue coordenadas que se sienten \"f√≠sicas\" y naturales, eliminando la alta frecuencia del ruido sin sacrificar la respuesta inmediata del sistema ante movimientos bruscos.",
      "metadata": {}
    },
    {
      "id": "0534d5da-c57d-461c-baad-84d93eb88dab",
      "cell_type": "markdown",
      "source": "## 5. Procesamiento de Tensores y Aceleraci√≥n en GPU\n\nEl motor de c√°lculo de MediaPipe no opera sobre p√≠xeles individuales de forma secuencial, sino que utiliza **√Ålgebra Multilineal** para procesar la informaci√≥n mediante Tensores.\n\n### 5.1 ¬øQu√© es un Tensor en MediaPipe?\nUn tensor es una generalizaci√≥n de los vectores y matrices a $n$ dimensiones. En el caso del video, la entrada es un tensor de cuarto orden:\n$$\\mathbf{T} \\in \\mathbb{R}^{B \\times H \\times W \\times C}$$\nDonde:\n* **$B$:** Batch size (usualmente 1 en tiempo real).\n* **$H, W$:** Altura y ancho de la imagen.\n* **$C$:** Canales de color (3 para RGB).\n\n### 5.2 Cuantizaci√≥n y Optimizaci√≥n Num√©rica\nPara ejecutar estos modelos en navegadores o m√≥viles, MediaPipe utiliza **Cuantizaci√≥n**. Esto consiste en reducir la precisi√≥n de los pesos de la red neuronal de `float32` (32 bits) a `int8` (8 bits). \n\n**Justificaci√≥n Num√©rica:** Aunque se introduce un peque√±o error de redondeo, el volumen de datos se reduce en un **75%**, permitiendo que las operaciones de producto punto en las capas de la red se realicen mucho m√°s r√°pido en la GPU.\n\n### 5.3 Implementaci√≥n v√≠a WebGL/WebGPU\nMediaPipe delega las multiplicaciones matriciales a la GPU mediante **Shaders de Computaci√≥n**. Esto permite resolver el sistema de la red neuronal en paralelo:\n$$\\vec{y} = \\sigma(\\mathbf{W}\\vec{x} + \\vec{b})$$\nDonde $\\mathbf{W}$ es la matriz de pesos, $\\vec{x}$ los datos de entrada, $\\vec{b}$ el sesgo y $\\sigma$ la funci√≥n de activaci√≥n (ej. ReLU). Millones de estas operaciones ocurren cada 16ms.",
      "metadata": {}
    },
    {
      "id": "8d5005f4-333b-4686-abd6-2142009b906f",
      "cell_type": "markdown",
      "source": "## 5. Procesamiento de Tensores y Aceleraci√≥n en GPU\n\nEl motor de c√°lculo de MediaPipe no opera sobre p√≠xeles individuales de forma secuencial, sino que utiliza **√Ålgebra Multilineal** para procesar la informaci√≥n mediante Tensores.\n\n### 5.1 ¬øQu√© es un Tensor en MediaPipe?\nUn tensor es una generalizaci√≥n de los vectores y matrices a $n$ dimensiones. En el caso del video, la entrada es un tensor de cuarto orden:\n$$\\mathbf{T} \\in \\mathbb{R}^{B \\times H \\times W \\times C}$$\nDonde:\n* **$B$:** Batch size (usualmente 1 en tiempo real).\n* **$H, W$:** Altura y ancho de la imagen.\n* **$C$:** Canales de color (3 para RGB).\n\n### 5.2 Cuantizaci√≥n y Optimizaci√≥n Num√©rica\nPara ejecutar estos modelos en navegadores o m√≥viles, MediaPipe utiliza **Cuantizaci√≥n**. Esto consiste en reducir la precisi√≥n de los pesos de la red neuronal de `float32` (32 bits) a `int8` (8 bits). \n\n**Justificaci√≥n Num√©rica:** Aunque se introduce un peque√±o error de redondeo, el volumen de datos se reduce en un **75%**, permitiendo que las operaciones de producto punto en las capas de la red se realicen mucho m√°s r√°pido en la GPU.\n\n### 5.3 Implementaci√≥n v√≠a WebGL/WebGPU\nMediaPipe delega las multiplicaciones matriciales a la GPU mediante **Shaders de Computaci√≥n**. Esto permite resolver el sistema de la red neuronal en paralelo:\n$$\\vec{y} = \\sigma(\\mathbf{W}\\vec{x} + \\vec{b})$$\nDonde $\\mathbf{W}$ es la matriz de pesos, $\\vec{x}$ los datos de entrada, $\\vec{b}$ el sesgo y $\\sigma$ la funci√≥n de activaci√≥n (ej. ReLU). Millones de estas operaciones ocurren cada 16ms.",
      "metadata": {}
    },
    {
      "id": "d7b17a5e-99f6-42cc-b056-42fe216fcddd",
      "cell_type": "markdown",
      "source": "## 6. Integraci√≥n T√©cnica: El V√≠nculo MediaPipe - Three.js\n\nEn tu implementaci√≥n, el flujo de datos representa un ciclo completo de **M√©todos Num√©ricos Aplicados**:\n\n1. **Captura:** La c√°mara obtiene la matriz de p√≠xeles $I$.\n2. **Inferencia (MediaPipe):** Se resuelve la regresi√≥n para obtener los landmarks $\\vec{P}_i$.\n3. **Mapeo:** Se transforman las coordenadas normalizadas $[0,1]$ al espacio euclidiano $[-n, n]$.\n4. **Transformaci√≥n (Three.js):** Se actualizan las matrices de rotaci√≥n y traslaci√≥n de los objetos 3D.\n5. **Render:** Se proyectan los objetos 3D de nuevo a 2D para el monitor.\n\n### 6.1 Ejemplo de L√≥gica de Uni√≥n (Fragmento del c√≥digo)\n```javascript\n// La regresi√≥n de MediaPipe entrega 'f' (landmarks)\nconst f = res.multiFaceLandmarks[0];\nconst le = f[33], re = f[263]; // Ojo Izquierdo y Derecho\n\n// C√°lculo de la distancia euclidiana (Norma L2) para la escala\nconst eyeDist = Math.abs(le.x - re.x) * 3;\n\n// Aplicaci√≥n de la transformaci√≥n af√≠n (Traslaci√≥n)\nglasses.position.set(centerX, centerY, -0.35);",
      "metadata": {}
    },
    {
      "id": "5ea0a6f0-e7d4-4569-88ea-ead18a42a643",
      "cell_type": "markdown",
      "source": "### 6.2 Resumen Final de la Tecnolog√≠a\n\n| Proceso | M√©todo Num√©rico / Matem√°tico |\n| :--- | :--- |\n| **Detecci√≥n de Objetos** | Convoluci√≥n Discreta y Redes Neuronales Convolucionales (CNN). |\n| **Landmarks (Puntos Clave)** | Regresi√≥n No-lineal de alta dimensionalidad sobre espacios latentes. |\n| **Estabilidad de Se√±al** | Filtros de Paso Bajo adaptativos (Filtro One Euro). |\n| **Alineaci√≥n 3D** | Resoluci√≥n del problema PnP (Perspective-n-Point) y Optimizaci√≥n de Reproyecci√≥n. |\n| **Rendimiento** | Cuantizaci√≥n de tensores y computaci√≥n paralela en GPU. |\n\n---\n\n> **Conclusi√≥n de MediaPipe:** Esta tecnolog√≠a representa la frontera de los m√©todos num√©ricos aplicados, donde la resoluci√≥n de sistemas de ecuaciones no se hace de forma manual, sino que se delega a modelos de aprendizaje autom√°tico que optimizan billones de par√°metros para transformar datos visuales brutos en informaci√≥n geom√©trica √∫til.",
      "metadata": {}
    },
    {
      "id": "65f9788d-29f8-4ce6-bccc-5f9df5def8d1",
      "cell_type": "code",
      "source": "from IPython.display import display, HTML\n\n# Guardamos todo el c√≥digo HTML/JS en una variable de texto de Python\ncodigo_html = \"\"\"\n<div id=\"main-container\">\n    <div class=\"panel\">\n      <button id=\"btn-glasses\">üëì Lentes</button>\n      <button id=\"btn-mustache\">ü•∏ Bigote</button>\n      <button id=\"btn-hat\">üé© Sombrero</button>\n      <button id=\"btn-mole\">üü§ Lunar</button>\n      <button id=\"btn-points\">üü¢ FaceMesh</button>\n    </div>\n\n    <video id=\"videoElement\" autoplay muted playsinline></video>\n    <canvas id=\"canvasElement\"></canvas>\n    <div id=\"status\">Cargando sistema...</div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\"></script>\n\n<style>\n    #main-container {\n        position: relative;\n        width: 640px;\n        height: 480px;\n        background: black;\n        margin: 0 auto;\n        border: 2px solid #444;\n        font-family: sans-serif;\n        overflow: hidden;\n    }\n    video, canvas {\n        position: absolute;\n        top: 0; left: 0;\n        width: 100%; height: 100%;\n        object-fit: cover;\n    }\n    .panel {\n        position: absolute;\n        top: 10px; left: 10px;\n        z-index: 50;\n        display: flex;\n        flex-direction: column;\n        gap: 5px;\n    }\n    button {\n        width: 140px;\n        padding: 8px;\n        cursor: pointer;\n        font-weight: bold;\n        border: 1px solid #999;\n        border-radius: 4px;\n        background: white;\n        transition: background 0.2s;\n    }\n    /* Clase activa (Verde) */\n    button.active {\n        background-color: #76ff03 !important;\n        border-color: #4caf50;\n    }\n    #status {\n        position: absolute;\n        bottom: 10px; left: 10px;\n        color: white;\n        background: rgba(0,0,0,0.5);\n        padding: 5px;\n        z-index: 40;\n        pointer-events: none;\n    }\n</style>\n\n<script>\n(function() {\n    // 1. Referencias\n    const container = document.getElementById('main-container');\n    if (!container) return; // Prevenir errores si se re-ejecuta\n    \n    const video = container.querySelector(\"#videoElement\");\n    const canvas = container.querySelector(\"#canvasElement\");\n    const ctx = canvas.getContext(\"2d\");\n    const statusDiv = container.querySelector(\"#status\");\n\n    // 2. Estado de los filtros\n    const filters = {\n        glasses: false,\n        mustache: false,\n        hat: false,\n        mole: false,\n        points: false\n    };\n\n    // 3. CONEXI√ìN DE BOTONES\n    const buttons = {\n        'btn-glasses': 'glasses',\n        'btn-mustache': 'mustache',\n        'btn-hat': 'hat',\n        'btn-mole': 'mole',\n        'btn-points': 'points'\n    };\n\n    // Asignar eventos clic\n    for (const [btnId, filterName] of Object.entries(buttons)) {\n        const btn = container.querySelector(\"#\"+btnId);\n        if (btn) {\n            btn.onclick = function() {\n                filters[filterName] = !filters[filterName];\n                // Feedback Visual\n                if (filters[filterName]) {\n                    btn.classList.add('active');\n                } else {\n                    btn.classList.remove('active');\n                }\n            };\n        }\n    }\n\n    // 4. Ajustar Canvas\n    function resize() {\n        canvas.width = 640;\n        canvas.height = 480;\n    }\n    resize();\n\n    // 5. Configurar FaceMesh\n    const faceMesh = new FaceMesh({\n        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n    });\n\n    faceMesh.setOptions({\n        maxNumFaces: 1,\n        refineLandmarks: true,\n        minDetectionConfidence: 0.5,\n        minTrackingConfidence: 0.5\n    });\n\n    faceMesh.onResults(onResults);\n\n    // 6. Dibujar Resultados\n    function onResults(res) {\n        statusDiv.innerText = \"Rostro detectado ‚úÖ\";\n        ctx.clearRect(0, 0, canvas.width, canvas.height);\n        \n        // Dibujar imagen de video\n        ctx.drawImage(res.image, 0, 0, canvas.width, canvas.height);\n\n        if (res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0) {\n            const landmarks = res.multiFaceLandmarks[0];\n            drawFilters(landmarks, canvas.width, canvas.height);\n        } else {\n            statusDiv.innerText = \"Buscando rostro...\";\n        }\n    }\n\n    function drawFilters(f, w, h) {\n        // Puntos clave\n        const le = f[33];   // Ojo izq\n        const re = f[263];  // Ojo der\n        const lip = f[13];  // Labio\n        const cheek = f[50];// Mejilla\n\n        // Geometr√≠a\n        const cx = (le.x + re.x) / 2 * w;\n        const cy = (le.y + re.y) / 2 * h;\n        const dx = (re.x - le.x) * w;\n        const dy = (re.y - le.y) * h;\n        const dist = Math.sqrt(dx*dx + dy*dy);\n\n        if (filters.points) {\n            ctx.fillStyle = \"#00ff00\";\n            for (let p of f) {\n                ctx.beginPath();\n                ctx.arc(p.x * w, p.y * h, 1, 0, 2 * Math.PI);\n                ctx.fill();\n            }\n        }\n\n        if (filters.glasses) {\n            ctx.strokeStyle = \"black\";\n            ctx.lineWidth = 5;\n            ctx.strokeRect(cx - dist*0.8, cy - dist*0.3, dist*0.7, dist*0.4);\n            ctx.strokeRect(cx + dist*0.1, cy - dist*0.3, dist*0.7, dist*0.4);\n            ctx.beginPath();\n            ctx.moveTo(cx - dist*0.1, cy - dist*0.1);\n            ctx.lineTo(cx + dist*0.1, cy - dist*0.1);\n            ctx.stroke();\n        }\n\n        if (filters.mustache) {\n            const mx = lip.x * w;\n            const my = lip.y * h - dist * 0.15;\n            ctx.strokeStyle = \"black\"; \n            ctx.lineWidth = dist * 0.2;\n            ctx.lineCap = \"round\";\n            ctx.beginPath();\n            ctx.moveTo(mx - dist * 0.5, my + dist * 0.1);\n            ctx.quadraticCurveTo(mx, my - dist * 0.2, mx + dist * 0.5, my + dist * 0.1);\n            ctx.stroke();\n        }\n\n        if (filters.hat) {\n            ctx.fillStyle = \"#5d4037\";\n            ctx.fillRect(cx - dist, cy - dist * 1.8, dist * 2, dist * 0.8);\n            ctx.fillRect(cx - dist * 1.4, cy - dist, dist * 2.8, dist * 0.2);\n        }\n        \n        if (filters.mole) {\n             ctx.fillStyle = \"#3e2723\";\n             ctx.beginPath();\n             ctx.arc(cheek.x*w, cheek.y*h, dist*0.1, 0, Math.PI*2);\n             ctx.fill();\n        }\n    }\n\n    // 7. Iniciar C√°mara\n    const camera = new Camera(video, {\n        onFrame: async () => {\n            await faceMesh.send({image: video});\n        },\n        width: 640,\n        height: 480\n    });\n\n    statusDiv.innerText = \"Solicitando c√°mara...\";\n    camera.start()\n        .then(() => statusDiv.innerText = \"C√°mara lista.\")\n        .catch(err => statusDiv.innerText = \"Error: \" + err);\n\n})();\n</script>\n\"\"\"\n\n# Esta l√≠nea final es la que hace la magia de mostrar el HTML\ndisplay(HTML(codigo_html))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}