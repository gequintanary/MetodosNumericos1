{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4b9882e9-3534-4819-9462-a1cb9df0d857",
      "cell_type": "markdown",
      "source": "# Algoritmo de Sobel\n\n## 1. Sustento Matemático\nEl operador de Sobel calcula una aproximación discreta del **gradiente** de la intensidad de una imagen. Este método está fundamentado en el concepto de **diferencias finitas** para aproximar las derivadas parciales de la función de imagen.\n\n### 1.1 Definición del Gradiente\nPara una función continua $f(x, y)$, el gradiente se define como un vector que apunta en la dirección de mayor cambio de intensidad:\n\n$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n\n### 1.2 Magnitud del Gradiente\nLa magnitud del gradiente representa la \"fuerza\" del borde y se calcula mediante la norma del vector:\n\n$$|\\nabla f| = \\sqrt{\\left( \\frac{\\partial f}{\\partial x} \\right)^2 + \\left( \\frac{\\partial f}{\\partial y} \\right)^2}$$",
      "metadata": {}
    },
    {
      "id": "6763bc84-fdcd-426c-8be4-e33f43a3714c",
      "cell_type": "markdown",
      "source": "## 2. Relación con Métodos Numéricos: Diferencias Finitas\n\n### 2.1 Aproximación de Derivadas\nEl método de Sobel utiliza **diferencias finitas ponderadas**, una técnica clásica de métodos numéricos para aproximar derivadas cuando solo disponemos de datos discretos (como los píxeles de una imagen).\n\n* **Derivada Parcial en X (dirección horizontal):**\n    Representa el cambio de intensidad a lo largo de las columnas:\n    $$\\frac{\\partial f}{\\partial x} \\approx G_x = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix} * I$$\n\n* **Derivada Parcial en Y (dirección vertical):**\n    Representa el cambio de intensidad a lo largo de las filas:\n    $$\\frac{\\partial f}{\\partial y} \\approx G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix} * I$$\n\n### 2.2 Interpretación como Operador de Diferenciación\n| Aspecto Numérico | Aplicación en Sobel |\n| :--- | :--- |\n| **Diferencias centrales** | Máscara con valores $-1, 0, +1$ |\n| **Suavizado (regularización)** | Coeficientes $1, 2, 1$ (promedio ponderado) |\n| **Convolución discreta** | Operador aplicado sobre vecindad $3 \\times 3$ |",
      "metadata": {}
    },
    {
      "id": "3e21483a-d68b-45e4-8fb3-9c39ba897b2e",
      "cell_type": "markdown",
      "source": "## 3. Pseudocódigo del Algoritmo\n\n### ALGORITMO SOBEL(I: imagen de entrada)\n\n**ENTRADA:** $I[m][n]$ (matriz de intensidades en escala de grises)  \n**SALIDA:** $G[m][n]$ (Magnitud del gradiente), $\\theta[m][n]$ (Dirección del gradiente)\n\n**PASOS:**\n\n1. **INICIALIZAR:** Matrices $G_x, G_y, G, \\theta$ con ceros (del mismo tamaño que $I$).\n2. **DEFINIR** máscaras de convolución:\n\n   **Máscara X:**\n   $$\\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}$$\n\n   **Máscara Y:**\n   $$\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}$$\n\n3. **PARA** cada píxel $(i, j)$ donde $1 \\leq i \\leq m-2$ y $1 \\leq j \\leq n-2$:\n   \n   a. **Extraer** vecindad $3 \\times 3$ centrada en $(i, j)$: $V = I[i-1:i+2, j-1:j+2]$\n   \n   b. **Calcular** gradiente horizontal: $G_x[i][j] = \\sum (V \\odot MascaraX)$\n   \n   c. **Calcular** gradiente vertical: $G_y[i][j] = \\sum (V \\odot MascaraY)$\n   \n   d. **Calcular** magnitud: $G[i][j] = \\sqrt{G_x[i][j]^2 + G_y[i][j]^2}$\n   \n   e. **Calcular** dirección (opcional): $\\theta[i][j] = \\arctan2(G_y[i][j], G_x[i][j])$\n\n4. **NORMALIZAR** $G$ al rango $[0, 255]$ si es necesario.\n5. **RETORNAR** $G, \\theta$",
      "metadata": {}
    },
    {
      "id": "b9196316-9850-4dbf-b7f7-90fd05ad30ba",
      "cell_type": "markdown",
      "source": "## 4. Desarrollo Teórico (Series de Taylor)\n\nPara fundamentar el error de aproximación, analizamos las expansiones de Taylor:\n\n### Para Diferencias Progresivas\n$$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}{2}f''(x) + O(h^2)$$\n$$\\underbrace{Error: O(h)}_{\\text{Primer orden}}$$\n\n### Para Diferencias Regresivas\n$$f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x) - f(x-h)}{h} + \\frac{h}{2}f''(x) + O(h^2)$$\n$$\\underbrace{Error: O(h)}_{\\text{Primer orden}}$$\n\n### Para Diferencias Centrales (Sobel)\nRestando las expansiones de Taylor anteriores:\n$$f(x+h) - f(x-h) = 2hf'(x) + O(h^3)$$\n\n**Despejando:**\n$$f'(x) = \\frac{f(x+h) - f(x-h)}{2h} + O(h^2)$$\n$$\\underbrace{Error: O(h^2)}_{\\text{Segundo orden (Más preciso)}}$$\n",
      "metadata": {}
    },
    {
      "id": "87cd8818-a918-46ec-96a9-854eba0f8de8",
      "cell_type": "markdown",
      "source": "## 5. Comparación de Operadores\n\nDependiendo del método numérico elegido para aproximar la derivada, las máscaras (kernels) resultantes cambian su estructura y precisión:\n\n### Sobel con Diferencias Regresivas (No estándar)\nEsta versión utiliza una aproximación hacia atrás. Aunque es computacionalmente sencilla, no es la más utilizada porque tiende a desplazar visualmente la posición de los bordes en la imagen:\n\n$$\\begin{bmatrix} -1 & 1 & 0 \\\\ -2 & 2 & 0 \\\\ -1 & 1 & 0 \\end{bmatrix}$$\n\n### Sobel Estándar (Diferencias Centrales)\nEs el estándar de la industria y el que se implementa en la mayoría de las librerías. Al usar diferencias centrales, logra una mejor simetría y un error de segundo orden $O(h^2)$:\n\n**Gradiente en X ($G_x$):**\n$$G_x = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}$$\n\n**Gradiente en Y ($G_y$):**\n$$G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}$$",
      "metadata": {}
    },
    {
      "id": "42c16de8-afd8-4525-a1b6-82971edd0384",
      "cell_type": "markdown",
      "source": "## 6. Interpretación Geométrica\n\nDesde una perspectiva geométrica, el algoritmo de Sobel no solo detecta cambios, sino que interpreta la imagen como una **superficie continua de intensidades**, donde cada píxel tiene una \"altura\" basada en su valor de gris.\n\n### 1. El Plano Tangente\nAl calcular $G_x$ y $G_y$, el algoritmo está estimando la inclinación de un **plano tangente** a la superficie de la imagen en el punto $(i, j)$. \n* $G_x$ representa la pendiente del plano en dirección este-oeste.\n* $G_y$ representa la pendiente del plano en dirección norte-sur.\n\n### 2. El Vector Gradiente\nEl vector $\\nabla f = (G_x, G_y)$ es perpendicular a las líneas de contorno de la imagen. \n* Su **dirección** $\\theta$ apunta hacia donde la intensidad crece más rápidamente.\n* Su **magnitud** $|G|$ indica qué tan \"empinada\" es la pendiente. Un borde es, simplemente, una zona con una pendiente muy pronunciada.\n\n### 3. Visualización de Bordes\nEn la práctica, los bordes detectados corresponden a los lugares donde la superficie de intensidad tiene su máxima tasa de cambio. Geométricamente, el operador de Sobel actúa como un **filtro de paso alto**, eliminando las zonas planas (frecuencias bajas) y resaltando las transiciones abruptas (frecuencias altas).",
      "metadata": {}
    },
    {
      "id": "459688ec-2681-4547-8cbc-a8f692b3394c",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "274ff0f0-e39d-47e5-9175-2921fa874804",
      "cell_type": "markdown",
      "source": "# Algoritmo de Canny (Canny Edge Detector)\n\n## 1. Sustento Matemático (Criterios de Optimización)\nA diferencia de otros operadores, el detector de bordes de Canny se diseñó como un problema de optimización matemática. John Canny estableció tres criterios fundamentales para considerar una detección de bordes como \"óptima\":\n\n### 1.1 Criterios de Calidad\n* **Baja Tasa de Error (Detección):** El algoritmo debe marcar tantos bordes reales como sea posible y evitar falsos positivos causados por el ruido.\n* **Buena Localización:** La distancia entre el píxel marcado como borde y el centro del borde real debe ser mínima.\n* **Respuesta Única:** Un solo borde real no debe generar múltiples bordes detectados (evitar el \"efecto de doble borde\").\n\n### 1.2 Formulación Matemática\nPara lograr esto, se busca una función $f(x)$ que maximice el producto de la relación señal-ruido ($SNR$) y la localización ($L$):\n\n$$SNR = \\frac{|\\int_{-W}^{W} G(-x)f(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} f^2(x)dx}}$$\n\n$$L = \\frac{|\\int_{-W}^{W} G'(-x)f'(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} (f')^2(x)dx}}$$\n\nDonde:\n* $G(x)$ representa la señal del borde.\n* $f(x)$ es el filtro aplicado.\n* $\\sigma$ es la desviación estándar del ruido.",
      "metadata": {}
    },
    {
      "id": "e958b92b-c224-4c66-8664-9e4b933fa1b6",
      "cell_type": "markdown",
      "source": "# Algoritmo de Canny (Canny Edge Detector)\n\n## 1. Sustento Matemático (Criterios de Optimización)\nA diferencia de otros operadores, el detector de bordes de Canny se diseñó como un problema de optimización matemática. John Canny estableció tres criterios fundamentales para considerar una detección de bordes como \"óptima\":\n\n### 1.1 Criterios de Calidad\n* **Baja Tasa de Error (Detección):** El algoritmo debe marcar tantos bordes reales como sea posible y evitar falsos positivos causados por el ruido.\n* **Buena Localización:** La distancia entre el píxel marcado como borde y el centro del borde real debe ser mínima.\n* **Respuesta Única:** Un solo borde real no debe generar múltiples bordes detectados (evitar el \"efecto de doble borde\").\n\n### 1.2 Formulación Matemática\nPara lograr esto, se busca una función $f(x)$ que maximice el producto de la relación señal-ruido ($SNR$) y la localización ($L$):\n\n$$SNR = \\frac{|\\int_{-W}^{W} G(-x)f(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} f^2(x)dx}}$$\n\n$$L = \\frac{|\\int_{-W}^{W} G'(-x)f'(x)dx|}{\\sigma \\sqrt{\\int_{-W}^{W} (f')^2(x)dx}}$$\n\nDonde:\n* $G(x)$ representa la señal del borde.\n* $f(x)$ es el filtro aplicado.\n* $\\sigma$ es la desviación estándar del ruido.",
      "metadata": {}
    },
    {
      "id": "efb873d3-df75-432e-9593-31e27b3cf859",
      "cell_type": "markdown",
      "source": "## 2. Relación con Métodos Numéricos: Regularización\n\n### 2.1 Suavizado Gaussiano\nDado que el cálculo de derivadas es una operación que amplifica el ruido (frecuencias altas), el algoritmo de Canny aplica una etapa de **suavizado** o **regularización** previa. Esto se basa en la convolución de la imagen original $I$ con un núcleo (kernel) Gaussiano.\n\nLa función Gaussiana en 2D se define como:\n\n$$G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}$$\n\n### 2.2 Discretización del Kernel\nEn métodos numéricos, aproximamos esta función continua mediante una máscara discreta. Por ejemplo, para un $\\sigma = 1.4$, un kernel $5 \\times 5$ aproximado es:\n\n$$K = \\frac{1}{273} \\begin{bmatrix} 1 & 4 & 7 & 4 & 1 \\\\ 4 & 16 & 26 & 16 & 4 \\\\ 7 & 26 & 41 & 26 & 7 \\\\ 4 & 16 & 26 & 16 & 4 \\\\ 1 & 4 & 7 & 4 & 1 \\end{bmatrix}$$\n\n### 2.3 Operación de Convolución\nLa imagen suavizada $I_{suave}$ se obtiene mediante la suma ponderada en la vecindad del píxel:\n\n$$I_{suave}(i, j) = \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I(i-m, j-n) \\cdot K(m, n)$$\n\n> **Importante:** Esta etapa reduce el error en la aproximación de las derivadas parciales que se calcularán en el siguiente paso.",
      "metadata": {}
    },
    {
      "id": "0ac0c641-7acf-47eb-adf6-55c6a02e87e9",
      "cell_type": "markdown",
      "source": "## 3. Cálculo de Gradiente y Dirección\n\nUna vez suavizada la imagen, el algoritmo de Canny utiliza operadores de diferenciación (como Sobel) para encontrar las variaciones de intensidad en las direcciones $x$ e $y$.\n\n### 3.1 Componentes del Gradiente\nSe aplican las máscaras de diferencias finitas para obtener las derivadas parciales:\n\n* **Gradiente Horizontal:** $G_x = M_x * I_{suave}$\n* **Gradiente Vertical:** $G_y = M_y * I_{suave}$\n\n### 3.2 Magnitud y Orientación\nA partir de estas componentes, se calcula la magnitud total del cambio y la dirección del vector gradiente:\n\n* **Magnitud del Gradiente:**\n    $$M(i, j) = \\sqrt{G_x(i, j)^2 + G_y(i, j)^2}$$\n\n* **Ángulo de Orientación:**\n    $$\\alpha(i, j) = \\arctan\\left(\\frac{G_y(i, j)}{G_x(i, j)}\\right)$$\n\n### 3.3 Cuantización de Direcciones\nPara el procesamiento numérico posterior, el ángulo $\\alpha$ se redondea a uno de los cuatro sectores de una vecindad $3 \\times 3$:\n\n| Ángulo Aproximado | Dirección del Borde |\n| :--- | :--- |\n| **$0^\\circ$** | Horizontal |\n| **$45^\\circ$** | Diagonal Positiva |\n| **$90^\\circ$** | Vertical |\n| **$135^\\circ$** | Diagonal Negativa |",
      "metadata": {}
    },
    {
      "id": "5f6fb268-ef5f-4d99-95ba-7fbc5647c060",
      "cell_type": "markdown",
      "source": "## 4. Supresión de No Máximos (Non-Maximum Suppression)\n\nEste proceso es una técnica de **adelgazamiento de bordes**. Su objetivo es eliminar los píxeles que no forman parte de la cresta del gradiente, dejando únicamente los máximos locales.\n\n### 4.1 Lógica Algorítmica\nPara cada píxel $(i, j)$, el algoritmo compara su magnitud $M(i, j)$ con las magnitudes de los dos píxeles vecinos en la dirección del gradiente $\\alpha(i, j)$.\n\n* Si $M(i, j)$ es la mayor de las tres, el píxel se conserva.\n* Si no es la mayor, se suprime asignándole un valor de $0$.\n\n### 4.2 Esquema de Comparación según la Dirección\nDependiendo del ángulo cuantizado, se eligen los vecinos específicos:\n\n| Ángulo ($\\alpha$) | Vecino 1 | Vecino 2 | Eje de Comparación |\n| :--- | :---: | :---: | :--- |\n| **$0^\\circ$** | $(i, j+1)$ | $(i, j-1)$ | Horizontal |\n| **$45^\\circ$** | $(i-1, j+1)$ | $(i+1, j-1)$ | Diagonal Positiva |\n| **$90^\\circ$** | $(i-1, j)$ | $(i+1, j)$ | Vertical |\n| **$135^\\circ$** | $(i-1, j-1)$ | $(i+1, j+1)$ | Diagonal Negativa |\n\n### 4.3 Resultado Numérico\nEl resultado de esta operación es una imagen de bordes delgados (frecuentemente llamada *Thin Edges*), definida matemáticamente como:\n\n$$M_{thin}(i, j) = \\begin{cases} M(i, j) & \\text{si } M(i, j) > M_{vecinos} \\\\ 0 & \\text{en otro caso} \\end{cases}$$",
      "metadata": {}
    },
    {
      "id": "474c44d5-06da-4150-bd6a-a5e9533d17b6",
      "cell_type": "markdown",
      "source": "## 5. Umbralización con Histéresis\n\nEste paso determina qué bordes son genuinos y cuáles deben descartarse. A diferencia de una umbralización simple, Canny utiliza dos niveles de decisión para mantener la conectividad de los bordes.\n\n### 5.1 Definición de Umbrales\nSe establecen dos valores críticos:\n* **$T_{high}$ (Umbral Superior):** Define los bordes de alta confianza.\n* **$T_{low}$ (Umbral Inferior):** Permite la inclusión de bordes más débiles siempre que tengan sustento estructural.\n\n### 5.2 Clasificación de Píxeles\nPara cada píxel de la imagen resultante del paso anterior, se aplica la siguiente lógica:\n\n| Categoría | Condición de Magnitud ($M$) | Decisión Final |\n| :--- | :--- | :--- |\n| **Borde Fuerte** | $M \\geq T_{high}$ | Se conserva (Píxel de borde definitivo) |\n| **Borde Débil** | $T_{low} \\leq M < T_{high}$ | Se conserva **solo si** está conectado a uno fuerte |\n| **No-Borde** | $M < T_{low}$ | Se elimina (Valor 0) |\n\n### 5.3 Análisis de Conectividad (Hysteresis)\nMatemáticamente, un píxel débil $W$ en la posición $(i, j)$ se convierte en borde si existe un píxel fuerte $S$ en su vecindad de Moore ($8$ vecinos):\n\n$$B(i, j) = \\begin{cases} 1 & \\text{si } M(i, j) \\geq T_{high} \\\\ 1 & \\text{si } T_{low} \\leq M(i, j) < T_{high} \\text{ y } \\exists \\text{ fuerte en vecindad} \\\\ 0 & \\text{en otro caso} \\end{cases}$$\n\n> **Ventaja:** Este método evita que un borde se \"rompa\" debido a pequeñas variaciones de intensidad a lo largo de su trayectoria.",
      "metadata": {}
    },
    {
      "id": "652065e7-f8e1-47a5-b2a4-6b8df2f631fd",
      "cell_type": "markdown",
      "source": "## 6. Pseudocódigo del Algoritmo de Canny\n\n**ALGORITMO CANNY_EDGE_DETECTOR($I$, $\\sigma$, $T_{low}$, $T_{high}$)**\n\n**ENTRADA:** * $I$: Imagen original en escala de grises.  \n* $\\sigma$: Desviación estándar para el suavizado Gaussiano.  \n* $T_{low}, T_{high}$: Umbrales para la histéresis.\n\n**SALIDA:** * $E$: Imagen binaria con los bordes detectados.\n\n**PASOS:**\n\n1. **REDUCCIÓN DE RUIDO:** Aplicar filtro Gaussiano de tamaño $(2k+1) \\times (2k+1)$ para obtener $I_{suave}$.  \n   $$I_{suave} = I * G_{\\sigma}$$\n\n2. **CÁLCULO DE GRADIENTES:** Aplicar máscaras de Sobel para obtener derivadas parciales $G_x$ y $G_y$.  \n   Calcular magnitud $M$ y dirección $\\alpha$ para cada píxel.\n\n3. **SUPRESIÓN DE NO MÁXIMOS:** **PARA** cada píxel $(i, j)$ en $M$:  \n   * Determinar dirección del gradiente cuantizada (0, 45, 90, 135).  \n   * Comparar $M(i, j)$ con vecinos en dicha dirección.  \n   * **SI** $M(i, j)$ no es el máximo local, $M_{thin}(i, j) = 0$.\n\n4. **UMBRALIZACIÓN DOBLE:** Clasificar píxeles en $M_{thin}$ como **Fuertes**, **Débiles** o **Suprimidos** según $T_{low}$ y $T_{high}$.\n\n5. **SEGUIMIENTO DE BORDES (HISTERESIS):** **PARA** cada píxel débil:  \n   * **SI** está conectado a un píxel fuerte (vecindad de 8), marcar como **Borde**.  \n   * **SINO**, marcar como **Fondo**.\n\n6. **RETORNAR** Imagen binaria final $E$.",
      "metadata": {}
    },
    {
      "id": "963894ad-1b42-4468-b87b-13a4baceb1d5",
      "cell_type": "markdown",
      "source": "## 7. Interpretación Geométrica y Comparativa\n\n### 7.1 La Geometría de la Cresta\nDesde una perspectiva geométrica, mientras que Sobel identifica una \"zona de cambio\" (una rampa de intensidad), Canny busca la **línea de cresta** exacta de esa rampa. La supresión de no máximos actúa como una operación de proyección que colapsa la pendiente del gradiente en un solo punto geométrico de máxima curvatura.\n\n### 7.2 Comparativa: Sobel vs. Canny\n\n| Característica | Operador de Sobel | Detector de Canny |\n| :--- | :--- | :--- |\n| **Complejidad** | Baja (1 solo paso) | Alta (múltiples etapas) |\n| **Grosor del Borde** | Grueso (varios píxeles) | Delgado (exactamente 1 píxel) |\n| **Sensibilidad al Ruido** | Alta | Baja (gracias al suavizado previo) |\n| **Continuidad** | Bordes suelen presentar huecos | Bordes continuos (por histéresis) |\n| **Precisión Matemática** | $O(h^2)$ en derivación | Óptima según criterios de SNR y Localización |\n\n### 7.3 Conclusión\nEl método de Canny representa la evolución del sustento matemático de las **diferencias finitas**. No se limita a calcular la derivada parcial, sino que aplica una cadena de filtros numéricos (Gauss, Sobel, Supresión y Conectividad) para transformar una señal ruidosa en una representación geométrica precisa de los bordes.",
      "metadata": {}
    },
    {
      "id": "c6f3fc2c-a4ff-41a0-a79f-f540bc9ec05b",
      "cell_type": "markdown",
      "source": "# A-Frame: Framework de Realidad Virtual y Gráficos 3D\n\n## 1. Sustento Tecnológico y el Grafo de Escena\n\nA-Frame es un framework web de código abierto basado en la arquitectura **Entity-Component-System (ECS)** que se ejecuta sobre WebGL. Desde la perspectiva de los métodos numéricos, A-Frame es un motor que gestiona un **Grafo de Escena**, donde cada nodo representa una transformación espacial.\n\n### 1.1 El Grafo de Escena como Estructura de Datos\nEl sistema organiza los objetos en una jerarquía de árbol. La posición, rotación y escala de cualquier entidad se calculan mediante la acumulación de transformaciones de sus ancestros.\n\n### 1.2 Justificación Matemática: Transformaciones Homogéneas\nCualquier entidad $E$ en el espacio 3D se representa mediante una **Matriz de Transformación Homogénea $4 \\times 4$**. Esto permite unificar traslación, rotación y escala en una sola operación matricial:\n\n$$M = \\begin{bmatrix} \nr_{11} & r_{12} & r_{13} & t_x \\\\ \nr_{21} & r_{22} & r_{23} & t_y \\\\ \nr_{31} & r_{32} & r_{33} & t_z \\\\ \n0 & 0 & 0 & 1 \n\\end{bmatrix}$$\n\nDonde:\n* El subbloque $3 \\times 3$ superior izquierdo gestiona la **Rotación ($R$)** y la **Escala ($S$)**.\n* La columna $[t_x, t_y, t_z]^T$ gestiona la **Traslación ($T$)**.\n\nLa posición final de un objeto \"hijo\" ($P_{final}$) respecto al origen global se obtiene mediante la multiplicación de matrices de su jerarquía:\n$$P_{final} = M_{padre} \\cdot M_{hijo} \\cdot P_{local}$$\n\n### 1.3 Coordenadas y Unidades\nA-Frame utiliza un sistema de mano derecha (Right-handed coordinate system):\n* **Eje X:** Positivo hacia la derecha.\n* **Eje Y:** Positivo hacia arriba.\n* **Eje Z:** Positivo hacia afuera de la pantalla (hacia el usuario).\n\n> **Nota de Aplicación:** En VR, la unidad de medida está estandarizada como $1 \\text{ unidad} = 1 \\text{ metro}$, lo cual es crucial para la precisión del cálculo de la cámara y la percepción de profundidad del usuario.",
      "metadata": {}
    },
    {
      "id": "6396a6c9-e189-41c3-8e4e-057e642968d1",
      "cell_type": "markdown",
      "source": "## 2. Geometría Analítica y Álgebra Vectorial\n\nEn A-Frame, cada objeto y cada interacción se define mediante vectores en $\\mathbb{R}^3$. La manipulación de estos vectores permite resolver problemas de posicionamiento, orientación y detección de colisiones.\n\n### 2.1 Representación Vectorial\nCualquier punto o dirección en el espacio se define por un vector columna:\n$$\\vec{v} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$$\n\nPara garantizar cálculos precisos (como en el caso de las direcciones de las luces o la vista de la cámara), es necesario trabajar con **Vectores Unitarios (Normalización)**:\n$$\\hat{u} = \\frac{\\vec{v}}{\\|\\vec{v}\\|} = \\frac{1}{\\sqrt{x^2 + y^2 + z^2}} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$$\n\n### 2.2 Operaciones Fundamentales y su Aplicación Numérica\n\n#### A. Producto Punto (Dot Product)\nSe utiliza para calcular el ángulo entre dos vectores (como la mirada del usuario y un objeto) y en modelos de iluminación:\n$$\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta)$$\n* **Uso:** Si $\\vec{a} \\cdot \\vec{b} > 0$, el objeto está frente a la cámara; si es $< 0$, está detrás.\n\n#### B. Producto Cruz (Cross Product)\nEs esencial para calcular la **Normal de una Superficie** ($\\vec{N}$), lo cual define cómo se renderiza una cara 3D:\n$$\\vec{N} = \\vec{v_1} \\times \\vec{v_2} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\end{vmatrix}$$\n* **Justificación:** El vector resultante es perpendicular al plano formado por $\\vec{v_1}$ y $\\vec{v_2}$, permitiendo al motor calcular el rebote de la luz.\n\n### 2.3 Interpolación Lineal (LERP)\nPara animaciones fluidas entre dos puntos $\\vec{P_0}$ y $\\vec{P_1}$, A-Frame utiliza interpolación numérica:\n$$\\vec{P}(t) = (1 - t)\\vec{P_0} + t\\vec{P_1}$$\nDonde $t \\in [0, 1]$ representa el paso del tiempo normalizado. Esta es la base de los métodos de aproximación para trayectorias de movimiento.",
      "metadata": {}
    },
    {
      "id": "55ec38f7-44df-4f6a-9aaf-a0c27a5d3b5e",
      "cell_type": "markdown",
      "source": "## 3. Rotaciones y Representación Algebraica: Cuaterniones\n\nEn el espacio 3D de A-Frame, las rotaciones se pueden definir mediante ángulos de Euler (Giro en X, Y, Z), pero internamente el motor utiliza **Cuaterniones** para realizar los cálculos numéricos de forma eficiente y evitar el fenómeno del *Gimbal Lock* (bloqueo de rotación).\n\n### 3.1 El Problema de los Ángulos de Euler\nCuando rotamos un objeto usando tres ángulos ($\\phi, \\theta, \\psi$), la matriz de rotación total es el producto de tres matrices individuales:\n$$R_{total} = R_z(\\psi)R_y(\\theta)R_x(\\phi)$$\n**Justificación Numérica:** Este método pierde un grado de libertad cuando dos ejes de rotación se alinean (singularidad), lo que impide cálculos de trayectoria suaves en sistemas de navegación VR.\n\n### 3.2 Solución mediante Cuaterniones ($\\mathbb{H}$)\nUn cuaternión es un número complejo en cuatro dimensiones que representa una rotación en torno a un eje arbitrario $\\vec{u}$ por un ángulo $\\alpha$:\n$$\\mathbf{q} = (s, \\vec{v}) = \\cos\\left(\\frac{\\alpha}{2}\\right) + \\sin\\left(\\frac{\\alpha}{2}\\right)(u_x\\mathbf{i} + u_y\\mathbf{j} + u_z\\mathbf{k})$$\n\nDonde se cumple la identidad fundamental:\n$$\\mathbf{i}^2 = \\mathbf{j}^2 = \\mathbf{k}^2 = \\mathbf{ijk} = -1$$\n\n### 3.3 Esferical Linear Interpolation (SLERP)\nPara animar rotaciones de manera numéricamente estable y uniforme, A-Frame utiliza la fórmula de **SLERP**, que interpola entre dos cuaterniones $q_0$ y $q_1$:\n\n$$SLERP(q_0, q_1, t) = \\frac{\\sin((1-t)\\Omega)}{\\sin\\Omega}q_0 + \\frac{\\sin(t\\Omega)}{\\sin\\Omega}q_1$$\n\nDonde:\n* $t \\in [0, 1]$ es el factor de interpolación.\n* $\\cos\\Omega = q_0 \\cdot q_1$ (producto escalar de los cuaterniones).\n\n### 3.4 Ventajas en el Cómputo Numérico\n1. **Menor costo computacional:** Multiplicar dos cuaterniones requiere menos operaciones de punto flotante que multiplicar dos matrices $3 \\times 3$.\n2. **Normalización:** Para que un cuaternión represente una rotación válida, debe ser un cuaternión unitario:\n$$\\|\\mathbf{q}\\| = \\sqrt{s^2 + x^2 + y^2 + z^2} = 1$$\nEsto permite corregir errores de redondeo numérico acumulados simplemente renormalizando el vector resultante.",
      "metadata": {}
    },
    {
      "id": "c9f0e7e2-d7f2-454d-8ffd-1b838e23deeb",
      "cell_type": "markdown",
      "source": "## 4. El Pipeline de Renderizado: De 3D a la Pantalla 2D\n\nPara que A-Frame pueda mostrar una escena en un monitor o en un visor de VR, debe resolver una serie de ecuaciones que proyectan los puntos del espacio euclidiano $\\mathbb{R}^3$ al plano de la pantalla $\\mathbb{R}^2$.\n\n### 4.1 Matriz de Vista (View Matrix)\nPrimero, se transforma todo el universo de la escena para que la cámara sea el origen $(0,0,0)$. Si la cámara tiene una posición $C$ y una orientación $R$, la matriz de vista $V$ es la inversa de la matriz de transformación de la cámara:\n$$V = (M_{camera})^{-1}$$\nEsto justifica que mover la cámara hacia adelante es numéricamente equivalente a mover todo el mundo hacia atrás.\n\n### 4.2 Matriz de Proyección de Perspectiva\nPara simular el ojo humano, los objetos lejanos deben verse más pequeños. Esto se logra dividiendo las coordenadas por su profundidad ($z$). La matriz de proyección $P$ transforma un volumen truncado (frustum) en un cubo normalizado:\n\n$$P = \\begin{bmatrix} \n\\frac{1}{aspect \\cdot \\tan(\\frac{fov}{2})} & 0 & 0 & 0 \\\\ \n0 & \\frac{1}{\\tan(\\frac{fov}{2})} & 0 & 0 \\\\ \n0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2fn}{f-n} \\\\ \n0 & 0 & -1 & 0 \n\\end{bmatrix}$$\n\nDonde:\n* **$fov$:** Campo de visión (Field of View).\n* **$aspect$:** Relación de aspecto de la pantalla (ancho/alto).\n* **$n, f$:** Planos de corte cercano (near) y lejano (far).\n\n### 4.3 Coordenadas de Dispositivo Normalizadas (NDC)\nTras multiplicar un punto $P_{world}$ por las matrices de Vista y Proyección, obtenemos coordenadas en un espacio intermedio. El paso final es la **División de Perspectiva**:\n$$x_{ndc} = \\frac{x_{clip}}{w_{clip}}, \\quad y_{ndc} = \\frac{y_{clip}}{w_{clip}}, \\quad z_{ndc} = \\frac{z_{clip}}{w_{clip}}$$\n\nEste proceso asegura que todos los puntos visibles queden dentro del rango $[-1, 1]$. Cualquier valor fuera de este rango es descartado numéricamente (**Clipping**), optimizando el uso de la GPU.\n\n### 4.4 Resumen del Flujo de Datos\nLa posición final de un vértice en pantalla ($v_{screen}$) se calcula como:\n$$v_{screen} = P \\cdot V \\cdot M \\cdot v_{local}$$\n\n> **Justificación Numérica:** Este encadenamiento de multiplicaciones de matrices permite procesar miles de vértices en paralelo mediante hardware especializado (GPU), resolviendo sistemas lineales masivos en milisegundos.",
      "metadata": {}
    },
    {
      "id": "d920411d-2f91-4b02-aafa-fa883612909b",
      "cell_type": "markdown",
      "source": "## 5. Modelos Numéricos de Iluminación y Sombreado (Shading)\n\nEl renderizado en A-Frame se basa en calcular la intensidad de color en cada píxel mediante modelos matemáticos que aproximan la interacción de la luz con las superficies.\n\n### 5.1 El Modelo de Reflexión de Lambert\nPara superficies opacas (mate), A-Frame utiliza la ley del coseno de Lambert. La intensidad de iluminación $I$ en un punto depende del ángulo entre el vector de la luz $\\vec{L}$ y el vector normal de la superficie $\\vec{N}$.\n\nLa aproximación numérica se resuelve mediante el **Producto Punto**:\n$$I = L_c \\cdot K_d \\cdot \\max(0, \\hat{N} \\cdot \\hat{L})$$\n\nDonde:\n* $L_c$: Intensidad y color de la fuente de luz.\n* $K_d$: Coeficiente de reflexión difusa del material.\n* $\\hat{N} \\cdot \\hat{L}$: Coseno del ángulo entre los vectores normalizados. Si el ángulo es $> 90^\\circ$, el producto punto es negativo y la luz no incide (valor 0).\n\n### 5.2 Modelo Especular (Phong/Blinn-Phong)\nPara simular el brillo en materiales como metal o plástico, se añade un componente que depende de la posición del observador (la cámara) $\\vec{V}$. Se calcula un vector intermedio llamado **Vector Medio** ($\\vec{H}$):\n\n$$\\vec{H} = \\frac{\\vec{L} + \\vec{V}}{\\|\\vec{L} + \\vec{V}\\|}$$\n\nLa intensidad especular se aproxima como:\n$$I_{spec} = K_s \\cdot (\\hat{N} \\cdot \\hat{H})^n$$\n* $n$: Es el exponente de brillo (Shininess). A mayor $n$, el reflejo es más pequeño y definido.\n\n### 5.3 Implementación en Shaders\nEstos cálculos no se realizan en la CPU, sino que se delegan a la GPU mediante programas llamados **Shaders** escritos en GLSL. Existen dos aproximaciones principales:\n\n| Método | Aplicación Numérica | Calidad Visual |\n| :--- | :--- | :--- |\n| **Gouraud Shading** | Calcula la iluminación en los vértices e interpola linealmente el color en el resto de la cara. | Baja (bordes visibles) |\n| **Phong Shading** | Interpola las **normales** de los vértices y calcula la iluminación píxel por píxel. | Alta (suavidad matemática) |\n\n### 5.4 Justificación del Error de Aproximación\nComo A-Frame busca el rendimiento en tiempo real (60 FPS), utiliza estos modelos empíricos en lugar de resolver la **Ecuación de Renderizado de Kajiya** (que es una integral compleja). Esto es un ejemplo clásico de cómo los métodos numéricos sacrifican exactitud física por eficiencia computacional.",
      "metadata": {}
    },
    {
      "id": "15cf8d60-5fc1-43fb-aead-4c24ed432e6b",
      "cell_type": "markdown",
      "source": "## 6. Arquitectura ECS y el Ciclo de Actualización Numérica\n\nA-Frame utiliza el patrón **Entity-Component-System (ECS)**, el cual permite desacoplar los datos de la lógica, facilitando el procesamiento masivo de entidades.\n\n### 6.1 Componentes y Sistemas\n* **Entidades ($E$):** Identificadores únicos (puntos en el grafo).\n* **Componentes ($C$):** Contenedores de datos (ej. posición, velocidad, masa).\n* **Sistemas ($S$):** La lógica que opera sobre los datos. Desde el punto de vista numérico, un sistema es un **solucionador** que itera sobre un conjunto de estados.\n\n### 6.2 El Método `tick`: Resolución en Tiempo Real\nEl motor de A-Frame ejecuta una función llamada `tick` aproximadamente cada $16.67$ ms (para alcanzar 60 FPS). Este es, en esencia, un paso de tiempo discreto $\\Delta t$:\n\n$$S_{t+1} = S_t + f(S_t, \\Delta t)$$\n\nDonde $f$ representa la función de actualización (física, animaciones, IA). Al ser un método iterativo, si $\\Delta t$ varía (debido a caídas de rendimiento), el sistema debe compensarlo para mantener la consistencia temporal, una técnica común en la resolución de ecuaciones diferenciales ordinarias (EDO) para simulaciones físicas.",
      "metadata": {}
    },
    {
      "id": "7e18a43a-1c67-4738-b9a6-d8fcb4115ebd",
      "cell_type": "markdown",
      "source": "## 6. Arquitectura ECS y el Ciclo de Actualización Numérica\n\nA-Frame utiliza el patrón **Entity-Component-System (ECS)**, el cual permite desacoplar los datos de la lógica, facilitando el procesamiento masivo de entidades.\n\n### 6.1 Componentes y Sistemas\n* **Entidades ($E$):** Identificadores únicos (puntos en el grafo).\n* **Componentes ($C$):** Contenedores de datos (ej. posición, velocidad, masa).\n* **Sistemas ($S$):** La lógica que opera sobre los datos. Desde el punto de vista numérico, un sistema es un **solucionador** que itera sobre un conjunto de estados.\n\n### 6.2 El Método `tick`: Resolución en Tiempo Real\nEl motor de A-Frame ejecuta una función llamada `tick` aproximadamente cada $16.67$ ms (para alcanzar 60 FPS). Este es, en esencia, un paso de tiempo discreto $\\Delta t$:\n\n$$S_{t+1} = S_t + f(S_t, \\Delta t)$$\n\nDonde $f$ representa la función de actualización (física, animaciones, IA). Al ser un método iterativo, si $\\Delta t$ varía (debido a caídas de rendimiento), el sistema debe compensarlo para mantener la consistencia temporal, una técnica común en la resolución de ecuaciones diferenciales ordinarias (EDO) para simulaciones físicas.",
      "metadata": {}
    },
    {
      "id": "1a9e3e94-9d59-4c05-ad6a-6af623cbf4ee",
      "cell_type": "markdown",
      "source": "## 7. Implementación de una Escena Base\n\nA continuación, se presenta la estructura declarativa de una escena. Aunque el usuario escribe etiquetas tipo HTML, el motor traduce cada etiqueta a los objetos matemáticos (matrices, mallas y vectores) descritos anteriormente.\n\n```html\n<a-scene>\n  <a-sky color=\"#ECECEC\"></a-sky>\n\n  <a-box position=\"-1 0.5 -3\" \n         rotation=\"0 45 0\" \n         color=\"#4CC3D9\"\n         shadow>\n  </a-box>\n\n  <a-plane position=\"0 0 -4\" \n           rotation=\"-90 0 0\" \n           width=\"4\" \n           height=\"4\" \n           color=\"#7BC8A4\">\n  </a-plane>\n\n  <a-camera position=\"0 1.6 0\"></a-camera>\n</a-scene>",
      "metadata": {}
    },
    {
      "id": "b02034e0-afe7-4706-8a55-644a67d9b0ea",
      "cell_type": "markdown",
      "source": "## 7. Implementación de una Escena Base\n\nA continuación, se presenta la estructura declarativa de una escena. Aunque el usuario escribe etiquetas tipo HTML, el motor traduce cada etiqueta a los objetos matemáticos (matrices, mallas y vectores) descritos anteriormente.\n\n```html\n<a-scene>\n  <a-sky color=\"#ECECEC\"></a-sky>\n\n  <a-box position=\"-1 0.5 -3\" \n         rotation=\"0 45 0\" \n         color=\"#4CC3D9\"\n         shadow>\n  </a-box>\n\n  <a-plane position=\"0 0 -4\" \n           rotation=\"-90 0 0\" \n           width=\"4\" \n           height=\"4\" \n           color=\"#7BC8A4\">\n  </a-plane>\n\n  <a-camera position=\"0 1.6 0\"></a-camera>\n</a-scene>",
      "metadata": {}
    },
    {
      "id": "46e61636-acd8-4987-8726-5a69a55ba0f3",
      "cell_type": "markdown",
      "source": "### 7.1 Resumen de la Justificación\nEl uso de **A-Frame** en el contexto de **Métodos Numéricos** permite:\n\n* **Visualización de Transformaciones:** Observar en tiempo real el efecto de las multiplicaciones matriciales y la composición de transformaciones lineales.\n* **Simulación de Modelos Físicos:** Implementar algoritmos de colisión, gravedad y movimiento basados en **integración numérica** (como el método de Euler o Verlet).\n* **Abstracción de WebGL:** Manipular el pipeline de renderizado (shading, proyección y rasterización) sin la necesidad de escribir manualmente miles de líneas de código de bajo nivel para la GPU, permitiendo enfocarse en la lógica matemática del espacio tridimensional.\n\n---\n\n> **Conclusión del Módulo:** Tanto los operadores de gradiente (**Sobel**), el algoritmo de optimización de bordes (**Canny**), como los motores de renderizado (**A-Frame**), comparten una base común: la discretización de funciones continuas para su resolución mediante cómputo digital. Mientras los primeros analizan la señal, el último la sintetiza, pero ambos dependen del rigor del álgebra lineal y el cálculo diferencial.",
      "metadata": {}
    },
    {
      "id": "83c51a8f-23db-4794-a403-14a326091d32",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d4201534-eed5-4b19-82d0-a304dab3472c",
      "cell_type": "markdown",
      "source": "# Three.js: Motor de Gráficos Imperativo y Álgebra Lineal\n\n## 1. Fundamentos Matemáticos y la Tubería de Datos (Pipeline)\n\nA diferencia de los frameworks declarativos, **Three.js** es una biblioteca de bajo nivel que permite la manipulación directa de los búferes de memoria y las operaciones matriciales. Su funcionamiento se basa en la resolución de transformaciones en espacios de $n$-dimensiones.\n\n### 1.1 El Objeto `BufferGeometry`: Eficiencia en Memoria\nDesde la perspectiva numérica, Three.js no trata a los objetos como entidades abstractas, sino como **arreglos de datos tipados (TypedArrays)**. Una malla (mesh) se define por un conjunto de atributos vectoriales almacenados de forma contigua para maximizar el *throughput* de la GPU:\n\n* **Vértices ($V$):** Un vector plano $[x_1, y_1, z_1, ..., x_n, y_n, z_n] \\in \\mathbb{R}^{3n}$.\n* **Normales ($N$):** Vectores unitarios para el cálculo de iluminación.\n* **Índices ($I$):** Define la topología de la malla mediante la conectividad de los vértices para formar triángulos.\n\n### 1.2 La Cadena de Transformaciones (Modelo-Vista-Proyección)\nPara renderizar un solo punto, Three.js debe resolver la ecuación fundamental de la computación gráfica, que es una cadena de multiplicaciones de matrices de $4 \\times 4$:\n\n$$P_{clip} = M_{proj} \\cdot M_{view} \\cdot M_{model} \\cdot v_{local}$$\n\nCada componente tiene una justificación numérica específica:\n1. **$M_{model}$ (World Matrix):** Transforma coordenadas locales al espacio global mediante traslación, rotación y escala.\n2. **$M_{view}$ (Camera Matrix):** Es la matriz inversa de la posición de la cámara ($C^{-1}$), que reorienta el universo respecto al observador.\n3. **$M_{proj}$ (Projection Matrix):** Aplica la distorsión de perspectiva, mapeando un volumen arbitrario a coordenadas normalizadas $[-1, 1]$.\n\n### 1.3 El Grafo de Escena y Matrices Globales\nThree.js utiliza un sistema de actualización recursiva para las matrices. Para cualquier objeto $i$, su matriz global $M_{global}^i$ se define como:\n\n$$M_{global}^i = M_{global}^{padre} \\cdot M_{local}^i$$\n\n> **Dato para Métodos Numéricos:** Para optimizar el rendimiento, Three.js no recalcula estas matrices en cada frame a menos que la propiedad `.matrixWorldNeedsUpdate` sea verdadera, lo que ejemplifica una estrategia de **ahorro computacional** en sistemas dinámicos.",
      "metadata": {}
    },
    {
      "id": "baa27541-8cb2-4eff-b305-d911391cbb23",
      "cell_type": "markdown",
      "source": "## 2. Geometría Computacional: Raycasting e Intersección\n\nEl **Raycasting** es la técnica que utiliza Three.js para resolver problemas de interactividad y detección de colisiones. Matemáticamente, consiste en proyectar una línea recta en el espacio y encontrar el punto de intersección con la geometría más cercana.\n\n### 2.1 Ecuación Paramétrica del Rayo\nUn rayo se define como una función lineal que depende de un parámetro de tiempo o distancia $t$:\n\n$$\\vec{R}(t) = \\vec{O} + t\\vec{D}$$\n\nDonde:\n* $\\vec{O}$: Es el origen del rayo (usualmente la posición de la cámara).\n* $\\vec{D}$: Es el vector de dirección unitario ($\\|\\vec{D}\\| = 1$).\n* $t$: Es la distancia desde el origen ($t \\geq 0$).\n\n### 2.2 Algoritmo de Intersección Rayo-Triángulo (Möller-Trumbore)\nPara determinar si un rayo golpea un objeto, Three.js debe resolver si el rayo intersecta alguno de los triángulos de la malla. Dado un triángulo con vértices $V_1, V_2, V_3$, un punto $P$ dentro del triángulo se define por sus **Coordenadas Baricéntricas** $(u, v)$:\n\n$$P(u, v) = (1 - u - v)V_1 + uV_2 + vV_3$$\n\nIgualando la ecuación del rayo con la del plano del triángulo, el sistema de ecuaciones lineales resultante es:\n\n$$\\vec{O} + t\\vec{D} = (1 - u - v)V_1 + uV_2 + vV_3$$\n\nEste sistema se resuelve numéricamente para $t, u, v$ mediante la regla de Cramer, lo que permite a la GPU o CPU determinar:\n1. **Si hay intersección:** Si $u \\geq 0, v \\geq 0$ y $u + v \\leq 1$.\n2. **La distancia exacta:** El valor de $t$.\n\n### 2.3 Frustum Culling: Optimización por Descarte\nAntes de realizar los cálculos costosos de intersección, Three.js aplica métodos de **delimitación volumétrica**. Se envuelve cada objeto en una **Esfera de Delimitación (Bounding Sphere)** mínima:\n\n$$\\|\\vec{P} - \\vec{C}\\|^2 \\leq r^2$$\n\n**Justificación Numérica:** Es mucho más barato computacionalmente verificar la intersección con una esfera (una sola ecuación cuadrática) que con miles de triángulos. Si el rayo no toca la esfera, se descarta el objeto inmediatamente, reduciendo la complejidad algorítmica de $O(n)$ a $O(1)$ para ese objeto.",
      "metadata": {}
    },
    {
      "id": "bb1fa405-1352-4ef1-8904-690717cc38eb",
      "cell_type": "markdown",
      "source": "## 3. Interpolación y Curvas Paramétricas\n\nEn Three.js, la animación y el movimiento de cámaras u objetos se basan en la aproximación de trayectorias mediante funciones polinómicas. Esto es fundamental para evitar movimientos \"robóticos\" y asegurar la continuidad en las derivadas de la posición (velocidad y aceleración).\n\n### 3.1 Curvas de Bézier (Interpolación Polinómica)\nPara definir trayectorias suaves, se utilizan las **Curvas de Bézier**, que se basan en los **Polinomios de Bernstein**. Una curva de tercer grado (cúbica) se define mediante cuatro puntos de control $P_0, P_1, P_2, P_3$:\n\n$$B(t) = (1-t)^3 P_0 + 3(1-t)^2 t P_1 + 3(1-t) t^2 P_2 + t^3 P_3, \\quad t \\in [0, 1]$$\n\n**Justificación Numérica:** Esta fórmula garantiza que la curva sea tangente a los segmentos de control, permitiendo un control geométrico preciso de la trayectoria sin necesidad de resolver sistemas de ecuaciones globales pesados.\n\n### 3.2 Splines de Catmull-Rom\nA diferencia de Bézier, donde la curva no siempre pasa por los puntos de control, Three.js utiliza frecuentemente **Catmull-Rom Splines** para que la trayectoria pase exactamente por los nodos definidos. La ecuación local para un segmento entre $P_i$ y $P_{i+1}$ es:\n\n$$P(t) = \\frac{1}{2} \\begin{bmatrix} 1 & t & t^2 & t^3 \\end{bmatrix} \\begin{bmatrix} 0 & 2 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 2 & -5 & 4 & -1 \\\\ -1 & 3 & -3 & 1 \\end{bmatrix} \\begin{bmatrix} P_{i-1} \\\\ P_i \\\\ P_{i+1} \\\\ P_{i+2} \\end{bmatrix}$$\n\n* **Continuidad $C^1$:** Este método asegura que la primera derivada (velocidad) sea continua en todos los puntos, eliminando cambios bruscos de dirección.\n\n### 3.3 El Ciclo de Animación y Delta Time ($\\Delta t$)\nEn métodos numéricos, la integración del movimiento requiere un paso de tiempo. Three.js utiliza el reloj del sistema para calcular el `delta`:\n\n$$\\text{Posición}_{nuevo} = \\text{Posición}_{anterior} + (\\text{Velocidad} \\cdot \\Delta t)$$\n\nSi la velocidad no es constante, se aplican métodos de integración como **Euler Simple** o, en simulaciones más avanzadas dentro de Three.js, **Integración de Verlet**, que es más estable numéricamente para sistemas de partículas:\n\n$$x_{t+ \\Delta t} = 2x_t - x_{t- \\Delta t} + a \\cdot \\Delta t^2$$",
      "metadata": {}
    },
    {
      "id": "39267076-6cc4-4c58-84c7-95e88f45c6b4",
      "cell_type": "markdown",
      "source": "## 4. Iluminación Avanzada y Mapeo de Normales (Normal Mapping)\n\nEn Three.js, la apariencia de una superficie no depende solo de la cantidad de triángulos, sino de cómo se manipulan los vectores normales en el nivel de los fragmentos (píxeles). Esto se basa en la aplicación de **Álgebra Lineal** para simular micro-geometría.\n\n### 4.1 Espacio Tangente y Base TBN\nPara aplicar un mapa de normales (una textura que indica hacia dónde \"mira\" cada píxel), Three.js debe construir un sistema de coordenadas local para cada punto de la superficie, llamado **Espacio Tangente**. Este se define por tres vectores ortonormales:\n* **$\\vec{T}$ (Tangente):** Vector en la dirección del eje U de la textura.\n* **$\\vec{B}$ (Bitangente):** Vector en la dirección del eje V de la textura.\n* **$\\vec{N}$ (Normal):** Vector perpendicular a la cara.\n\nEstos vectores forman la **Matriz TBN**, que permite transformar vectores del \"Espacio de Textura\" al \"Espacio del Mundo\":\n$$M_{TBN} = \\begin{bmatrix} T_x & B_x & N_x \\\\ T_y & B_y & N_y \\\\ T_z & B_z & N_z \\end{bmatrix}$$\n\n### 4.2 Perturbación de la Normal\nEl valor de color de un píxel en el mapa de normales $(R, G, B)$ se decodifica para obtener un vector unitario en el rango $[-1, 1]$:\n$$\\vec{n}_{local} = \\begin{bmatrix} 2R - 1 \\\\ 2G - 1 \\\\ 2B - 1 \\end{bmatrix}$$\n\nLa normal final utilizada para el cálculo de iluminación ($\\vec{n}_{final}$) es el producto de la matriz TBN por este vector local:\n$$\\vec{n}_{final} = M_{TBN} \\cdot \\vec{n}_{local}$$\n\n### 4.3 El Modelo de PBR (Physically Based Rendering)\nThree.js utiliza en su `MeshStandardMaterial` un modelo numérico avanzado llamado **Ecuación de Reflectancia**, que es una integral de iluminación:\n$$L_o(p, \\omega_o) = \\int_{\\Omega} f_r(p, \\omega_i, \\omega_o) L_i(p, \\omega_i) (\\vec{n} \\cdot \\omega_i) d\\omega_i$$\n\n**Justificación Numérica:** Como resolver esta integral en tiempo real es imposible, Three.js utiliza una aproximación de **Suma de Riemann** o **Importancia de Muestreo (Importance Sampling)** para simular cómo la luz se refleja en micro-facetas, basándose en la función de distribución de micro-facetas de **Trowbridge-Reitz (GGX)**.\n\n### 4.4 Cálculo de Sombras (Shadow Mapping)\nLas sombras se calculan mediante una comparación de profundidad ($z$). Para cada píxel, el motor resuelve:\n$$Visible = \\begin{cases} 1 & \\text{si } z_{píxel} \\leq z_{shadow\\_map} + \\epsilon \\\\ 0 & \\text{en otro caso} \\end{cases}$$\n*Donde $\\epsilon$ es un factor de sesgo (bias) introducido para corregir errores de precisión numérica de punto flotante.*",
      "metadata": {}
    },
    {
      "id": "895dbbb4-2286-4a29-a3fe-51bb6bd81893",
      "cell_type": "markdown",
      "source": "## 5. El Ciclo de Renderizado (Render Loop) y Estabilidad Numérica\n\nEn Three.js, la generación de imágenes no es un evento estático, sino un proceso iterativo continuo que debe mantener la estabilidad numérica para evitar errores de precisión o \"saltos\" en la simulación.\n\n### 5.1 Discretización del Tiempo\nCada iteración del ciclo de renderizado representa un paso de tiempo discreto $\\Delta t$. Para que las simulaciones físicas sean consistentes, Three.js utiliza el método de `requestAnimationFrame`, que busca sincronizarse con la tasa de refresco del monitor.\n\nLa actualización del estado del sistema se define como:\n$$X_{t+1} = X_t + \\int_{t}^{t+\\Delta t} v(t) dt \\approx X_t + v_t \\cdot \\Delta t$$\n\n### 5.2 Manejo del Error de Redondeo (Floating Point Precision)\nDado que WebGL utiliza precisión de punto flotante de 32 bits, en escenas con escalas masivas (ej. simulaciones astronómicas) se presenta el error de **Z-Fighting**.\nEste fenómeno ocurre cuando dos profundidades $z_1$ y $z_2$ son tan cercanas que la precisión numérica no puede distinguirlas:\n$$|z_1 - z_2| < \\epsilon$$\nDonde $\\epsilon$ es la resolución del búfer de profundidad. Three.js permite mitigar esto mediante el uso de un **Logarithmic Depth Buffer**, que redistribuye la precisión de forma no lineal (logarítmica) para dar más detalle a objetos lejanos.",
      "metadata": {}
    },
    {
      "id": "7f3d8e88-15a3-4e1e-9669-e6415a97092f",
      "cell_type": "markdown",
      "source": "## 5. El Ciclo de Renderizado (Render Loop) y Estabilidad Numérica\n\nEn Three.js, la generación de imágenes no es un evento estático, sino un proceso iterativo continuo que debe mantener la estabilidad numérica para evitar errores de precisión o \"saltos\" en la simulación.\n\n### 5.1 Discretización del Tiempo\nCada iteración del ciclo de renderizado representa un paso de tiempo discreto $\\Delta t$. Para que las simulaciones físicas sean consistentes, Three.js utiliza el método de `requestAnimationFrame`, que busca sincronizarse con la tasa de refresco del monitor.\n\nLa actualización del estado del sistema se define como:\n$$X_{t+1} = X_t + \\int_{t}^{t+\\Delta t} v(t) dt \\approx X_t + v_t \\cdot \\Delta t$$\n\n### 5.2 Manejo del Error de Redondeo (Floating Point Precision)\nDado que WebGL utiliza precisión de punto flotante de 32 bits, en escenas con escalas masivas (ej. simulaciones astronómicas) se presenta el error de **Z-Fighting**.\nEste fenómeno ocurre cuando dos profundidades $z_1$ y $z_2$ son tan cercanas que la precisión numérica no puede distinguirlas:\n$$|z_1 - z_2| < \\epsilon$$\nDonde $\\epsilon$ es la resolución del búfer de profundidad. Three.js permite mitigar esto mediante el uso de un **Logarithmic Depth Buffer**, que redistribuye la precisión de forma no lineal (logarítmica) para dar más detalle a objetos lejanos.",
      "metadata": {}
    },
    {
      "id": "22ac4f83-dc69-4e5e-bd53-eeb36dc969fe",
      "cell_type": "markdown",
      "source": "## 6. Estructura de Implementación y Código Base\n\nA diferencia de A-Frame, en Three.js la configuración de la escena requiere la instanciación explícita de los objetos matemáticos (Cámara, Escena, Renderizador).\n\n### 6.1 Componentes Esenciales del Script\nEl siguiente bloque muestra cómo se traduce la teoría de matrices y vectores a código funcional:\n\n```javascript\n// 1. Inicialización del Motor (Espacio de Renderizado)\nconst scene = new THREE.Scene();\nconst camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\nconst renderer = new THREE.WebGLRenderer();\n\n// 2. Creación de Geometría Discreta\n// Se define la topología de la malla (8 vértices, 12 caras triangulares)\nconst geometry = new THREE.BoxGeometry(1, 1, 1);\n\n// 3. Definición del Material (Modelo de Iluminación)\n// Utiliza algoritmos de sombreado de fragmentos para calcular la luz\nconst material = new THREE.MeshStandardMaterial({ color: 0x00ff00 });\n\n// 4. Creación de la Malla (Mesh = Geometría + Material)\nconst cube = new THREE.Mesh(geometry, material);\nscene.add(cube);\n\n// 5. Bucle de Animación (Método Iterativo)\nfunction animate() {\n  requestAnimationFrame(animate);\n\n  // Aplicación de rotación incremental (Transformación de matriz local)\n  cube.rotation.x += 0.01;\n  cube.rotation.y += 0.01;\n\n  // Resolución de la Matriz de Proyección y dibujado en el Canvas\n  renderer.render(scene, camera);\n}\nanimate();",
      "metadata": {}
    },
    {
      "id": "38209b45-0e63-40b6-8cec-9a68e310872c",
      "cell_type": "code",
      "source": "from IPython.display import display, HTML\n\ncodigo_seguro = \"\"\"\n<div id=\"app-container\">\n    <div class=\"controls\">\n        <button id=\"btn-glasses\">👓 Lentes</button>\n        <button id=\"btn-mustache\">🥸 Bigote</button>\n        <button id=\"btn-hat\">🎩 Sombrero</button>\n        <button id=\"btn-mole\">🟤 Lunar</button>\n        <button id=\"btn-points\">🟢 Puntos (Debug)</button>\n    </div>\n\n    <div class=\"viewport\">\n        <video id=\"input-video\" style=\"display:none\" autoplay muted playsinline></video>\n        <canvas id=\"output-canvas\"></canvas>\n        <div id=\"status-text\">Iniciando sistema...</div>\n    </div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\"></script>\n\n<style>\n    #app-container {\n        font-family: sans-serif;\n        width: 640px;\n        margin: 0 auto;\n        background: #222;\n        padding: 10px;\n        border-radius: 8px;\n        color: white;\n    }\n\n    .viewport {\n        position: relative;\n        width: 640px;\n        height: 480px;\n        background: black;\n        border: 2px solid #444;\n        overflow: hidden;\n    }\n\n    canvas {\n        width: 100%;\n        height: 100%;\n        object-fit: cover;\n    }\n\n    .controls {\n        display: flex;\n        gap: 5px;\n        margin-bottom: 10px;\n        justify-content: center;\n        flex-wrap: wrap;\n    }\n\n    button {\n        background: #444;\n        color: white;\n        border: 1px solid #666;\n        padding: 8px 12px;\n        cursor: pointer;\n        border-radius: 4px;\n        font-weight: bold;\n    }\n\n    button:hover { background: #555; }\n    \n    /* Clase para botón activo (Verde) */\n    button.active {\n        background: #00e676;\n        color: black;\n        border-color: #00a152;\n    }\n\n    #status-text {\n        position: absolute;\n        bottom: 10px;\n        left: 10px;\n        background: rgba(0, 0, 0, 0.7);\n        padding: 5px 10px;\n        border-radius: 4px;\n        font-size: 14px;\n        pointer-events: none;\n    }\n</style>\n\n<script>\n(function() {\n    // 1. Configuración Inicial\n    const container = document.getElementById('app-container');\n    const videoElement = container.querySelector('#input-video');\n    const canvasElement = container.querySelector('#output-canvas');\n    const ctx = canvasElement.getContext('2d');\n    const statusText = container.querySelector('#status-text');\n    \n    // Forzamos tamaño\n    canvasElement.width = 640;\n    canvasElement.height = 480;\n\n    // Estado de filtros\n    const state = {\n        glasses: false,\n        mustache: false,\n        hat: false,\n        mole: false,\n        points: false // Activado por defecto para probar\n    };\n\n    // 2. Conectar Botones\n    function setupButton(id, key) {\n        const btn = container.querySelector('#' + id);\n        if(!btn) return;\n        btn.onclick = () => {\n            state[key] = !state[key];\n            // Visualmente cambiar el botón\n            if(state[key]) btn.classList.add('active');\n            else btn.classList.remove('active');\n            console.log(\"Toggle \" + key + \": \" + state[key]);\n        };\n    }\n\n    setupButton('btn-glasses', 'glasses');\n    setupButton('btn-mustache', 'mustache');\n    setupButton('btn-hat', 'hat');\n    setupButton('btn-mole', 'mole');\n    setupButton('btn-points', 'points');\n\n    // 3. Lógica de Dibujo (Filtros 2D)\n    function drawScene(results) {\n        // Limpiar\n        ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n        \n        // A. Dibujar la imagen de la cámara DE FONDO\n        ctx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);\n\n        // B. Verificar si hay cara\n        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\n            statusText.innerText = \"✅ Rostro detectado - Filtros activos\";\n            statusText.style.color = \"#00e676\";\n            \n            const landmarks = results.multiFaceLandmarks[0];\n            drawFilters(landmarks);\n        } else {\n            statusText.innerText = \"⚠️ Buscando rostro... (Acércate a la cámara)\";\n            statusText.style.color = \"orange\";\n        }\n    }\n\n    function drawFilters(landmarks) {\n        const w = canvasElement.width;\n        const h = canvasElement.height;\n\n        // Puntos Clave\n        const leftEye = landmarks[33];\n        const rightEye = landmarks[263];\n        const noseTip = landmarks[1];\n        const upperLip = landmarks[13];\n        const forehead = landmarks[10];\n        const leftCheek = landmarks[234];\n\n        // Matemáticas para posición y tamaño\n        const cx = (leftEye.x + rightEye.x) / 2 * w;\n        const cy = (leftEye.y + rightEye.y) / 2 * h;\n        const dx = (rightEye.x - leftEye.x) * w;\n        const dy = (rightEye.y - leftEye.y) * h;\n        const dist = Math.sqrt(dx*dx + dy*dy); // Distancia entre ojos\n        const angle = Math.atan2(dy, dx); // Inclinación de la cabeza\n\n        ctx.save(); // Guardar estado para rotaciones\n\n        // --- 1. PUNTOS (DEBUG) ---\n        if (state.points) {\n            ctx.fillStyle = \"#00FF00\";\n            for (let point of landmarks) {\n                ctx.beginPath();\n                ctx.arc(point.x * w, point.y * h, 1, 0, 2 * Math.PI);\n                ctx.fill();\n            }\n        }\n\n        // --- 2. LENTES ---\n        if (state.glasses) {\n            ctx.translate(cx, cy);\n            ctx.rotate(angle);\n            \n            ctx.strokeStyle = \"black\";\n            ctx.lineWidth = 5;\n            // Marco Izq\n            ctx.strokeRect(-dist*0.9, -dist*0.3, dist*0.8, dist*0.5);\n            // Marco Der\n            ctx.strokeRect(dist*0.1, -dist*0.3, dist*0.8, dist*0.5);\n            // Puente\n            ctx.beginPath();\n            ctx.moveTo(-dist*0.1, 0);\n            ctx.lineTo(dist*0.1, 0);\n            ctx.stroke();\n\n            ctx.rotate(-angle); // Deshacer rotación\n            ctx.translate(-cx, -cy); // Deshacer traslación\n        }\n\n        // --- 3. BIGOTE ---\n        if (state.mustache) {\n            const mx = upperLip.x * w;\n            const my = upperLip.y * h;\n            \n            ctx.translate(mx, my);\n            ctx.rotate(angle);\n\n            ctx.fillStyle = \"#3e2723\";\n            ctx.beginPath();\n            ctx.ellipse(0, -dist*0.1, dist*0.6, dist*0.15, 0, 0, Math.PI*2);\n            ctx.fill();\n\n            ctx.rotate(-angle);\n            ctx.translate(-mx, -my);\n        }\n\n        // --- 4. SOMBRERO ---\n        if (state.hat) {\n            const hx = forehead.x * w;\n            const hy = forehead.y * h;\n\n            ctx.translate(hx, hy);\n            ctx.rotate(angle);\n\n            ctx.fillStyle = \"#1a237e\"; // Azul oscuro\n            // Copa\n            ctx.fillRect(-dist*0.8, -dist*1.5, dist*1.6, dist*1.0);\n            // Ala\n            ctx.fillRect(-dist*1.2, -dist*0.5, dist*2.4, dist*0.2);\n\n            ctx.rotate(-angle);\n            ctx.translate(-hx, -hy);\n        }\n\n        // --- 5. LUNAR ---\n        if (state.mole) {\n            const lx = leftCheek.x * w;\n            const ly = leftCheek.y * h;\n            ctx.fillStyle = \"black\";\n            ctx.beginPath();\n            ctx.arc(lx + dist*0.2, ly, dist*0.08, 0, Math.PI*2);\n            ctx.fill();\n        }\n\n        ctx.restore();\n    }\n\n    // 4. Iniciar MediaPipe FaceMesh\n    const faceMesh = new FaceMesh({\n        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n    });\n\n    faceMesh.setOptions({\n        maxNumFaces: 1,\n        refineLandmarks: true,\n        minDetectionConfidence: 0.5,\n        minTrackingConfidence: 0.5\n    });\n\n    faceMesh.onResults(drawScene);\n\n    // 5. Iniciar Cámara\n    const camera = new Camera(videoElement, {\n        onFrame: async () => {\n            await faceMesh.send({image: videoElement});\n        },\n        width: 640,\n        height: 480\n    });\n\n    statusText.innerText = \"Solicitando cámara...\";\n    camera.start()\n        .then(() => statusText.innerText = \"Cámara activa. Cargando modelo...\")\n        .catch(err => statusText.innerText = \"Error de cámara: \" + err);\n\n})();\n</script>\n\"\"\"\n\ndisplay(HTML(codigo_seguro))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6b88eae5-70bd-48b0-b26e-77ab5d53727b",
      "cell_type": "markdown",
      "source": "# MediaPipe: Inferencia Perceptual y Optimización de Grafos\n\n## 1. Fundamentos de Visión Computacional y Redes Convolucionales\n\nMediaPipe es un framework basado en el paradigma de **ML Pipelines**. Su función principal en el contexto numérico es la **regresión de coordenadas**, donde una imagen de entrada $I$ se procesa para obtener un conjunto de vectores de características.\n\n### 1.1 Inferencia como Función de Mapeo\nLa detección de una cara o mano se puede expresar como una función $f$ que mapea el espacio de la imagen al espacio de parámetros:\n$$f(I) = \\Theta$$\nDonde:\n* $I \\in \\mathbb{R}^{W \\times H \\times 3}$: Es la matriz de píxeles (ancho, alto, canales RGB).\n* $\\Theta = \\{\\vec{P}_1, \\vec{P}_2, ..., \\vec{P}_n\\}$: Es el conjunto de puntos clave (landmarks), donde cada $\\vec{P}_i = (x, y, z)$.\n\n### 1.2 La Convolución Discreta (Sustento del Modelo)\nPara extraer características de la imagen, MediaPipe utiliza **Redes Neuronales Convolucionales (CNN)**. La operación fundamental es la convolución discreta entre la imagen $I$ y un filtro (kernel) $K$ de tamaño $m \\times n$:\n\n$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i-m, j-n) \\cdot K(m, n)$$\n\n**Justificación Numérica:** Este proceso actúa como un detector de gradientes y patrones (similar a los filtros de Sobel/Canny pero con pesos aprendidos), reduciendo la dimensionalidad de la imagen a un vector de características abstractas.\n\n### 1.3 El Grafo de Flujo (Calculators)\nMediaPipe organiza el procesamiento en un **Grafo Acíclico Dirigido (DAG)**. Cada nodo es una \"calculadora\" que recibe un paquete de datos $T$ (Tensor) y emite un resultado tras aplicar una transformación numérica:\n$$T_{out} = \\phi(T_{in})$$\nEsta arquitectura permite el **paralelismo masivo**, resolviendo múltiples etapas de la tubería de datos simultáneamente.\n\n### 1.4 Normalización y Coordenadas Relativas\nMediaPipe no entrega coordenadas en píxeles, sino valores normalizados en el rango $[0, 1]$. La conversión a coordenadas de pantalla (necesaria para Three.js) es una **Transformación Lineal Escalar**:\n\n$$P_{pixel} = \\begin{bmatrix} x_{norm} \\cdot W \\\\ y_{norm} \\cdot H \\\\ z_{norm} \\cdot W \\end{bmatrix}$$\n\n*Donde $W$ y $H$ son el ancho y alto de la imagen.* La coordenada $z$ se calcula de manera relativa al centro de gravedad del objeto detectado, permitiendo una estimación de profundidad sin necesidad de una cámara estéreo.",
      "metadata": {}
    },
    {
      "id": "a4113ec0-85be-4a6a-827a-144f636bdc67",
      "cell_type": "markdown",
      "source": "## 2. El Modelo Face Mesh y Regresión de Puntos Clave\n\nEl sistema **Face Mesh** de MediaPipe predice la posición de 468 puntos (landmarks) en 3D. A diferencia de un detector de objetos simple que solo encuentra una caja delimitadora, este modelo realiza una **Regresión de Coordenadas de Alta Dimensionalidad**.\n\n### 2.1 Función de Pérdida y Optimización\nPara que el modelo aprenda a colocar los puntos correctamente, durante su entrenamiento se minimiza una **Función de Pérdida (Loss Function)**, generalmente basada en el **Error Cuadrático Medio (MSE)** entre la posición predicha ($\\hat{P}$) y la real ($P$):\n\n$$\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} \\|\\vec{P}_i - \\hat{\\vec{P}}_i\\|^2$$\n\nDonde $n = 468$. El modelo utiliza el **Gradiente Descendente** para ajustar los pesos de la red y minimizar este error numérico.\n\n### 2.2 Reconstrucción de la Superficie (Topología Fija)\nMediaPipe no solo entrega puntos aislados; los entrega con una **conectividad predefinida**. Esto significa que los 468 puntos forman una malla de triángulos fija.\n* **Justificación:** Se utiliza una matriz de adyacencia constante para definir cómo se conectan los vértices. Esto permite calcular el vector normal ($\\vec{N}$) de cualquier parte de la cara mediante el producto cruz de las aristas del triángulo formado por los landmarks:\n$$\\vec{N}_{cara} = (\\vec{P}_{v2} - \\vec{P}_{v1}) \\times (\\vec{P}_{v3} - \\vec{P}_{v1})$$\n\n### 2.3 Estimación de Profundidad Relativa\nUna de las mayores proezas numéricas de MediaPipe es obtener la coordenada $z$ (profundidad) a partir de una cámara 2D. Esto lo logra mediante **Inferencia de Profundidad**:\n1. El modelo asume un modelo de cabeza promedio (Weak Perspective Projection).\n2. La coordenada $z$ de los puntos se predice basándose en la deformación y el tamaño relativo de las facciones.\n3. El origen ($z=0$) se sitúa generalmente en el centro de masa de la cara o en el puente de la nariz.\n\n### 2.4 Alineación Espacial: Transformaciones Afines\nPara alinear los filtros (lentes, sombreros) con la cara detectada, se resuelven transformaciones afines que mantienen el paralelismo de las líneas. Si tenemos tres puntos de referencia en la cara (ojos y nariz), podemos calcular la **Matriz de Transformación Afín $A$** que mapea el objeto al espacio facial:\n\n$$\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$$\n\nEste sistema de ecuaciones se resuelve en cada frame para asegurar que el \"filtro\" siga el movimiento del usuario con precisión milimétrica.",
      "metadata": {}
    },
    {
      "id": "b3a23061-217e-4fd6-9232-6e8cfd205786",
      "cell_type": "markdown",
      "source": "## 3. El Problema PnP y Estimación de la Pose (Head Pose Estimation)\n\nPara que los filtros 3D se orienten correctamente, MediaPipe debe resolver el problema de **Perspectiva de n-Puntos (PnP)**. Esto consiste en encontrar la rotación ($R$) y traslación ($\\vec{t}$) de la cabeza a partir de las coordenadas 2D observadas en la imagen.\n\n### 3.1 El Modelo de Cámara Pinhole\nEl sistema asume que los puntos del mundo 3D ($X, Y, Z$) se proyectan en el plano de la imagen ($u, v$) siguiendo la ecuación de proyección central:\n\n$$s \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{K} [ \\mathbf{R} | \\vec{t} ] \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$$\n\nDonde:\n* **$\\mathbf{K}$:** Es la matriz de parámetros intrínsecos de la cámara (focal, centro óptico).\n* **$[\\mathbf{R}|\\vec{t}]$:** Es la matriz de parámetros extrínsecos (la pose que queremos hallar).\n* **$s$:** Es un factor de escala.\n\n### 3.2 Resolución mediante Iteración Numérica\nComo la imagen es una proyección 2D de un objeto 3D, el problema está **sobredeterminado**. MediaPipe utiliza algoritmos como **Levenberg-Marquardt** para minimizar el **Error de Reproyección**:\n\n$$\\min_{\\mathbf{R}, \\vec{t}} \\sum_{i=1}^{n} \\| p_i - \\text{proj}(\\mathbf{K}, \\mathbf{R}, \\vec{t}, P_i) \\|^2$$\n\nEste es un método numérico de optimización que ajusta la rotación y posición hasta que los puntos del modelo 3D \"calzan\" perfectamente sobre los píxeles de la cara detectada.\n\n### 3.3 Extracción de los Ángulos de Euler (Tait-Bryan)\nUna vez obtenida la matriz de rotación $\\mathbf{R}$, se descomponen los ángulos para aplicarlos en Three.js o A-Frame:\n* **Pitch (X):** Inclinación arriba/abajo.\n* **Yaw (Y):** Giro izquierda/derecha.\n* **Roll (Z):** Inclinación lateral.\n\nLa matriz de rotación se define como el producto:\n$$\\mathbf{R} = R_z(\\psi)R_y(\\theta)R_x(\\phi)$$\n\n### 3.4 Estabilización de la Pose (Filtrado de Señal)\nDebido al ruido en la captura de video, los valores de $R$ y $t$ pueden oscilar. Para evitar esto, se aplican técnicas de **Suavizado Exponencial**:\n\n$$S_t = \\alpha \\cdot X_t + (1 - \\alpha) \\cdot S_{t-1}$$\n\nDonde $\\alpha \\in [0, 1]$ es el factor de suavizado. Este método numérico permite que el sombrero o los lentes no \"tiemblen\" ante pequeñas variaciones de luz o movimiento del sensor.",
      "metadata": {}
    },
    {
      "id": "849d8e6f-6774-49f3-9b03-252c8b9d033f",
      "cell_type": "markdown",
      "source": "## 4. Estabilización de Datos: El Filtro \"One Euro\" ($1€$)\n\nEn la visión computacional en tiempo real, los landmarks detectados suelen presentar un fenómeno de \"jitter\" (vibración aleatoria) debido a errores de cuantización en la imagen y ruido en el sensor. MediaPipe utiliza el **Filtro One Euro**, un filtro de paso bajo adaptativo de primer orden.\n\n### 4.1 La Limitación de los Filtros Tradicionales\nUn filtro de paso bajo estándar tiene un compromiso (trade-off) crítico:\n1. **Si el corte es bajo:** Se elimina el ruido, pero se introduce **lag** (retraso) en movimientos rápidos.\n2. **Si el corte es alto:** El sistema responde rápido, pero los puntos tiemblan cuando el usuario está quieto.\n\n### 4.2 El Modelo Matemático Adaptativo\nEl filtro $1€$ resuelve esto ajustando dinámicamente su frecuencia de corte $f_c$ basándose en la velocidad de la señal. La ecuación fundamental del filtro de paso bajo es:\n\n$$\\hat{X}_k = \\alpha X_k + (1 - \\alpha) \\hat{X}_{k-1}$$\n\nDonde el factor de suavizado $\\alpha$ se calcula a partir de la frecuencia de corte:\n$$\\alpha = \\frac{1}{1 + \\frac{1}{2\\pi \\cdot \\Delta t \\cdot f_c}}$$\n\n### 4.3 Adaptación mediante la Velocidad (Derivada Discreta)\nLa clave numérica está en definir $f_c$ como una función de la velocidad de cambio de la señal ($\\dot{X}$):\n\n1. **Cálculo de la velocidad:** $\\dot{X}_k = \\frac{X_k - \\hat{X}_{k-1}}{\\Delta t}$\n2. **Suavizado de la velocidad:** $\\hat{\\dot{X}}_k = \\text{LowPass}(\\dot{X}_k, \\alpha_{speed})$\n3. **Frecuencia de corte adaptativa:** $f_c = f_{c_{min}} + \\beta |\\hat{\\dot{X}}_k|$\n\nDonde:\n* **$f_{c_{min}}$:** Evita el jitter cuando el usuario está quieto.\n* **$\\beta$:** Coeficiente de intensidad que aumenta la frecuencia de corte si hay mucha velocidad, reduciendo el lag.\n\n### 4.4 Justificación en Métodos Numéricos\nEste algoritmo representa un **controlador proporcional** simple aplicado al procesamiento de señales. Permite que MediaPipe entregue coordenadas que se sienten \"físicas\" y naturales, eliminando la alta frecuencia del ruido sin sacrificar la respuesta inmediata del sistema ante movimientos bruscos.",
      "metadata": {}
    },
    {
      "id": "0534d5da-c57d-461c-baad-84d93eb88dab",
      "cell_type": "markdown",
      "source": "## 5. Procesamiento de Tensores y Aceleración en GPU\n\nEl motor de cálculo de MediaPipe no opera sobre píxeles individuales de forma secuencial, sino que utiliza **Álgebra Multilineal** para procesar la información mediante Tensores.\n\n### 5.1 ¿Qué es un Tensor en MediaPipe?\nUn tensor es una generalización de los vectores y matrices a $n$ dimensiones. En el caso del video, la entrada es un tensor de cuarto orden:\n$$\\mathbf{T} \\in \\mathbb{R}^{B \\times H \\times W \\times C}$$\nDonde:\n* **$B$:** Batch size (usualmente 1 en tiempo real).\n* **$H, W$:** Altura y ancho de la imagen.\n* **$C$:** Canales de color (3 para RGB).\n\n### 5.2 Cuantización y Optimización Numérica\nPara ejecutar estos modelos en navegadores o móviles, MediaPipe utiliza **Cuantización**. Esto consiste en reducir la precisión de los pesos de la red neuronal de `float32` (32 bits) a `int8` (8 bits). \n\n**Justificación Numérica:** Aunque se introduce un pequeño error de redondeo, el volumen de datos se reduce en un **75%**, permitiendo que las operaciones de producto punto en las capas de la red se realicen mucho más rápido en la GPU.\n\n### 5.3 Implementación vía WebGL/WebGPU\nMediaPipe delega las multiplicaciones matriciales a la GPU mediante **Shaders de Computación**. Esto permite resolver el sistema de la red neuronal en paralelo:\n$$\\vec{y} = \\sigma(\\mathbf{W}\\vec{x} + \\vec{b})$$\nDonde $\\mathbf{W}$ es la matriz de pesos, $\\vec{x}$ los datos de entrada, $\\vec{b}$ el sesgo y $\\sigma$ la función de activación (ej. ReLU). Millones de estas operaciones ocurren cada 16ms.",
      "metadata": {}
    },
    {
      "id": "8d5005f4-333b-4686-abd6-2142009b906f",
      "cell_type": "markdown",
      "source": "## 5. Procesamiento de Tensores y Aceleración en GPU\n\nEl motor de cálculo de MediaPipe no opera sobre píxeles individuales de forma secuencial, sino que utiliza **Álgebra Multilineal** para procesar la información mediante Tensores.\n\n### 5.1 ¿Qué es un Tensor en MediaPipe?\nUn tensor es una generalización de los vectores y matrices a $n$ dimensiones. En el caso del video, la entrada es un tensor de cuarto orden:\n$$\\mathbf{T} \\in \\mathbb{R}^{B \\times H \\times W \\times C}$$\nDonde:\n* **$B$:** Batch size (usualmente 1 en tiempo real).\n* **$H, W$:** Altura y ancho de la imagen.\n* **$C$:** Canales de color (3 para RGB).\n\n### 5.2 Cuantización y Optimización Numérica\nPara ejecutar estos modelos en navegadores o móviles, MediaPipe utiliza **Cuantización**. Esto consiste en reducir la precisión de los pesos de la red neuronal de `float32` (32 bits) a `int8` (8 bits). \n\n**Justificación Numérica:** Aunque se introduce un pequeño error de redondeo, el volumen de datos se reduce en un **75%**, permitiendo que las operaciones de producto punto en las capas de la red se realicen mucho más rápido en la GPU.\n\n### 5.3 Implementación vía WebGL/WebGPU\nMediaPipe delega las multiplicaciones matriciales a la GPU mediante **Shaders de Computación**. Esto permite resolver el sistema de la red neuronal en paralelo:\n$$\\vec{y} = \\sigma(\\mathbf{W}\\vec{x} + \\vec{b})$$\nDonde $\\mathbf{W}$ es la matriz de pesos, $\\vec{x}$ los datos de entrada, $\\vec{b}$ el sesgo y $\\sigma$ la función de activación (ej. ReLU). Millones de estas operaciones ocurren cada 16ms.",
      "metadata": {}
    },
    {
      "id": "d7b17a5e-99f6-42cc-b056-42fe216fcddd",
      "cell_type": "markdown",
      "source": "## 6. Integración Técnica: El Vínculo MediaPipe - Three.js\n\nEn tu implementación, el flujo de datos representa un ciclo completo de **Métodos Numéricos Aplicados**:\n\n1. **Captura:** La cámara obtiene la matriz de píxeles $I$.\n2. **Inferencia (MediaPipe):** Se resuelve la regresión para obtener los landmarks $\\vec{P}_i$.\n3. **Mapeo:** Se transforman las coordenadas normalizadas $[0,1]$ al espacio euclidiano $[-n, n]$.\n4. **Transformación (Three.js):** Se actualizan las matrices de rotación y traslación de los objetos 3D.\n5. **Render:** Se proyectan los objetos 3D de nuevo a 2D para el monitor.\n\n### 6.1 Ejemplo de Lógica de Unión (Fragmento del código)\n```javascript\n// La regresión de MediaPipe entrega 'f' (landmarks)\nconst f = res.multiFaceLandmarks[0];\nconst le = f[33], re = f[263]; // Ojo Izquierdo y Derecho\n\n// Cálculo de la distancia euclidiana (Norma L2) para la escala\nconst eyeDist = Math.abs(le.x - re.x) * 3;\n\n// Aplicación de la transformación afín (Traslación)\nglasses.position.set(centerX, centerY, -0.35);",
      "metadata": {}
    },
    {
      "id": "5ea0a6f0-e7d4-4569-88ea-ead18a42a643",
      "cell_type": "markdown",
      "source": "### 6.2 Resumen Final de la Tecnología\n\n| Proceso | Método Numérico / Matemático |\n| :--- | :--- |\n| **Detección de Objetos** | Convolución Discreta y Redes Neuronales Convolucionales (CNN). |\n| **Landmarks (Puntos Clave)** | Regresión No-lineal de alta dimensionalidad sobre espacios latentes. |\n| **Estabilidad de Señal** | Filtros de Paso Bajo adaptativos (Filtro One Euro). |\n| **Alineación 3D** | Resolución del problema PnP (Perspective-n-Point) y Optimización de Reproyección. |\n| **Rendimiento** | Cuantización de tensores y computación paralela en GPU. |\n\n---\n\n> **Conclusión de MediaPipe:** Esta tecnología representa la frontera de los métodos numéricos aplicados, donde la resolución de sistemas de ecuaciones no se hace de forma manual, sino que se delega a modelos de aprendizaje automático que optimizan billones de parámetros para transformar datos visuales brutos en información geométrica útil.",
      "metadata": {}
    },
    {
      "id": "65f9788d-29f8-4ce6-bccc-5f9df5def8d1",
      "cell_type": "code",
      "source": "from IPython.display import display, HTML\n\n# Guardamos todo el código HTML/JS en una variable de texto de Python\ncodigo_html = \"\"\"\n<div id=\"main-container\">\n    <div class=\"panel\">\n      <button id=\"btn-glasses\">👓 Lentes</button>\n      <button id=\"btn-mustache\">🥸 Bigote</button>\n      <button id=\"btn-hat\">🎩 Sombrero</button>\n      <button id=\"btn-mole\">🟤 Lunar</button>\n      <button id=\"btn-points\">🟢 FaceMesh</button>\n    </div>\n\n    <video id=\"videoElement\" autoplay muted playsinline></video>\n    <canvas id=\"canvasElement\"></canvas>\n    <div id=\"status\">Cargando sistema...</div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\"></script>\n\n<style>\n    #main-container {\n        position: relative;\n        width: 640px;\n        height: 480px;\n        background: black;\n        margin: 0 auto;\n        border: 2px solid #444;\n        font-family: sans-serif;\n        overflow: hidden;\n    }\n    video, canvas {\n        position: absolute;\n        top: 0; left: 0;\n        width: 100%; height: 100%;\n        object-fit: cover;\n    }\n    .panel {\n        position: absolute;\n        top: 10px; left: 10px;\n        z-index: 50;\n        display: flex;\n        flex-direction: column;\n        gap: 5px;\n    }\n    button {\n        width: 140px;\n        padding: 8px;\n        cursor: pointer;\n        font-weight: bold;\n        border: 1px solid #999;\n        border-radius: 4px;\n        background: white;\n        transition: background 0.2s;\n    }\n    /* Clase activa (Verde) */\n    button.active {\n        background-color: #76ff03 !important;\n        border-color: #4caf50;\n    }\n    #status {\n        position: absolute;\n        bottom: 10px; left: 10px;\n        color: white;\n        background: rgba(0,0,0,0.5);\n        padding: 5px;\n        z-index: 40;\n        pointer-events: none;\n    }\n</style>\n\n<script>\n(function() {\n    // 1. Referencias\n    const container = document.getElementById('main-container');\n    if (!container) return; // Prevenir errores si se re-ejecuta\n    \n    const video = container.querySelector(\"#videoElement\");\n    const canvas = container.querySelector(\"#canvasElement\");\n    const ctx = canvas.getContext(\"2d\");\n    const statusDiv = container.querySelector(\"#status\");\n\n    // 2. Estado de los filtros\n    const filters = {\n        glasses: false,\n        mustache: false,\n        hat: false,\n        mole: false,\n        points: false\n    };\n\n    // 3. CONEXIÓN DE BOTONES\n    const buttons = {\n        'btn-glasses': 'glasses',\n        'btn-mustache': 'mustache',\n        'btn-hat': 'hat',\n        'btn-mole': 'mole',\n        'btn-points': 'points'\n    };\n\n    // Asignar eventos clic\n    for (const [btnId, filterName] of Object.entries(buttons)) {\n        const btn = container.querySelector(\"#\"+btnId);\n        if (btn) {\n            btn.onclick = function() {\n                filters[filterName] = !filters[filterName];\n                // Feedback Visual\n                if (filters[filterName]) {\n                    btn.classList.add('active');\n                } else {\n                    btn.classList.remove('active');\n                }\n            };\n        }\n    }\n\n    // 4. Ajustar Canvas\n    function resize() {\n        canvas.width = 640;\n        canvas.height = 480;\n    }\n    resize();\n\n    // 5. Configurar FaceMesh\n    const faceMesh = new FaceMesh({\n        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n    });\n\n    faceMesh.setOptions({\n        maxNumFaces: 1,\n        refineLandmarks: true,\n        minDetectionConfidence: 0.5,\n        minTrackingConfidence: 0.5\n    });\n\n    faceMesh.onResults(onResults);\n\n    // 6. Dibujar Resultados\n    function onResults(res) {\n        statusDiv.innerText = \"Rostro detectado ✅\";\n        ctx.clearRect(0, 0, canvas.width, canvas.height);\n        \n        // Dibujar imagen de video\n        ctx.drawImage(res.image, 0, 0, canvas.width, canvas.height);\n\n        if (res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0) {\n            const landmarks = res.multiFaceLandmarks[0];\n            drawFilters(landmarks, canvas.width, canvas.height);\n        } else {\n            statusDiv.innerText = \"Buscando rostro...\";\n        }\n    }\n\n    function drawFilters(f, w, h) {\n        // Puntos clave\n        const le = f[33];   // Ojo izq\n        const re = f[263];  // Ojo der\n        const lip = f[13];  // Labio\n        const cheek = f[50];// Mejilla\n\n        // Geometría\n        const cx = (le.x + re.x) / 2 * w;\n        const cy = (le.y + re.y) / 2 * h;\n        const dx = (re.x - le.x) * w;\n        const dy = (re.y - le.y) * h;\n        const dist = Math.sqrt(dx*dx + dy*dy);\n\n        if (filters.points) {\n            ctx.fillStyle = \"#00ff00\";\n            for (let p of f) {\n                ctx.beginPath();\n                ctx.arc(p.x * w, p.y * h, 1, 0, 2 * Math.PI);\n                ctx.fill();\n            }\n        }\n\n        if (filters.glasses) {\n            ctx.strokeStyle = \"black\";\n            ctx.lineWidth = 5;\n            ctx.strokeRect(cx - dist*0.8, cy - dist*0.3, dist*0.7, dist*0.4);\n            ctx.strokeRect(cx + dist*0.1, cy - dist*0.3, dist*0.7, dist*0.4);\n            ctx.beginPath();\n            ctx.moveTo(cx - dist*0.1, cy - dist*0.1);\n            ctx.lineTo(cx + dist*0.1, cy - dist*0.1);\n            ctx.stroke();\n        }\n\n        if (filters.mustache) {\n            const mx = lip.x * w;\n            const my = lip.y * h - dist * 0.15;\n            ctx.strokeStyle = \"black\"; \n            ctx.lineWidth = dist * 0.2;\n            ctx.lineCap = \"round\";\n            ctx.beginPath();\n            ctx.moveTo(mx - dist * 0.5, my + dist * 0.1);\n            ctx.quadraticCurveTo(mx, my - dist * 0.2, mx + dist * 0.5, my + dist * 0.1);\n            ctx.stroke();\n        }\n\n        if (filters.hat) {\n            ctx.fillStyle = \"#5d4037\";\n            ctx.fillRect(cx - dist, cy - dist * 1.8, dist * 2, dist * 0.8);\n            ctx.fillRect(cx - dist * 1.4, cy - dist, dist * 2.8, dist * 0.2);\n        }\n        \n        if (filters.mole) {\n             ctx.fillStyle = \"#3e2723\";\n             ctx.beginPath();\n             ctx.arc(cheek.x*w, cheek.y*h, dist*0.1, 0, Math.PI*2);\n             ctx.fill();\n        }\n    }\n\n    // 7. Iniciar Cámara\n    const camera = new Camera(video, {\n        onFrame: async () => {\n            await faceMesh.send({image: video});\n        },\n        width: 640,\n        height: 480\n    });\n\n    statusDiv.innerText = \"Solicitando cámara...\";\n    camera.start()\n        .then(() => statusDiv.innerText = \"Cámara lista.\")\n        .catch(err => statusDiv.innerText = \"Error: \" + err);\n\n})();\n</script>\n\"\"\"\n\n# Esta línea final es la que hace la magia de mostrar el HTML\ndisplay(HTML(codigo_html))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}